[{"slug":"accessing-parent-methods-in-javascript-classes","category":"blog","title":"Accessing Parent Methods in Javascript Classes","description":"Accessing Parent Methods in Javascript Classes","tags":["javascript","classes"],"body":"\nI recently got to do [Object-Oriented programming](https://en.wikipedia.org/wiki/Object-oriented_programming) in Javascript. Many of the Javascript features resemble Java language (`class`, `super`, `extends` and `static` keywords to name a few).\n\nOne of the differences with Java is that `super()` will not be called implicitly in child class so you must call it explicitly always when extending a class.\n\nAnother interesting issue I ran into is that if `super.someMethod()` is called from a child class, then `someMethod` **must not** be implemented as an arrow function. For example, the code below will trigger an error:\n\n```js\nclass Car {\n  drive = () => \"drive\"\n}\n\nclass FastCar extends Car {\n  drive = () => super.drive() + \" fast\"\n}\n\nconst fastCar = new FastCar()\nconsole.log(fastCar.drive())\n```\n\nThe error is `TypeError: Cannot read property 'call' of undefined` will occur on the invocation of the line `console.log(fastCar.drive())`. The error occurs because the `drive` method in `Car` is implemented as an arrow function. As per this [stackoverflow answer](https://stackoverflow.com/questions/46869503/es6-arrow-functions-trigger-super-outside-of-function-or-class-error) a class method implemented as an arrow function become a property with function type while a method implemented as a regular function stays a class method. So while methods are stored in class prototype, properties are only accessible on class instances. Therefore the correct implementation is as follows:\n\n```js\nclass Car {\n  drive() {\n    return \"drive\"\n  }\n}\n\nclass FastCar extends Car {\n  drive = () => super.drive() + \" fast\"\n}\n\nconst fastCar = new FastCar()\nconsole.log(fastCar.drive())\n```\n"},{"slug":"apollo-client-and-ssl-handshake-failure","category":"blog","title":"Cookie-based Authentication + Apollo React Client Results in SSL Handshake Failure","description":"TL;DR: handle host header with care.","tags":["graphql","apollo-client","ssl","cookies"],"body":"\nI recently was given a task to add user authentication to our website. It was decided to implement the authentication logic using [httpOnly](https://developer.mozilla.org/en-US/docs/Web/HTTP/Cookies) cookie. Client-side the browser will take care of passing the cookie of course so the only thing left was to pass the cookie header server-side. Headers can be passed to Apollo client in the [constructor](https://github.com/apollographql/apollo-client/blob/master/src/ApolloClient.ts#L40). The diagram below displays our technology stack:\n\n![our technology stack](/images/blog/stackDiagram.png)\n\n`next-with-apollo` is a handy npm [package](https://www.npmjs.com/package/next-with-apollo) which makes headers available from [Express.js](https://expressjs.com) (which serves server-side pages), thus:\n\n```jsx\nimport withApollo from \"next-with-apollo\"\nimport ApolloClient, { InMemoryCache } from \"apollo-boost\"\nimport renderFn from \"./render\"\n\nexport default withApollo(\n  ({ initialState, headers }) => {\n    return new ApolloClient({\n      uri: \"https://example.com/graphql\",\n      cache: new InMemoryCache().restore(initialState || {}),\n      headers, // <- headers are passed here\n    })\n  },\n  {\n    render: renderFn,\n  }\n)\n```\n\nAfter I implemented the logic I was quite happy and went for a break to practice my [Yo-Yo](https://www.youtube.com/watch?v=-wiNh4LLQzg) skills (Yo-Yo-ing has become trendy in our office).\n\nI returned to my work station in order to finish the task: polish the server logic which handles authentication. Eventually I was done.\n\n> Everything worked perfectly in the development environment.\n\nI had another Yo-Yo practice. The only thing left was to deploy the logic to our staging environment. When I checked the website after deployment there was an error:\n\n> ApolloError: Network error: request to https://example.com/graphql/ failed, reason: write EPROTO 140152723232576:error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1544:SSL alert number 40\n\nSince I added a lot of logic in the task including [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS), cookies and a bunch of graphql resolvers I had to minimize the number of possible causes of bugs.\n\nAt first I suspected that CORS is culprit, that somehow it blocks browser requests. This was not the case however. After removing authentication logic piece by piece in order to identify the problem I discovered the issue was passing headers to the Apollo client as can be seen in the code excerpt above, specifically the `host` header which was something like `bla.bla.bla.elasticbeanstalk.com`.\n\n\"But what does that have to do with the SSL handshake error?\", you ask. Well, it turns out that `host` header is quite important and it may be used in Server Name Indication ([SNI](https://en.wikipedia.org/wiki/Server_Name_Indication)) extension to TLS protocol. Here's the gist:\n\n> Serving multiple domains (virtual hosts) per a given IP address is called [name-based virtual hosting](https://en.wikipedia.org/wiki/Virtual_hosting#Name-based). In name-based hosting you tell the server the virtual host you're interested in via the host header. Such approach wouldn't work out of the box because headers are not sent to the server before SSL handshake has occurred. But in order for the SSL handshake to occur the server must somehow know for which domain to serve the SSL certificate. In order to solve the chicken-and-egg conundrum SNI was invented: at the beginning of SSL handshake you tell the server which domain you're interested in, usually via the `servername` option. Thus SNI allows to use SSL with name-based virtual hosting which is significantly [cheaper](https://aws.amazon.com/cloudfront/custom-ssl-domains/) than having a dedicated IP address per domain. Different clients implement SNI differently, specifically Apollo Client [uses](https://github.com/apollographql/apollo-client/blob/master/src/link/http/checkFetcher.ts) fetch API-based utility `node-fetch` which in [its turn](https://github.com/node-fetch/node-fetch/blob/cd33d2237889e13847b9b5168075753b66a16449/src/index.js#L60) uses Node.js `https` module. If the host header is set to some value then `https` module [assigns](https://github.com/nodejs/node/blob/6bcea0a38365f518580a4dbbf2f5627bede5aac5/lib/_http_agent.js#L275) the value to `servername`, otherwise the hostname is assigned to `servername`.\n\nSo because our AWS setup was configured to use SNI for the website and I was explicitly passing the host header `bla.bla.bla.elasticbeanstalk.com`, the `servername` was set to it and as a result SNI failed because our SSL certificate was rather for `example.com`.\n\nIn order to fix the bug I [found out](https://github.com/lfades/next-with-apollo/issues/88#issuecomment-570010727) that you can simply pass only the header you need in Apollo client constructor, so I only passed the cookie header:\n\n```jsx\nimport withApollo from \"next-with-apollo\"\nimport ApolloClient, { InMemoryCache } from \"apollo-boost\"\nimport renderFn from \"./render\"\n\nexport default withApollo(\n  ({ initialState, headers }) => {\n    return new ApolloClient({\n      uri: \"https://example.com/graphql\",\n      cache: new InMemoryCache().restore(initialState || {}),\n      headers: {\n        cookie: headers?.cookie,\n      },\n    })\n  },\n  {\n    render: renderFn,\n  }\n)\n```\n\nIn case you're wondering how the question mark in this line `cookie: headers?.cookie` is valid Javascript check out the [optional chaining](https://www.npmjs.com/package/babel-plugin-transform-optional-chaining) Babel.js plugin, it's awesome!\n"},{"slug":"case-sensitive-file-systems-and-git","category":"blog","title":"The Curious Case of Git, macOS Filesystem and Case Sensitivity","description":"Case Sensitivity on macOS Filesystem and in Git","tags":["filesystem","git","macOS"],"body":"\nWhile working on this blog (which I forked from [here](https://github.com/gatsbyjs/gatsby-starter-blog)) I noticed that I have a file `bio.js` which was a React component. I'm used to capitalizing React components so I renamed the file `Bio.js`. I worked on some other files, made a git commit, pushed the changes and deployed. The build failed. I use Netlify for deployments so I started reading their build logs. The build failed because of this error:\n\n```\nCan't resolve './Bio' in '/opt/build/repo/src/components'\n```\n\nAfter some [googling](https://stackoverflow.com/a/53116/5863693) I found out that Git was ignoring letter casing so the remote repository still contained `bio.js`. I changed git config to **not** ignore casing by running this command:\n\n```\ngit config core.ignorecase false\n```\n\nI deployed but the build failed again. This time the error was:\n\n```\nerror Multiple \"root\" queries found: \"BioQuery\" and \"BioQuery\".\n```\n\nMy `Bio.js` component was indeed making a static graphql query but I made sure it was only done once. Then I noticed that I actually had 2 files in Git: `Bio.js` and `bio.js`. I eventually found out that this was happening because macOS is [case-insensitive but case-preserving](https://stackoverflow.com/a/18000286/5863693). This means that as far as macOS is concerned `Bio.js` and `bio.js` are the same file.\n\nLinux is case sensitive of course, so because Netlify uses Linux servers my deployment failed but in development everything worked. So the correct way to change casing in files in macOS which are already in Git is first move them to a temporary location, remove them from Git (using `git rm`), push the changes, then return the file back and add it to Git.\n\n---\n\nBy the way, macOS filesystem is not necessarily case-insensitive as in my case. To [find out](https://apple.stackexchange.com/a/22304/228585) regarding your macOS run `diskutil list` in Terminal to list your devices and `diskutil info some-device`. In my case it was `Name (User Visible): Mac OS Extended (Journaled)` which means it's not case sensitive but it can also be `Mac OS Extended (Case-sensitive, Journaled)`\n"},{"slug":"cors-and-other-gotchas-with-aws-api-gateway-lambda","category":"blog","title":"How to Configure CORS in AWS API Gateway with Lambda And Other Gotchas","description":"How to Configure CORS in AWS API Gateway with Lambda And Other Gotchas","tags":["cors","aws","serverless"],"body":"\nI recently had a project where I had a website which used a REST API comprised of just one endpoint, which performed a very simple task. This is a great use case for a serverless function, like [AWS Lambda](https://aws.amazon.com/lambda).\n\n### Table of Contents:\n\n1. [CORS issue.](#cors)\n2. [Test event quirk.](#test-event)\n\n<a name=\"cors\" style=\"color: black;box-shadow: none;\">In</a> order to expose an AWS Lambda function to the \"outer world\" another AWS service can be used: [API Gateway](https://aws.amazon.com/api-gateway). A great tutorial from AWS explains how to set up [AWS Lambda](https://webapp.serverlessworkshops.io/serverlessbackend/) and [API Gateway](https://webapp.serverlessworkshops.io/restfulapis/restapi/) services.\n\nSince my website and the REST API used different domains and [non-simple requests](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS#simple_requests) I had to whitelist the website domain in the backend service in order to comply with CORS. API Gateway passes incoming requests to AWS Lambda so I thought all I need to do is enable CORS in API Gateway which is quite easy because during API Gateway resource setup you can check `Enable CORS` checkbox:\n\n![enable CORS in API Gateway](/images/blog/api-gateway.png)\n\nHowever, when the website made a request to the API Gateway invoke URL the browser notified of CORS error, specifically that the `Access-Control-Allow-Origin` header was missing:\n\n<span style=\"background-color: pink;\">Access to fetch at 'myApiUrl.com' from origin 'myWebsiteUrl.com' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.</span>\n\nIt turns out that in API Gateway integration with AWS Lambda, the latter is [responsible](https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html) for returning the headers required by CORS\n\n![enable CORS in AWS Lambda](/images/blog/api-gateway-cors.png)\n\nBelow is an example from [AWS docs](https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-cors.html) on how to add the necessary CORS headers to your handler response:\n\n```js\nexports.handler = async event => {\n  const response = {\n    statusCode: 200,\n    headers: {\n      \"Access-Control-Allow-Headers\": \"Content-Type\",\n      \"Access-Control-Allow-Origin\": \"https://www.example.com\",\n      \"Access-Control-Allow-Methods\": \"OPTIONS,POST,GET\",\n    },\n    body: JSON.stringify(\"Hello from Lambda!\"),\n  }\n  return response\n}\n```\n\nOnce I added the above headers the CORS issue was resolved.\n\n<a name=\"test-event\" style=\"color: black;box-shadow: none;\">Another</a> quirk I came across was inconsistency between the type of request body and test event (using Node.js environment):\n\n```js\nexports.handler = async event => {\n  let url\n  if (typeof event.body === \"string\") {\n    // handle real requests\n    url = JSON.parse(event.body).url\n  } else {\n    // handle test events\n    url = event.body.url\n  }\n\n  const response = {\n    statusCode: 200,\n  }\n\n  return response\n}\n```\n\nWhen the handler receives a real request with a JSON body, the body is a string which needs to be parsed as an object first in order to access its fields (`url` in the code above). When configuring a test event as follows:\n\n![AWS Lambda test event](/images/blog/test-event.png)\n\nI expected that the test event would also be a string, because the input must be JSON however, it is actually parsed automatically before arriving to the handler. This means a condition is required (as in the code above) to distinguish between real requests and test events.\n\nApart from the above issues I had a great experience with AWS Lambda. The fact the you can create a REST API so quickly without having to maintain any server environment is amazing!\n"},{"slug":"creating-slideshow-component","category":"blog","title":"How to Create a Simple Slideshow Component in React","description":"How to Create a Simple Slideshow Component in React","tags":["react","slideshow","tutorial"],"body":"\nI recently had to create a slidehow component, something along [these](https://react-slideshow.herokuapp.com/fade-effect) lines. I was surprised how easy it is to create such a component and would like to share an example below. In the example I will be concentrating on logic more rather than on the UI design.\n\nThe component will be changing slides every `n` seconds. Also the component will have buttons area below (a button per each slide) so that the user can click a button and thus change a slide.\n\nBecause slides are changed only in one direction (unless the user clicks a particular button) essentially a circular linked list data structure would serve us best. But such data structure is not provided by Javascript therefore we'll use an array. We will need state to hold the current image as well. It's important to understand that although we'll be using an array we want to somehow achieve the \"circular\" property therefore we can use modulo operation in order to get the index of the next slide to show:\n\n```js\nconst initialIndex = 0\nconst SlideShow = ({ slides, intervalBetweenSlidesSec = 5 }) => {\n  const [activeSlide, setActiveSlide] = useState({\n    slide: slides[initialIndex],\n    index: initialIndex,\n  })\n\n  const changeSlides = () =>\n    setActiveSlide(({ index }) => {\n      const nextIndex = (index + 1) % slides.length\n      return {\n        slide: slides[nextIndex],\n        index: nextIndex,\n      }\n    })\n}\n```\n\nThe next piece of functionality we need to add is an interval so that the slides will be changed automatically every `intervalBetweenSlidesSec` seconds. If the user clicks on a certain button the previous interval must be cleared and a new one created. Because the interval id will be used both inside `useEffect` which kicks off automatic slides change and in the button `onClick` handler the interval id must be stored in a variable which will be accessible in those both places, a ref. This is the complete example:\n\n```jsx\nconst MILLIS = 1000\nconst initialIndex = 0\nconst SlideShow = ({ slides, intervalBetweenSlidesSec = 5 }) => {\n  const [activeSlide, setActiveSlide] = useState({\n    slide: slides[initialIndex],\n    index: initialIndex,\n  })\n  const intervalRef = useRef()\n  const changeSlides = useCallback(() => {\n    setActiveSlide(({ index }) => {\n      const nextIndex = (index + 1) % slides.length\n      return {\n        slide: slides[nextIndex],\n        index: nextIndex,\n      }\n    })\n  }, [slides])\n\n  useEffect(() => {\n    intervalRef.current = setInterval(\n      changeSlides,\n      intervalBetweenSlidesSec * MILLIS\n    )\n\n    return () => clearInterval(intervalRef.current)\n  }, [changeSlides, intervalBetweenSlidesSec])\n\n  const {\n    slide: { src, alt, title },\n  } = activeSlide\n  return (\n    <div style={{ position: \"relative\", height: \"100%\" }}>\n      <div>\n        <img src={src} alt={alt} title={title} style={{ height: 200 }} />\n      </div>\n      <div\n        style={{ position: \"absolute\", zIndex: 1, bottom: 1, width: \"100%\" }}\n      >\n        <div style={{ display: \"flex\", justifyContent: \"center\" }}>\n          {slides.map((_, index) => {\n            return (\n              <span\n                key={index}\n                style={{ paddingRight: index !== slides.length - 1 ? 20 : 0 }}\n              >\n                <button\n                  style={{\n                    height: 16,\n                    display: \"inline-block\",\n                    borderRadius: \"100%\",\n                    background: activeSlide.index === index ? \"black\" : \"white\",\n                  }}\n                  onClick={() => {\n                    setActiveSlide({\n                      slide: slides[index],\n                      index,\n                    })\n                    clearInterval(intervalRef.current)\n                    intervalRef.current = setInterval(\n                      changeSlides,\n                      intervalBetweenSlidesSec * MILLIS\n                    )\n                  }}\n                />\n              </span>\n            )\n          })}\n        </div>\n      </div>\n    </div>\n  )\n}\n```\n\nI wanted to wrap the buttons inside a `div` with absolute position so that if the images are of different dimensions (which is not recommended but can happen) there's no jump and the buttons area always stays in the same place.\n\nThe only thing missing is some animation. I think fade in animation will work great in this case. Because I often use such animation I created a custom hook for it:\n\n```js\nimport { useState, useCallback } from \"react\"\n\nconst useFadeAnimation = (options = {}) => {\n  const { animationDuration = 1000, animateInitially = false } = options\n  const [showAnimation, setShowAnimation] = useState(animateInitially)\n  const animate = useCallback(() => {\n    setShowAnimation(true)\n    setTimeout(() => setShowAnimation(false), animationDuration + 100)\n  }, [animationDuration])\n\n  return {\n    showAnimation,\n    animate,\n  }\n}\n```\n\nWe'll also create animation component using [styled-components](https://styled-components.com/) from this awesome [article](https://medium.com/codeuai/working-with-animations-using-styled-components-de4dca3a0e79). You can find the code for this in the codesandbox link below.\n\nThe final result can be seen below and of course you can check out the code using the link below!\n\n<iframe src=\"https://codesandbox.io/embed/slideshow-67skz?fontsize=14&hidenavigation=1&theme=dark\"\n     style=\"width:100%; height:500px; border:0; border-radius: 4px; overflow:hidden;\"\n     title=\"SlideShow\"\n     allow=\"accelerometer; ambient-light-sensor; camera; encrypted-media; geolocation; gyroscope; hid; microphone; midi; payment; usb; vr; xr-spatial-tracking\"\n     sandbox=\"allow-forms allow-modals allow-popups allow-presentation allow-same-origin allow-scripts\"\n   ></iframe>\n"},{"slug":"defaults-with-nested-schemas-in-mongoose","category":"blog","title":"How To Add Defaults In Mongoose Nested Schemas","description":"How To Add Defaults In Mongoose Nested Schemas","tags":["mongoose","mongodb"],"body":"\nRecently, I needed to update an existing [mongoose](https://mongoosejs.com/) schema (for those unfamiliar mongoose is a Javascript ORM for MongoDB) with a nested `rating` object. During the task I needed to define default values for nested mongoose schemas and update the existing documents with those which turned out a bit tricky. Read further for tips on how to do it.\n\nThe schema was for `Brand` object and already had a lot of fields. For the purposes of the article let's define the `Brand` schema like so:\n\n```js\nconst mongoose = require(\"mongoose\")\nconst Schema = mongoose.Schema\nconst Brand = new Schema({\n  meta: {\n    name: {\n      type: String,\n      required: true,\n      unique: true,\n    },\n    slug: {\n      type: String,\n      required: true,\n      unique: true,\n    },\n  },\n  contact: {\n    email: String,\n    address: String,\n  },\n})\n\nmodule.exports = mongoose.model(\"Brand\", Brand, \"Brand\")\n```\n\nThe new `rating` object consisted of multiple sub-fields of objects. The lowest object level consisted of the fields `score` whose default value should be `0` and `maxScore` which can vary with each object field. Also the `rating` object is required. An intuitive implemention which doesn't implement defaults and mandatory fields might look like this:\n\n```js\nconst mongoose = require(\"mongoose\")\nconst Schema = mongoose.Schema\nconst Brand = new Schema({\n  meta: {\n    name: {\n      type: String,\n      required: true,\n      unique: true,\n    },\n    slug: {\n      type: String,\n      required: true,\n      unique: true,\n    },\n  },\n  contact: {\n    email: String,\n    address: String,\n  },\n  rating: {\n    userExperience: {\n      overall: {\n        score: Number,\n        maxScore: Number,\n      },\n    },\n    independentReviews: {\n      overall: {\n        score: Number,\n        maxScore: Number,\n      },\n    },\n    overall: {\n      score: Number,\n      maxScore: Number,\n    },\n  },\n})\n\nmodule.exports = mongoose.model(\"Brand\", Brand, \"Brand\")\n```\n\nAs you can see the fields `score` and `maxScore` appear a lot in the schema so it will be easier to create a helper function `getRating` which receives a `maxScore` and returns an object with the relevant `score` and `maxScore` fields. In addition we'll create a new type called `BrandRating` so that we can mark it as required on `Brand` object:\n\n```js\nconst mongoose = require(\"mongoose\")\nconst Schema = mongoose.Schema\nconst dbUrl = \"mongodb://localhost:27017/local\"\n\nconst main = async () => {\n  mongoose.connect(dbUrl, {\n    useNewUrlParser: true,\n    socketTimeoutMS: 3000000,\n    keepAlive: 3000000,\n    reconnectTries: 30,\n    reconnectInterval: 10000,\n  })\n\n  const minScore = 0\n  const getRating = ({ maxScore }) => ({\n    type: new Schema(\n      {\n        score: {\n          type: Number,\n          min: minScore,\n          max: maxScore,\n        },\n        maxScore: Number,\n      },\n      { _id: false }\n    ),\n    required: true,\n    default: {\n      score: minScore,\n      maxScore,\n    },\n  })\n\n  const BrandRating = new Schema(\n    {\n      userExperience: {\n        overall: getRating({ maxScore: 50 }),\n      },\n      independentReviews: {\n        overall: getRating({ maxScore: 50 }),\n      },\n      overall: getRating({ maxScore: 100 }),\n    },\n    { _id: false }\n  )\n\n  const Brand = new Schema({\n    meta: {\n      name: {\n        type: String,\n        required: true,\n        unique: true,\n      },\n      slug: {\n        type: String,\n        required: true,\n        unique: true,\n      },\n    },\n    rating: {\n      type: BrandRating,\n      required: true,\n      default: {},\n    },\n  })\n  const BrandModel = mongoose.model(\"Brand\", Brand, \"Brand\")\n  const CurbYourEnthusiasm = new BrandModel({\n    meta: {\n      name: \"Curb Your Enthusiasm\",\n      slug: \"cye\",\n    },\n    contact: {\n      email: \"cye@example.com\",\n      address: \"California, USA\",\n    },\n  })\n  await CurbYourEnthusiasm.validate()\n  await CurbYourEnthusiasm.save()\n}\n\nmain().catch(error => console.error(error))\n```\n\nThe code above can be run as is and will create a `CurbYourEnthusiasm` document complete with the `rating` object and all of the defaults. In case you already had a lot of documents and you just need to update them with the new rating system then it will just take a one-liner:\n\n```js\nawait BrandModel.updateMany({}, { $set: { rating: {} } }).lean()\n```\n\n### A Few Side Notes:\n\n- Setting the default for `rating` as empty object is crucial in order to enable creation of defaults. Without it default `score` values will not be created on nested schemas.\n- Setting `_id: false` option is handy because id's are not needed on nested schemas because they're just an abstraction to enable validation (`required` fields) and default values.\n- Calling `.lean()` on mongoose model methods is an optimization technique which return a plain Javascript object of the document instead of a mongoose object which has mongoose-specific metadata and internal state sometimes making the lean document [10 times](https://mongoosejs.com/docs/tutorials/lean.html) smaller than the mongoose object.\n"},{"slug":"deploying-to-elastic-beanstalk","category":"blog","title":"How to Deploy Your Dockerized Application to AWS Elastic Beanstalk","description":"How to Deploy Your Dockerized Application to AWS Elastic Beanstalk","tags":["docker","aws","elastic-beanstalk"],"body":"\nDockerizing applications has become the norm: it allows for easy, reproducible deploys on numerous CI/CD platforms. A common workflow is to push docker images to a docker registry, for example, [Docker Hub](https://hub.docker.com) which is free, then have the deployment platform pull the image from the registry and deploy it. In this article I will explain how to deploy a dockerized application to [Elastic Beanstalk (EB)](https://aws.amazon.com/elasticbeanstalk/) which is an Amazon Web Services (AWS) platform used to manage docker containers. The article assumes you already have an AWS account. If you don't you can [create](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc) one for free.\n\n### Docker Hub Setup <a name=\"dockerhub\"></a>\n\nThe information on how to push a docker image to Docker Hub can be found [here](https://docs.docker.com/docker-hub/repos/). Once your image is pushed, we can proceed to EB setup.\n\n### Initial EB Setup <a name=\"intialebssetup\"></a>\n\nThe <a name=\"aws-setup-link\" style=\"color: black;box-shadow: none;\">instructions</a> from this [link](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/GettingStarted.CreateApp.html) explain how to set up a test app. This will greatly simplify the setup and we'll change the test app to our dockerized app later. It's important to select `Docker` as the platform and `Multi-container Docker running on 64bit Amazon Linux` as the platform branch. In the course of the setup an [EC2](https://aws.amazon.com/ec2/?ec2-whats-new.sort-by=item.additionalFields.postDateTime&ec2-whats-new.sort-order=desc) instance will be created as well. EC2 is an AWS service which provides a virtual machine where the Docker containers actually run. My instance was spun up as a t2.micro instance which has 1GB of RAM. It takes about 10-15 min until the instance is up.\n\n### Allow EB to Pull Your Images from Docker Hub\n\nThe next step is to allow EB to connect to Docker Hub and pull the image of your application. This step is only required if your Docker Hub account is private. Docker Hub authentication details will be provided via `Dockerrun.aws.json`:\n\n1. It depends on the OS you're using and it [could](https://www.oasisworkflow.com/accessing-private-docker-images-from-aws-elastic-beanstalk) be that running `docker login` will automatically generate `dockercfg` file for you, however on macOS this is not the case. I pulled a docker image which accepts Docker Hub credentials and generates the `dockercfg` file in the same directory where it was run (works on macOS and perhaps Linux as well.). First create a file `credentials.env` and paste the following details in it:\n\n```\nDOCKER_USERNAME=YOUR_DOCKER_HUB_USERNAME\nDOCKER_PASSWORD=YOUR_DOCKER_HUB_PASSWORD\nDOCKER_REGISTRY=https://index.docker.io/v1/\n```\n\nthen run the following command:\n\n```bash\ndocker run -it --rm \\\n   --env-file=credentials.env \\\n   -v \"$(pwd):/opt/data/\" \\\n   -v \"/var/run/docker.sock:/var/run/docker.sock\" \\\n   codeship/dockercfg-generator /opt/data/dockercfg\n```\n\nThe format of the generated file may not be compliant with the format expected by EB as stated [here](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/private-auth-container-instances.html):\n\n> Newer versions of Docker create a configuration file as shown above with an outer auths object. The Amazon ECS agent only supports dockercfg authentication data that is in the below format, without the auths object.\n\nTherefore, edit the generated file to have the following format:\n\n```\n{\n  \"https://index.docker.io/v1/\": {\n    \"auth\": \"YOUR_AUTH_CODE\",\n    \"email\": \"your@email.com\"\n  }\n}\n```\n\nI renamed the file to `dockercfg`.\n\n2. Upload the `dockercfg` File to S3\n\n[S3](https://aws.amazon.com/s3/) is an AWS object storage service. In order for EB to access the `dockercfg` file it must be stored in S3. Because we used the [link](#aws-setup-link) to set up a test application an S3 bucket was already set up for us. Therefore, we can navigate to S3 in AWS console and select the bucket with `elasticbeanstalk` in its name. I created a folder called `config` in which I uploaded the `dockercfg` file. Make sure that the S3 region corresponds to the region selected for EB and follow the instructions in this [link](https://console.aws.amazon.com/iam/home#/roles) in order to make sure that `aws-elasticbeanstalk-ec2-role` role has the `s3:GetObject` permission. In my case I had the permission policy `AWSElasticBeanstalkWebTier` in EB role by default which already contains the permission (this role should be created during the intial [setup](#aws-setup-link)).\n\n## Create Dockerrun.aws.json\n\nThe EB equivalent of docker-compose.yml is a configuration file called `Dockerrun.aws.json`. Specifically [version 2](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_v2config.html) of the file should be used with multi-container EB platform. Below is a sample configuration:\n\n```json\n{\n  \"AWSEBDockerrunVersion\": 2,\n  \"authentication\": {\n    \"bucket\": \"your_bucket_name\",\n    \"key\": \"config/dockercfg\"\n  },\n  \"containerDefinitions\": [\n    {\n      \"name\": \"api\",\n      \"image\": \"dockerhubuser/image:version\",\n      \"memory\": 256,\n      \"essential\": true,\n      \"environment\": [\n        {\n          \"name\": \"YOUR_ENVIRONMENT_VARIABLE_NAME\",\n          \"value\": \"YOUR_ENVIRONMENT_VARIABLE_VALUE\"\n        }\n      ],\n      \"portMappings\": [\n        {\n          \"hostPort\": 80,\n          \"containerPort\": 8001\n        }\n      ]\n    }\n  ]\n}\n```\n\nThe `authentication` field points EB to the `dockercfg` file with Docker Hub authentication details. An array of runtime environment variables may be specified under `environment` field. Finally `portMappings` maps host port to container port. Once the configuration file is created it can be uploaded to EB via AWS console: go to Elastic Beanstalk and click on \"Upload and deploy\":\n\n![upload new configuration](/images/blog/upload-and-deploy.png)\n\nthen click on \"Application Versions page\" on the pop-up window:\n![upload popup](/images/blog/app-version-page.png)\n\nfinally click \"Upload\":\n![actual upload](/images/blog/actual-upload.png)\n\nand select the configuration file in the format of `Dockerrun.aws.json` as the above sample, and give it a name under version label. Once the new configuration was uploaded select the corresponding checkbox and click \"Deploy\":\n\n![deploy](/images/blog/deploy.png)\n\nEB will start the deploy of your application which will take a few minutes. In my case a load balancer was configured in EC2 for the application as well. It is worth noting that load balancer will perform health checks on your app by making a GET http request to `/` route on port `80` by default. If the application doesn't respond the enviroment health icon will become red and the application will not accept any other network requests until load balancer receives a valid response from the application.\n\n![health](/images/blog/health.png)\n\nThere're 2 options in such case: either create a health check route in your server application or [remove](https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-delete.html) the load balancer (which is a paid service unless you're in free tier and plan to use it less than the quota).\n\nYour application is now live ðŸ”¥!\n"},{"slug":"filtering-values-in-nested-arrays-mongodb","category":"blog","title":"Filtering Values In Nested Arrays In MongoDB","description":"How To Filter Values In Nested Arrays In MongoDB","tags":["mongodb"],"body":"\nUsually when you make queries in MongoDB, the returned result is the whole document with all the fields unless you make a projection. However, sometimes you may want to filter some field, specifically an array field by a certain condition. There're 3 options to achieve this:\n\n1. [\\$elemMatch](#elemMatch)\n2. [\\$filter](#filter)\n3. [\\$lookup with pipeline](#lookup)\n\n## `$elemMatch` <a name=\"elemMatch\"></a>\n\n`$elemMatch` can be used in find operations. Suppose we have a collection named `UserReview` which contains reviews users made for a certain product and the documents also have a `replies` field which contains an array of comments made to a review. Each reply also has a `status` field which tells us whether a reply is approved or not to be displayed on the website. A document looks like so:\n\n```js\n{\n      _id: 1,\n      username: \"John\",\n      content: \"I liked the product\",\n      replies: [\n        {\n          username: \"Andy\",\n          content: \"I liked the product\",\n          status: \"REJECTED\"\n        },\n        {\n          username: \"Bob\",\n          content: \"cool\",\n          status: \"APPROVED\"\n        }\n      ]\n    }\n```\n\nIf we want to find user reviews only with approved replies we can run the following query from MongoDB shell:\n\n```js\ndb.UserReview.find({ \"replies.status\": \"APPROVED\" })\n```\n\nwhich will give us the following result:\n\n```json\n[\n  {\n    \"_id\": 1,\n    \"content\": \"I liked the product\",\n    \"replies\": [\n      {\n        \"content\": \"I liked the product\",\n        \"status\": \"REJECTED\",\n        \"username\": \"Andy\"\n      },\n      {\n        \"content\": \"cool\",\n        \"status\": \"APPROVED\",\n        \"username\": \"Bob\"\n      }\n    ],\n    \"username\": \"John\"\n  }\n]\n```\n\nAs can be seen the `replies` field contains both approved and rejected replies because MongoDB returns the document with all the fields by default. If we want to see only approved replies we can use the following projection:\n\n```js\ndb.UserReview.find(\n  {\n    \"replies.status\": \"APPROVED\",\n  },\n  {\n    content: 1,\n    username: 1,\n    replies: {\n      $elemMatch: {\n        status: \"APPROVED\",\n      },\n    },\n  }\n)\n```\n\nThis will return the following result:\n\n```json\n[\n  {\n    \"_id\": 1,\n    \"content\": \"I liked the product\",\n    \"replies\": [\n      {\n        \"content\": \"cool\",\n        \"status\": \"APPROVED\",\n        \"username\": \"Bob\"\n      }\n    ],\n    \"username\": \"John\"\n  }\n]\n```\n\nYou can check out the query and play with it in this [playground](https://mongoplayground.net/p/ZYRudaLv5h6).\n\n## `$filter` <a name=\"filter\"></a>\n\nIn aggregation pipeline `$filter` can be used. Given the same document as above the aggregation would be as follows:\n\n```js\ndb.UserReview.aggregate([\n  {\n    $match: {\n      \"replies.status\": \"APPROVED\",\n    },\n  },\n  {\n    $project: {\n      username: 1,\n      content: 1,\n      replies: {\n        $filter: {\n          input: \"$replies\",\n          as: \"reply\",\n          cond: {\n            $in: [\"$$reply.status\", [\"APPROVED\"]],\n          },\n        },\n      },\n    },\n  },\n])\n```\n\nIn `$filter` operation a temporary internal variable `reply` is created on which conditional operator `cond` is invoked. `$$reply.status` is prefixed with `$$` in order to refer to the temporary variable. You can play with the aggregation [here](https://mongoplayground.net/p/ljWhbenpOb8).\n\nI personally prefer to use `$addFields` stage in this case because it allows to overwrite the original `replies` field. This way other user review fields like `content` and `username` do not need to be explicitly specified in projection:\n\n```js\ndb.UserReview.aggregate([\n  {\n    $match: {\n      \"replies.status\": \"APPROVED\",\n    },\n  },\n  {\n    $addFields: {\n      replies: {\n        $filter: {\n          input: \"$replies\",\n          as: \"reply\",\n          cond: {\n            $in: [\"$$reply.status\", [\"APPROVED\"]],\n          },\n        },\n      },\n    },\n  },\n])\n```\n\n## `$lookup` with pipeline <a name=\"lookup\"></a>\n\nIt's worth mentioning that in a real life scenario user reviews and user replies may be stored in separate collections, therefore a `$lookup` stage will be required. In such case a special case of lookup can be used which uses `pipeline` [operation](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/index.html#join-conditions-and-uncorrelated-sub-queries).\n\nSuppose the `UserReview` collections has the following document:\n\n```js\n    {\n      _id: 1,\n      username: \"John\",\n      content: \"I liked the product\"\n    }\n```\n\nwhile the `UserReply` has the follwing documents:\n\n```js\n    {\n      _id: 11,\n      username: \"Andy\",\n      content: \"I liked the product\",\n      userReviewId: 1,\n      status: \"REJECTED\"\n    },\n    {\n      _id: 12,\n      username: \"Bob\",\n      content: \"cool\",\n      userReviewId: 1,\n      status: \"APPROVED\"\n    }\n```\n\nIn such case the following aggregation can be performed ([playground](https://mongoplayground.net/p/73CYI8Hw136)):\n\n```js\ndb.UserReview.aggregate([\n  {\n    $lookup: {\n      from: \"UserReply\",\n      let: {\n        id: \"$_id\",\n      },\n      pipeline: [\n        {\n          $match: {\n            $expr: {\n              $eq: [\"$$id\", \"$userReviewId\"],\n            },\n            status: \"APPROVED\",\n          },\n        },\n      ],\n      as: \"replies\",\n    },\n  },\n])\n```\n\nBecause only approved replies are matched within the lookup pipeline the join includes only approved replies in the first place, so no filtering is necessary. Thus the result would be:\n\n```json\n[\n  {\n    \"_id\": 1,\n    \"content\": \"I liked the product\",\n    \"replies\": [\n      {\n        \"_id\": 12,\n        \"content\": \"cool\",\n        \"status\": \"APPROVED\",\n        \"userReviewId\": 1,\n        \"username\": \"Bob\"\n      }\n    ],\n    \"username\": \"John\"\n  }\n]\n```\n\nHowever, if the aggregation were:\n\n```js\ndb.UserReview.aggregate([\n  {\n    $lookup: {\n      from: \"UserReply\",\n      localField: \"_id\",\n      foreignField: \"userReviewId\",\n      as: \"replies\",\n    },\n  },\n  {\n    $match: {\n      \"replies.status\": \"APPROVED\",\n    },\n  },\n])\n```\n\na user review would be matched with all the replies where `userReviewId` equals to the `_id` of the user review and the `replies` field contains at least 1 approved reply. In such case `$filter` would need to be used in projection stage.\n"},{"slug":"from-java-to-kotlin","category":"blog","title":"From Java To Kotlin Or 13 Features Which Make Kotlin Developers Happy","description":"Learn about Kotlin main features and why it makes sense to switch from Java to Kotlin.","tags":["kotlin","java","productivity"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n    <img src=\"/images/blog/kotlin_logo.svg\"\n    alt=\"Kotlin Logo\"\n    style=\"margin:0;\"\n    />\n</div>\n\n1. [What Is Kotlin?](#what-is-kotlin)\n2. [Productivity and Developer Experience](#productivity)\n    1. [Null Safety](#null-safety)\n    2. [Elvis Operator](#elvis-operator)\n    3. [String Interpolation](#string-interpolation)\n    4. [Destructuring](#destructuring)\n    5. [Method Default Arguments](#method-default-arguments)\n    6. [Named Method Arguments](#named-default-arguments)\n    7. [Automatic Initialization of Class Properties](#automatic-class-props-init)\n    8. [Copy Function](#copy-function)\n    9. [Extensions](#extensions)\n    10. [Type Aliases](#type-aliases)\n    11. [Readable Test Names](#readable-test-names)\n    12. [Simple Builder Pattern Implemention](#simple-builder-pattern-implementation)\n3. [Concurrency](#concurrency)\n    1. [Coroutines and Channels](#coroutines-and-channels)\n    2. [Handling CompletableFuture](#handling-completablefuture)\n    3. [Does Project Loom Solve Java Concurrency?](#project-loom)\n4. [Bonus](#bonus)\n\n### <a name=\"what-is-kotlin\"></a>What Is Kotlin?\n\n[Kotlin](https://kotlinlang.org) is a JVM-based language developed by [JetBrains](https://www.jetbrains.com/).\nAccording to JetBrains it's:\n>A modern programming language that makes developers happier.\n\n#### Fun facts:\n- Open source.\n- Recommended by Google for building Android apps (Android mobile development has been Kotlin-first since Google I/O in 2019).\n- At the time of the writing there're 84k Stackoverflow questions tags with Kotlin vs 67k Golang vs 1.8 mil Java.\n- First-class support since Spring 5 (choose Kotlin as the language in the [Spring initilizer](https://start.spring.io/)).\n- Full interoperability with Java: call Kotlin code from Java files and vice versa.\n- Compiling Kotlin to Java bytecode is very easy: just add Kotlin compiler plugin (via Maven or Gradle).\n- Compiles to Java 8 compatible bytecode by [default](https://kotlinlang.org/docs/faq.html#which-versions-of-jvm-does-kotlin-target).\n\n#### Uses:\n- For developing Android applications.\n- For developing serverside applications.\n- [Scripting](https://kotlinlang.org/docs/command-line.html#run-scripts).\n- [Data science](https://kotlinlang.org/docs/data-science-overview.html) e.g. [Jupyter Notebooks](https://kotlinlang.org/docs/data-science-overview.html#jupyter-kotlin-kernel).\n\nOverall, Kotlin is a modern language which tries to simplify common actions which usually result in a lot of Java boilerplate.\nIts approach to concurrency is a breath of fresh air compared to Java `CompletableFuture`. Below I'll showcase the main features which in my opinion make Kotlin a game-changer in terms of productivity, developer experience and concurrency.\n\n### <a name=\"productivity\"></a>Productivity and Developer Experience\n#### <a name=\"null-safety\"></a>Null Safety\nIn Kotlin null safety in built into the language. A variable must be declared as nullable or non-nullable:\n```kotlin\nvar nullableStr: String? = null\nvar nonNullableStr: String = \"abc\"\n```\n\nWhen a method is invoked on a nullable variable, safe call operator (`?`) is used as opposed to the verbose `Optional` in Java:\n```kotlin\nprintln(nullableStr?.length)\n```\nwhich will return the string length if the string is not null or `null` otherwise. Imagine an object which has a hierarchy of at least 2 levels of nullable fields. In Kotlin you could check for nullness as easily as `myObject?.fieldA?.fieldB == null` as opposed to multiple `Optional`s in Java.\n\n#### <a name=\"elvis-operator\"></a>Elvis Operator\nA default value can be provided in case a variable is null as follows:\n```kotlin\nvar nullableStr: String? = null\nprintln(nullableStr ?: \"default str\")\n```\nA default value can even be an exception:\n```kotlin\nval result = getResult() ?: throw RuntimeException(\"could not get a result\")\n```\n\n#### <a name=\"string-interpolation\"></a>String Interpolation\n```kotlin\nval name = \"Paul\"\nval lastName = \"McCartney\"\nprintln(\"Full name: $name $lastName\")\n```\n#### <a name=\"destructuring\"></a>Destructuring\nAnother handy feature which will look familiar to Javascript users:\n```kotlin\ndata class Person(val name: String, val lastName: String)\nval musician = Person(\"Mark\", \"Knopfler\")\nval (name, lastName) = musician\nprintln(\"Full name: $name $lastName\")\n```\nHere, after defining a `Person` class we extract its fields using destructuring. Values from arrays can be extracted in a similar fashion:\n```kotlin\nval lotteryNumbers = listOf(42, 7, 5)\nval (firstNum) = lotteryNumbers\nprintln(\"first number: $firstNum\")\n```\n\n#### <a name=\"method-default-arguments\"></a>Method Default Arguments\nKotlin allows to provide default arguments to a method:\n```kotlin\nfun read(\n    permissions: String,\n    fileName: String = \"/home/default-file\"\n) {}\n```\nIn this example, the default value for `fileName` argument is `\"/home/default-file\"`.\n\n#### <a name=\"named-default-arguments\"></a>Named Method Arguments\nWhen calling a method we can refer to its arguments by their name:\n```kotlin\nfun read(\npermissions: String,\nfileName: String = \"/home/default-file\"\n) {}\n\nread(permissions = \"rw\", fileName = \"./data\") // use arguments by name\nread(\"rw\", \"./data\") // don't use arguments by name\n```\nThis feature may be handy if your method receives several arguments of the same type to prevent supplying value for argument `x` to argument `y` by mistake.\n\n#### <a name=\"automatic-class-props-init\"></a>Automatic Initialization of Class Properties\nClass fields are automatically initialized for primary constructors:\n```kotlin\nclass Person(val name: String, val lastName: String) {\n\n}\n\nfun main() {\n    val billy = Person(\"Billy\", \"Joel\")\n    println(billy.name)\n}\n```\n\n#### <a name=\"copy-function\"></a>Copy Function\nTraditionally, in Java copy constructor needs to be defined in order to copy an object instance. In Kotlin each class has a built-in `copy()` method which copies fields (references are shallow cloned):\n```kotlin\ndata class Person(val name: String, val lastName: String)\nval musician = Person(\"Kurt\", \"Cobain\")\nval anotherMusician = musician.copy()\n\nprintln(musician.equals(anotherMusician))\nprintln(musician === anotherMusician)\n```\nThe first `println` prints `true` since the objects values are the same. The second `println` prints `false` since the references of the objects are not the same.\n\n#### <a name=\"extensions\"></a>extensions\nIt's very common to have a util class which has a bunch of static helper methods. Kotlin allows to define extension functions on a class and then invoke the function directly on an object instance.\nBelow an extension function `isUrl()` is defined on `String` class:\n```kotlin\nimport java.net.URL\nimport java.net.MalformedURLException\n\n// Checks if a string is a valid URL\nfun String.isUrl(): Boolean {\n    var isUrl = false\n\n    try {\n        URL(this)\n        isUrl = true\n    } catch (exception: MalformedURLException) {\n\n    }\n    return isUrl\n}\n\nfun main() {\n    val maybeUrl = \"https://www.google.com\"\n    println(maybeUrl.isUrl())\n}\n```\n\n#### <a name=\"type-aliases\"></a>Type Aliases\nAliases can be defined for classes. This is useful when the object type is either verbose or an alias can better describe it:\n```kotlin\n// key - restaurant name\n// value - pair: left - user rating, right - restaurant critic rating\ntypealias RestaurantRatings = MutableMap<String, Pair<Int, Int>>\n\ntypealias UrlStr = String\n\nfun buildRatings(): RestaurantRatings {\n    val map = mutableMapOf<String, Pair<Int, Int>>()\n    map.put(\"Nono-Mimi\", Pair(5, 4))\n    return map\n}\n\nfun makeRequest(address: UrlStr, accessToken: String) {\n // do something\n}\n\nfun main() {\n    val ratings: RestaurantRatings = buildRatings()\n    println(ratings)\n\n    makeRequest(\"http://google.com\", \"token\")\n}\n```\nIn this example using `RestaurantRatings` alias better describes the type and we don't have to type out `MutableMap<String, Pair<Int, Int>>` each time.\nAlso `UrlStr` better describes a URL than simply a string.\n\n#### <a name=\"readable-test-names\"></a>Readable Test Names\nFunctions in Kotlin can have space characters. This allows to give descriptive names to test functions instead of using `@DisplayName` as in JUnit for example:\n```kotlin\nclass PersonTest {\n\n    @Test\n    fun `person is successfully created`() {\n\n    }\n}\n```\n\n#### <a name=\"simple-builder-pattern-implementation\"></a>Simple Builder Pattern Implemention\n[Builder pattern](https://www.digitalocean.com/community/tutorials/builder-design-pattern-in-java) is a very popular design pattern in Java\nhowever it's notoriously verbose: same fields, getters/setters needs to declared twice, first in the target class, secondly in the builder class.\nThe implementation of the pattern in Kotlin is much more [concise](https://www.baeldung.com/kotlin/builder-pattern#kotlin-style):\n```kotlin\nclass FoodOrder private constructor(\nval bread: String?,\nval condiments: String?,\nval meat: String?,\nval fish: String?) {\n\n    data class Builder(\n    var bread: String? = null,\n    var condiments: String? = null,\n    var meat: String? = null,\n    var fish: String? = null) {\n\n        fun bread(bread: String) = apply { this.bread = bread }\n        fun condiments(condiments: String) = apply { this.condiments = condiments }\n        fun meat(meat: String) = apply { this.meat = meat }\n        fun fish(fish: String) = apply { this.fish = fish }\n        fun build() = FoodOrder(bread, condiments, meat, fish)\n    }\n}\n\nfun main() {\n    val foodOrder = FoodOrder.Builder()\n    .bread(\"white bread\")\n    .meat(\"bacon\")\n    .condiments(\"olive oil\")\n    .build()\n\n    println(foodOrder.bread)\n}\n```\n\n### <a name=\"concurrency\"></a>Concurrency\n#### <a name=\"coroutines-and-channels\"></a>Coroutines and Channels\nKotlin concurrency model is based on the concept of [communicating sequential processes](https://en.wikipedia.org/wiki/Communicating_sequential_processes#:~:text=In%20computer%20science%2C%20communicating%20sequential,on%20message%20passing%20via%20channels.) (CSP) introduced by Tony Hoare.\nIt uses channels and coroutines (like Go). The motto is:\n>Do not communicate by sharing memory; instead, share memory by communicating.\n\nKotlin uses coroutines which are light-weight threads as well as [structured concurrency](https://kotlinlang.org/docs/coroutines-basics.html#structured-concurrency):\n>An outer scope cannot complete until all its children coroutines complete. Structured concurrency also ensures that any errors in the code are properly reported and are never lost.\n\nCoroutines are created by adding `suspend` keyword before a function definition. Coroutines are not part of the language but rather are implemnted in `kotlinx.coroutines` package which can be imported as a [dependency](https://mvnrepository.com/artifact/org.jetbrains.kotlinx/kotlinx-coroutines-core).\n\n#### Coroutines Are Executed Sequentially\n```kotlin\nimport kotlinx.coroutines.*\nimport kotlinx.coroutines.channels.Channel\nimport kotlin.system.measureTimeMillis\n\nsuspend fun doSomethingUsefulOne(): Int {\n    delay(1000L) // pretend we are doing something useful here\n    println(\"doSomethingUsefulOne\")\n    return 13\n}\n\nsuspend fun doSomethingUsefulTwo(): Int {\n    delay(5000L) // pretend we are doing something useful here, too\n    println(\"doSomethingUsefulTwo\")\n    return 29\n}\n\n// coroutines are executed sequentially\nsuspend fun sequentialSum() {\n    doSomethingUsefulOne()\n    doSomethingUsefulTwo()\n    println(\"done\")\n}\n\nfun demoCoroutines() {\n    runBlocking {\n        sequentialSum()\n    }\n}\n\nfun main() {\n    demoCoroutines()\n}\n```\nIn the example below first `doSomethingUsefulOne()` is executed then `doSomethingUsefulTwo()` is. Please note the simplicity of use, there's no awaiting!\nIn case functions don't depend on each other they can be executed concurrently:\n```kotlin\nsuspend fun concurrentSum() = coroutineScope {\n    val one = async { doSomethingUsefulOne() }\n    val two = async { doSomethingUsefulTwo() }\n    println(\"The answer is ${one.await() + two.await()}\")\n}\n```\n`await()` is used in order to wait for the result of both functions. Try/catch can be used for error handling. This is in stark contract to Java `CompletableFuture` where `handlyAsync` would have to be used not to mention the family of `thenXXX()` functions like `thenApply()` etc. which break normal coding flow.\n\nExecuting coroutines concurrently and awaiting their result is easy:\n```kotlin\nsuspend fun asyncLoop() = coroutineScope {\n    val asyncResults = (0..100).map {\n        async {\n            delay(1000L)\n            println(\"asyncLoop iteration number: $it\")\n        }\n    }\n    asyncResults.awaitAll()\n}\n```\n\nAnother great concurrency feature of Kotlin is channels (will look familiar to Golang users). Channels are great for implementing consumer/producer patterns as well as to prevent synchronization issues with shared state.\nFor example, let's take a look at the example below:\n```kotlin\nsuspend fun channelSharedStateDemoBad() {\n    var counter = 0\n    withContext(Dispatchers.Default) {\n        val n = 10000  // number of coroutines to launch\n        val time = measureTimeMillis {\n            coroutineScope { // scope for coroutines\n                repeat(n) {\n                    async {\n                        counter++\n                    }\n                }\n            }\n        }\n        println(\"Completed $n actions in $time ms\")\n    }\n    println(\"Counter = $counter\")\n}\n```\n`counter` which is shared state among coroutines is modified concurrently without being synchronized which almost surely will result in incorrect count at the end.\nOn the other hand using channels doesn't require synchronization because channel throughput is one message by default so only one consumer can process a message at a time.\n```kotlin\nsuspend fun channelSharedStateDemoGood() = coroutineScope {\n    var counter = 0\n    val upperBound = 10000\n    val sumChannel = Channel<Int>()\n        (1..upperBound).map {\n            async {\n                println(it)\n                sumChannel.send(1)\n            }\n        }\n        repeat(upperBound) {\n            val number = sumChannel.receive()\n            counter += number\n\n        }\n        println(\"counter: $counter\")\n    }\n```\n\n#### <a name=\"handling-completablefuture\"></a>Handling CompletableFuture\nIf Java methods returning `CompletableFuture` are called then official Kotlin [dependency](https://mvnrepository.com/artifact/org.jetbrains.kotlinx/kotlinx-coroutines-jdk8/1.6.4) `kotlinx-coroutines-jdk8` can be installed in order simply use `.await()` on `CompletableFuture`s to wait for their completion.\n\nExample Java code:\n```java\npublic class Fetcher {\n    public CompletableFuture<Integer> fetchMyLuckyNumber() {\n        // do some work\n        return CompletableFuture.completedFuture(5);\n    }\n}\n```\n\nExample Kotlin code which calls `fetchMyLuckyNumber()`:\n```kotlin\nsuspend fun handleLuckyNumber() {\n    val luckyNumber = Fetcher().fetchMyLuckyNumber().await()\n}\n```\n\n#### <a name=\"project-loom\"></a>Does Project Loom Solve Java Concurrency?\nSome may ask whether the upcoming [Project Loom for Java](https://blogs.oracle.com/javamagazine/post/java-loom-virtual-threads-platform-threads) solves concurrency issues in Java.\nWhile it does offer [virtual threads](https://docs.oracle.com/en/java/javase/19/docs/api/java.base/java/lang/Thread.html#startVirtualThread(java.lang.Runnable)) as well as structured concurrency, it forces to write code using Java `Runnable`s which are essentially callbacks:\n```java\nThread.startVirtualThread(() -> {\n    // code to be executed asynchronously\n});\n```\nThe problem with this approach is that nested callbacks usually lead to [callback hell](https://stackoverflow.com/a/25098230) in the worst case scenario and not very readable code in the best case.\nIn addition, using lambda functions in Java usually introduces [issues](https://stackoverflow.com/a/34866174) when using non-effectively final variables.\nLastly, this feature was released in Java 19 and is still in preview mode.\n\n\n### <a name=\"bonus\"></a>Bonus\n1. Official Kotlin [playground](https://play.kotlinlang.org/).\n2. Official language resources:\n    1. [Kotlin basics](https://kotlinlang.org/docs/basic-syntax.html)\n    2. [Hands-on coroutines tutorial](https://play.kotlinlang.org/hands-on/Introduction%20to%20Coroutines%20and%20Channels/01_Introduction)\n    3. [Comparison of asynchronous programming techniques](https://kotlinlang.org/docs/async-programming.html)\n2. When copying Java code into a Kotlin file IntelliJ will automatically convert a Java code to Kotlin.\n3. A great article [_What Color is Your Function?_](https://journal.stuffwithstuff.com/2015/02/01/what-color-is-your-function/) comparing concurrency implementation in different languages."},{"slug":"get-js-objects-keys-as-strings-with-proxy-api","category":"blog","title":"How To Programmatically Get Javascript Objects Key Names Using Proxy API","description":"How To Programmatically Get Javascript Objects Key Names Using Proxy API","tags":["javascript","objects"],"body":"\nI find myself frequently iterating over some Javascript object keys and then performing specific actions which are relevant for some key. For example, imagine there's a website which aggregates various ratings of TV shows. The website wants to display all ratings of a particular show but the font size of IMDB rating should be 24 while all other ratings font size should be 16. Let's say the website uses [React.js](https://reactjs.org/) as their frontend framework:\n\n```jsx\nconst theOffice = {\n  imdbRating: 8.9,\n  usersRating: 10,\n  criticsRating: 7,\n}\n\nconst Rating = ({ tvShow }) => (\n  <>\n    {Object.keys(tvShow).map(rating => (\n      <div key={rating} style={{ fontSize: rating === \"imdbRating\" ? 24 : 16 }}>\n        {rating} - {tvShow[rating]}\n      </div>\n    ))}\n  </>\n);\n\n// The `Rating` component is used as follows:\n<Rating tvShow={theOffice}> />\n```\n\nI personally don't like how `imdbRating` is a sting literal in the code (especially if there're dozens of keys and many of them require specific handling). So there're 2 conventional approaches:\n\n1. Create a `constants` object which will contain all string literals used in the code.\n2. Create another object where each key is the same as the original object key. Each key value provides data specific to the key for example returning the font size. Something like:\n\n```js\nconst tvShowFieldsActions = {\n    imdbRating: {\n      fontSize: 24,\n      // other details\n    },\n    usersRating: {\n      fontSize: 16\n    }\n    criticsRating: {\n      fontSize: 16,\n    }\n}\n```\n\nBoth options are not great because they require creating additional objects, re-typing the keys of the original object and updating the additional objects every time the structure of the original object changes. If only there was a way to get an object's key name as a string programmatically...\n\nWell, there's such a way indeed! Fortunately, Javascript has `Proxy` objects which can do exactly what I wanted.\n\n> A [Proxy](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy) is created with two parameters: a target which is the original object which you want to proxy and the handler which is an object that defines which operations will be intercepted and how to redefine intercepted operations.\n\nLet's see an example with the abovementioned `theOffice` object:\n\n```js\nconst proxy = new Proxy(theOffice, {\n  get: function(originalObject, objectKey) {\n    return \"That's what she said!\"\n  },\n})\n\nconsole.log(\"theOffice.imdbRating\", theOffice.imdbRating) // logs 8.9\nconsole.log(\"proxy.imdbRating\", proxy.imdbRating) // logs \"That's what she said!\"\n```\n\nAs you can see we've overridden the usual behavior of `theOffice` fields because when accessing `imdbRating` on the proxy object we're not receiving a number. However the original object continues to behave as usual, returning the actual rating.\n\nNotice that the second parameter to the proxy get handler is `objectKey` which is none other than the key name. Therefore we can re-write the handler:\n\n```js\nconst proxy = new Proxy(theOffice, {\n  get: function(originalObject, objectKey) {\n    return objectKey\n  },\n})\n\nconsole.log(proxy.imdbRating) // logs \"imdbRating\"\n```\n\nNow we have a simple way to create a proxy object of the original and automatically get the names of each key by simply accessing it on the proxy object. Also if new fields are added to the original object the proxy object will be aware of that and still provide the names of new fields (that's why `Proxy` API can be used to create [observables](https://github.com/indiejs/structures)).\n\nFinally the above example with `Rating` component can be updated:\n\n```jsx\nconst theOffice = {\n  imdbRating: 8.9,\n  usersRating: 10,\n  criticsRating: 7,\n}\n\nconst proxy = new Proxy(theOffice, {\n  get: function(originalObject, objectKey) {\n    if (originalObject.hasOwnProperty(objectKey)) {\n      return objectKey\n    }\n    throw new Error(`The field '${objectKey}' doesn't exist.`)\n  },\n})\n\nconst Rating = ({ tvShow }) => (\n  <>\n    {Object.keys(tvShow).map(rating => (\n      <div\n        key={rating}\n        style={{ fontSize: rating === proxy.imdbRating ? 24 : 16 }}\n      >\n        {rating} - {tvShow[rating]}\n      </div>\n    ))}\n  </>\n)\n```\n\nI personally think it's a really cool way to dynamically retrieve object key names!\n"},{"slug":"graphql-request-resolution-in-apollo-server-deep-dive","category":"blog","title":"Deep Dive into GraphQL Request Resolution in Apollo Server","description":"The post explains the details of how Apollo server resolves graphql requests and how graphql resolvers middleware is executed.","tags":["graphql","apollo-server","deep-dive"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/GraphQL_Logo.svg\"\n            alt=\"GraphQL Logo\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 64px;padding-left:16px;padding-right:16px;\">+</span>\n    <div style=\"width:30%;\">\n    <img src=\"/images/blog/apollo.png\"\n        alt=\"Apollo Logo\"\n        />\n    </div>\n</div>\n\nI often wanted to better understand the inner workings of [Apollo Server](https://www.apollographql.com/docs/apollo-server/). Specifically, how the resolvers integrate with Apollo because they're not part of Apollo per se but are either written manually or produced automatically by other tools (for example [prisma.io](https://www.prisma.io/) or [graphql-compose](https://graphql-compose.github.io/)). First, I will describe the process of request resolution and then I will give an example of how one may run into problems if the process is not understood clearly.\n\nAs usual everything starts with the request from the client which contains a query, mutation or a subscription. For the purposes of the article let's suppose it's a query. The central Apollo function which handles requests is `processGraphQLRequest` ([here](https://github.com/apollographql/apollo-server/blob/8dd114455925dfe177dfbfb21449bdfc658aaf1e/packages/apollo-server-core/src/requestPipeline.ts#L107)). In it Apollo parses the GraphQL query, validates it and registers hooks, if provided under [plugins](https://www.apollographql.com/docs/apollo-server/integrations/plugins/) option in `ApolloServer` [config](https://www.apollographql.com/docs/apollo-server/api/apollo-server/). Then Apollo triggers request execution in the `execute` [function](https://github.com/apollographql/apollo-server/blob/8dd114455925dfe177dfbfb21449bdfc658aaf1e/packages/apollo-server-core/src/requestPipeline.ts#L449). `execute` receives all the needed information that the GraphQL executor requires in order to return result, including the schema, the query itself, variable names and forwards the information to the executor which is where query resolution actually happens. Not surprisingly the default executor used is [graphql](https://www.npmjs.com/package/graphql) npm package but you can configure your own executor, which is not mentioned in Apollo [API reference](https://www.apollographql.com/docs/apollo-server/api/apollo-server/) but is nonetheless [possible](https://github.com/apollographql/apollo-server/blob/21651bd4ae00b5aade89831ab67a00e0e7094bd6/packages/apollo-server-core/src/types.ts#L53).\n\nIt's worth noting that the schema parameter forwarded to the executor will already contain all the resolvers under `schema._queryType._fields`. [graphql](https://www.npmjs.com/package/graphql) package starts the resolution of all fields: it invokes all of the resolvers you defined that are relevant for the query. The central [function](https://github.com/graphql/graphql-js/blob/278bde0a5cd71008452b555065f19dcd1160270a/src/execution/execute.js#L344) here is `executeOperation`. Eventually `resolveField` is called which does 2 things:\n\n1. Resolves the top-level resolver via `resolveFieldValueOrError`.\n2. Calls `completeValueCatchingError` which in its turn calls `completeValue` [which](https://github.com/graphql/graphql-js/blob/278bde0a5cd71008452b555065f19dcd1160270a/src/execution/execute.js#L802) recursively resolves the inner fields by calling their respective resolvers if any. Finally, the resulting data is returned to Apollo.\n\nIt's important to note that **resolver middleware** of the parent field is called before the resolvers of child fields as is the case of course with the resolvers themselves.\n\nThere're several ways of changing the resulting data before it is returned to the client, perhaps the easiest is by [using](https://www.apollographql.com/docs/apollo-server/api/apollo-server/) `formatResponse(result, ApolloCtx)` function in `ApolloServer` options.\n\n> The important conclusion of what is described above is that you can't intercept the complete query result inside parent resolver middleware.\n\nConsider this example, with the following schema for an app which allows users to make reviews and reply to the reviews (of course in reality there would be many more fields):\n\n```graphql\ntype Review {\n  id\n  content\n}\n\ntype Reply {\n  id\n  reviewId\n  content\n}\n```\n\nSuppose you define separate resolvers for `Review` and `Reply`. [prisma.io](https://www.prisma.io/) or [graphql-compose](https://graphql-compose.github.io/) can be used to provide such resolvers out of the box with support for sorting and filter arguments. Then, you would define a relation between `Review` and `Reply` so that you can query for all reviews populated with replies and sorted in ascending chronological order as follows:\n\n```graphql\nreviews(sort: ASCENDING_ORDER) {\n    content\n    replies {\n        content\n    }\n}\n```\n\nFinally, suppose you need to re-order some elements: perhaps all reviews should be ordered chronologically, but the review with the most replies should be the one on top. The first impulse some may have (I certainly did! &#128578;) is to use some resolver middleware on `Review` resolver in order to receive all of the reviews, find the review with most replies and move it on top of resulting array. For example, this can be done like so:\n\n```js\nimport { applyMiddleware } from \"graphql-middleware\"\nconst { ApolloServer } = require(\"apollo-server-express\")\nimport { schema } from \"./someSchema\"\nconst reviewsMiddleware = {\n  Query: {\n    reviews: async (resolve, parent, args, context, info) => {\n      const reviews = await resolve(parent, args, context, info)\n      // perform custom logic\n      return reviews\n    },\n  },\n}\n\nconst server = new ApolloServer({\n  schema: applyMiddleware(schema, userReviewsMiddleware),\n  ...otherOptions,\n})\n```\n\nBut this won't work because there're 2 resolvers here: one for `Review` and another for `Reply` and child resolvers are called after parent resolvers (this is how GraphQL works). So when reviews resolver middleware is called reviews will not have the `replies` field resolved yet! Therefore you will not be able to perform custom logic on incomplete result. The proper way to do it would be to use `formatResponse` function which I mentioned above.\n\n---\n\nP.S.: in case you want to debug GraphQL internals you may find it useful to set `schema.polling.enable` to `false` in GraphQL playground in order to concentrate on debugging the query you're interested in and not be disturbed by automatic playground queries.\n"},{"slug":"graphql-resolver-middleware-apollo-server-plugins","category":"blog","title":"GraphQL Resolvers Middleware and Apollo Server Plugins","description":"The post provides examples for possible GraphQL resolvers middleware and explains host to use Apollo Server plugins or extensions.","tags":["graphql","apollo-server","middleware"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/GraphQL_Logo.svg\"\n            alt=\"GraphQL Logo\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 64px;padding-left:16px;padding-right:16px;\">+</span>\n    <div style=\"width:30%;\">\n    <img src=\"/images/blog/apollo.png\"\n        alt=\"Apollo Logo\"\n        />\n    </div>\n</div>\n\nI recently came across a problem where I needed to perform some custom logic on the result of a GraphQL query before it's returned to the client and now I'd like to share some of the possible solutions in [Apollo Server](https://www.apollographql.com/docs/apollo-server/) and [graphql-compose](https://graphql-compose.github.io/).\n\nIt's important to understand whether resolver middleware can be used for the specific task or you will need Apollo Server-level middleware. Keep in mind that resolver middleware is run per resolver while some Apollo Server plugins will be run on the final result before it is sent back to the client. You can learn more on when resolver middleware will not be of help in my previous [post](https://spektor.dev/graphql-resolver-middleware-apollo-server-plugins/).\n\n## Table of Contents\n\n1. [Resolver middleware](#resolver_middleware)\n2. [Response middleware in Apollo Server](#response_middleware)\n\n### Resolver middleware: <a name=\"resolver_middleware\"></a>\n\n1. In graphql-compose you can [use](https://github.com/graphql-compose/graphql-compose-mongoose/blob/980044bcf481f9168ef5938ca0b5fb01abaca978/README.md) `wrapResolve` helper as follows:\n\n```js\nimport { schemaComposer } from \"graphql-compose\"\nschemaComposer.Query.addFields({\n  someResolver: someResolverFunction.wrapResolve(next => async rp => {\n    // do something before the result\n    const result = await next(rp)\n    // do something after the result\n    return result\n  }),\n})\n```\n\n2. You can use `applyMiddleware` utility from [graphql-middleware](https://www.npmjs.com/package/graphql-middleware) from [Prisma Labs](https://www.prisma.io/) like so:\n\n```js\nimport { applyMiddleware } from \"graphql-middleware\"\nconst { ApolloServer } = require(\"apollo-server-express\")\nimport { schema } from \"./someSchema\"\nconst someMiddleware = {\n  Query: {\n    someResolver: async (resolve, parent, args, context, info) => {\n      // do something before the result\n      const result = await resolve(parent, args, context, info)\n      // do something after the result\n      return result\n    },\n  },\n}\n\nconst server = new ApolloServer({\n  schema: applyMiddleware(schema, userReviewsMiddleware),\n  ...otherOptions,\n})\n```\n\n3. `willResolveField` which comes from [graphql-extensions](https://github.com/apollographql/apollo-server/tree/master/packages/graphql-extensions). I couldn't find any documentation for this option but it exists. Better name for the extension would be `didResolveField` since it's called after the field was actually resolved. This is how you can enable it:\n\n```js\nconst { ApolloServer } = require(\"apollo-server-express\")\nconst server = new ApolloServer({\n  extensions: [\n    function middleware() {\n      return {\n        willResolveField(source, args, context, info) {\n          return (error, result) => {\n            if (error) {\n              // do something\n            }\n            // do something\n            return result\n          }\n        },\n      }\n    },\n  ],\n  ...otherOptions,\n})\n```\n\n### Response middleware in Apollo Server: <a name=\"response_middleware\"></a>\n\n1. The easiest option which requires minimal configuration is `formatResponse` which can be enabled like so:\n\n```js\nconst { ApolloServer } = require(\"apollo-server-express\")\nconst server = new ApolloServer({\n  formatResponse: (result, ctx) => {\n    // do something\n    return result\n  },\n  ...otherOptions,\n})\n```\n\n2. Apollo uses [graphql-extensions](https://github.com/apollographql/apollo-server/tree/master/packages/graphql-extensions) package to allow us to hook into many stages of request execution via plugins. There's nice [documentation](https://www.apollographql.com/docs/apollo-server/integrations/plugins/) on this but some options are only mentioned [here](https://github.com/apollographql/apollo-server/blob/ef6e118e11edd51f702b9f74b0bd81142dc44549/packages/graphql-extensions/src/index.ts#L32). It's important to understand that request execution flow starts with `requestDidStart` so even if you're interested in other plugins you still must use `requestDidStart` to hook into the flow. Below is an example of `willSendResponse` where you can change the response:\n\n```js\nconst { ApolloServer } = require(\"apollo-server-express\")\nconst server = new ApolloServer({\n  plugins: [\n    {\n      requestDidStart(request) {\n        return {\n          willSendResponse(result, context) {\n            // do something\n            return {\n              graphqlResponse: result,\n              context,\n            }\n          },\n        }\n      },\n    },\n  ],\n  ...otherOptions,\n})\n```\n\nIn case you're using `willSendResponse` and `formatResponse` simultaneously it's worth noting that `formatResponse` is executed before `willSendResponse`.\n"},{"slug":"how-to-access-and-handle-java-exceptions-using-spring-aop","category":"blog","title":"How To Access and Handle Java Exceptions Using Spring Aspect-Oriented Programming","description":"How To Access Java Exceptions Using Spring AOP, how to handle Java Exceptions, how to catch Java Exceptions, how to centrally access Java exceptions, how to execute custom logic on each Java exception","tags":["java","exception-handling","aop","spring-aop"],"body":"![Spring Framework Logo](/images/blog/spring-logo.svg)\n\n1. [Why Aspect-Oriented Programming?](#motivation)\n2. [Aspect-Oriented Programming Basics](#aop-basics)\n3. [How Spring AOP Works](#how-it-works)\n4. [Exception Handling Implementations Using Spring AOP](#implementations)\n\n<h3 name=\"motivation\">Why Aspect-Oriented Programming (AOP)?</h3> \n\nIn software engineering often times certain logic needs to be applied in multiple modules of an application, e.g. making client requests to a server, database access etc. In object-oriented programming classes are usually used to encapsulate such logic. For example, an `ApiClient` class which multiple modules of an application would use in order to make requests to some API service.\n\nThat being said some application logic may be used so extensively that it becomes extremely repetetive to use such logic each time. Suppose we want to log with debug level the name of each method, its arguments and the return value. We could manually write such code for each and every method but there're a few disadvantages with this approach:\n\n1. A lot of manual, tedious work.\n2. One can forget to log some method.\n3. Code becomes harder to read.\n4. Time-consuming refactors: say message format needs to be changed then the change needs to be performed across all occurrences.\n\nSuch ubiquitous logic which is scattered all over the application is an example of a **cross-cutting concern**: it literally cuts across almost every part of the application. Examples of cross-cutting concerns include logging, exception handling, recording application metrics, tracing etc. [Aspect-oriented programming](https://en.wikipedia.org/wiki/Aspect-oriented_programming) aims to tackle these issues.\n\nSeveral implementations of aspect-oriented programming are available for Java where [AspectJ](https://www.eclipse.org/aspectj/) is the de-facto standard. In addition Java Spring framework [provides](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop) a subset of AspectJ features. In this tutorial I will describe several strategies to access and handle Java exceptions using aspect-oriented prpgramming. This can be handy if exception handling involves repetetive tasks like sending an error metric on each exception or executing other custom exception related logic. If you just want to get error metrics on each exception you may also be interested in Spring actuator [Logback metrics](https://docs.spring.io/spring-boot/docs/2.0.x/reference/html/production-ready-metrics.html#production-ready-metrics-meter). This metric will not include the actual type of exception though.\n\n<h3 name=\"aop-basics\">Aspect-Oriented Programming Basics</h3>\n\nAOP and AspectJ deserve comprehensive study in their own right hence I will give a very brief introduction into the main concepts:\n\n- **Aspect**: logic which cuts across multiple parts of an application (exception handling in our case). Fun fact: transaction management (`@Transactional`) is implemented using AOP in Spring.\n- **Join point**: a point during application execution when a cross-cutting concern occurs e.g. method execution, accessing a class field. **Spring AOP only allows method execution join points**.\n- **Advice**: dictates when aspect logic is invoked e.g. before method execution, after etc.\n- **Pointcut**: a logical condition which needs to be matched so that an aspect is executed. For example, we may want to execute certain logic only for methods of Java package `services`.\n\n<h3 name=\"how-it-works\">How Spring AOP Works</h3> \n\nTo summarize the above Spring AOP can intercept method execution before/after etc. the actual execution. You can also tell it which methods to intercept and what to do following the intercept. How does Spring AOP actually intercept method execution? It does it **at runtime** using [two options](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop-introduction-proxies):\n\n1. Native JDK dynamic proxies if the intercepted method belongs to a class which implements an interface.\n2. CGLIB in all other cases.\n\nIt's important to understand the implications of proxies usage (this should already be familiar to anyone using `@Transactional` annotation in Spring):\n\n1. Only public methods can be intercepted.\n2. Method which is called by another method of the same class will not be intercepted.\n\nYou can read more about Spring AOP proxies [here](https://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop-understanding-aop-proxies). In addition Spring AOP components are regular Spring beans and are annotated with `@Aspect` + `@Component`, `@Service` etc. **This means that aspects can't be applied to classes which are not Spring beans.**\n\nTo [enable](https://docs.spring.io/spring-framework/docs/4.3.15.RELEASE/spring-framework-reference/html/aop.html#aop-enable-aspectj-xml) `@AspectJ` support with Java `@Configuration` add the `@EnableAspectJAutoProxy` annotation. Lastly, make sure that `org.aspectj:aspectjweaver` is on the class path (Maven central [link](https://mvnrepository.com/artifact/org.aspectj/aspectjweaver)).\n\n<h3 name=\"implementations\">Exception Handling Implementations Using Spring AOP</h3> \n\nThe simplest way to access exceptions is to use after throwing advice. Each time a method exits because an exception was thrown the aspect will be invoked. An important caveat to this advice is that if `try/catch` is used the aspect will not be invoked (because the method didn't exit):\n\n```java\npackage com.myapp.mypackage;\n\nimport org.aspectj.lang.JoinPoint;\nimport org.aspectj.lang.annotation.AfterThrowing;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class ExceptionHandlingAspect {\n\n\t@Pointcut(\"within(com.myapp..*) && execution(* *(..))\")\n\tpublic void matchAllMyMethods() {}\n\n\t@AfterThrowing(value = \"matchAllMyMethods()\", throwing = \"exception\")\n\tpublic void doSomethingWithException(JoinPoint joinPoint, Throwable exception) {\n\t\t// get access to the actual exception thrown\n\t\tSystem.out.println(exception);\n\n        // get access to the class name of the method which threw exception\n        String className = joinPoint.getSignature().getDeclaringType().getSimpleName();\n\n        // get access to the method name of the method which threw exception\n        String methodName = joinPoint.getSignature().getName();\n\n        // get access to the arguments of the method which threw exception\n        Object[] methodArgs = joinPoint.getArgs();\n\t}\n}\n```\n\nLet's break down the pointcut expression:\n\n- `within(com.myapp..*)` will match all methods belonging to all the Java packages inside `com.myapp` essentially all methods of an application. If `within` condition wasn't used the aspect would be invoked on Spring beans of other dependencies (e.g. metrics registry beans) so it's a good idea to always limit execution to the methods within our actual application at the very least.\n- `execution(* *(..))` will match method execution join points (the only join point type supported by Spring AOP). Here `* *(..)` refers to a method signature: first `*` matches any method return type, second `*` matches any method name, `..` matches any method arguments. If we wanted to match only a specific method `public int getRandomNumber()` we could use the following pointcut expression: `execution(int getRandomNumber())`. It's worth noting that pointcut syntax is the same as that of AspectJ, [please see AspectJ docs for more examples of pointcut expressions](https://www.eclipse.org/aspectj/doc/released/progguide/language-joinPoints.html).\n\nAs can be seen we can get access to the exception thrown itself and the metadata of the method which threw the exception. We could do something useful like perhaps send a metric with exception/method data.\n\nConsider the following classes in an application with the above aspect:\n\n```java\npublic class UserService {\n    public User getUser() {\n\t\t// simulate exception\n        throw new RuntimeException();\n    }\n}\n\npublic class AuthService {\n    public void handleAuth() {\n        User user = userService.getUser(); // no try/catch\n\t\t// do something\n    }\n}\n```\n\n`getUser` throws an exception so the aspect will be invoked on it. `handleAuth` will also exit when `getUser` is called so the aspect **will be invoked again on the same exception** which may or may not be desired. If certain exception related logic needs to be performed exactly once the state has to be tracked somewhere, perhaps using `ThreadLocal` variable (make sure performance doesn't degrade when using `ThreadLocal` variables). Lastly, aspect logic is executed synchronously in the above example: adding time-consuming calculations in the aspect will negatively affect performance.\n\nNote that the problem with the above approach is that we don't get access to exceptions which were caught inside `try/catch`. This is actually impossible to do in Spring AOP however it is possible to achieve this if plain AspectJ is used. AspectJ uses [complilation](https://www.baeldung.com/aspectj) or [load-time weaving](https://www.eclipse.org/aspectj/doc/released/devguide/ltw-configuration.html) in order to provide AOP functionality which will definitely increase build complexity. If AspectJ is used then `\"handler(*) && args(e)\"` pointcut can be used to access exceptions in `catch` blocks as described [here](https://stackoverflow.com/a/42093318/5863693).\n\nAnother problem with the above approach is that we get access to the exception thrown and its metadata however we can't actually handle or catch the exception: by the time the aspect is invoked the method had already exited. In order to actually catch the exception around advice can be used:\n\n```java\npackage com.myapp.mypackage;\n\nimport org.aspectj.lang.ProceedingJoinPoint;\nimport org.aspectj.lang.annotation.Around;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Pointcut;\nimport org.springframework.stereotype.Component;\n\n@Aspect\n@Component\npublic class ExceptionHandlingAspect {\n\n\t@Pointcut(\"within(com.myapp..*) && execution(* *(..))\")\n\tpublic void matchAllMyMethods() {}\n\n\t@Around(value = \"matchAllMyMethods()\")\n\tpublic Object doSomethingWithException(ProceedingJoinPoint joinPoint) throws Throwable {\n\t\tString className = joinPoint.getSignature().getDeclaringType().getSimpleName();\n\t\tString methodName = joinPoint.getSignature().getName();\n\t\tObject[] methodArgs = joinPoint.getArgs();\n\n\t\ttry {\n\t\t\t// continue the original method execution\n\t\t\treturn joinPoint.proceed();\n\t\t} catch (Exception exception) {\n\t\t\t// custom aspect logic\n\t\t\tthrow exception;\n\t\t} finally {\n\t\t\t// custom aspect logic\n\t\t}\n\t}\n}\n```\nThe above approach uses `ProceedingJoinPoint` which allows to intercept the actual method which throws the exception, catch the exception and do something about it. Then the exception is re-thrown in order to let the original method also deal with the exception. All this makes around advice probably the most powerful advice available in AOP. You can see the full list of advice supported by Spring AOP [here](https://docs.spring.io/spring-framework/docs/4.3.15.RELEASE/spring-framework-reference/html/aop.html#aop-introduction-defn).\n\nEnjoy aspect-oriented programming!\n\n"},{"slug":"how-to-access-host-network-from-docker-on-macos","category":"blog","title":"How To Enjoy Docker For Linux Networking on macOS","description":"How To Enjoy Docker For Linux Networking on macOS Or How To Set Up Simple Linux VM on Mac","tags":["docker","linux","vm"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;padding-bottom: 120px;\">\n    <div style=\"width:20%;\">\n        <img src=\"/images/blog/docker.svg\"\n            alt=\"Docker Logo\"\n            style=\"margin:0;\"\n            />\n    </div>\n    <div style=\"width:20%;padding-left: 16px;\">\n    <img src=\"/images/blog/linux.png\"\n        alt=\"Linux Logo\"\n        /></div>\n        <span style=\"font-size: 32px;padding-left:16px;padding-right:16px;\">macOS</span>\n    </div>\n    \n</div>\n\nI recently came across this really cool project called [lima](https://github.com/AkihiroSuda/lima). This is the project description as per the project itself:\n\n> Lima launches Linux virtual machines with automatic file sharing, port forwarding, and containerd.\n\nLet's break the description apart:\n\n1. \"automatic file sharing, port forwarding\" - this is very handy if for example you want to test your app build on Linux but your main development environment is on another OS, like macOS. This is also handy if some actions can be more easily done on Linux ([more on that later](#moreonthat)).\n\n2. [containerd](https://github.com/containerd) is an industry-standard container runtime. It's used by Docker and Kubernetes in order to push/pull images, manage storage, networking etc. In particular if you take a look at containerd [nerdctl](https://github.com/containerd/nerdctl#whale-nerdctl-run) command you will notice almost identical API to the familar Docker CLI commands (e.g. `run`, `build`).\n\nThe first point is achieved by using ssh for port forwarding and file sharing ([link](https://github.com/AkihiroSuda/sshocker)).\n\nLima is very easy to install following the [instructions](https://github.com/AkihiroSuda/lima). Once it's installed first you need to start lima:\n\n```bash\nlimactl start\n```\n\nNow any given command `x` that you wish to run on Linux, can be run from Mac by adding `lima` prefix. For example running the below command from Mac termianl:\n\n```bash\nlima uname -a\n```\n\nreturns the ouput `Linux lima-default 5.11.0-17-generic #18-Ubuntu SMP Thu May 6 20:10:11 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux`. In addition running the command `lima bash` opens a Linux terminal (if you're using Bash). I used to have a [VirtualBox](https://www.virtualbox.org) instance on my Mac specifically so that I can run Linux but Lima is so much better! Configuring file sharing in VirtualBox is a big deal while in Lima you get it out of the box with zero configuration (of course if you need Linux GUI then Lima is not a real substitute for VirtualBox but in my case I don't need a Linux GUI).\n\n<a name=\"moreonthat\"></a>A quite interesting application of Lima can be simplified workflow with (Docker) containers. For example, suppose you're developing some service which accesses a server or database, and the service is run inside a container while the server or database run on host OS. In order to access the host server the container would need to be run with `--net=host` option. This will work on Linux but because Docker runs on top of a virtual machine on macOS, `--net=host` option will solely expose the virtual machine ports but that's not what you really want. For development purposes containers can refer to the `localhost` of the host network via `host.docker.internal` address. However, this often requires code changes in the service itself which is not ideal.\n\nUsing Lima can solve this problem because containers can be spun off on Linux via Lima and thus be able to use `--net=host` option. Below is the demonstration:\n\n1. `lima nerdctl run --rm -it -p 8080:80 nginx:alpine` - this opens an nginx server on Linux host port `8080`.\n2. In order to simulate Linux host port access from within a container the following Dockerfile can be used:\n\n```docker\nFROM alpine\nRUN apk --no-cache add curl\nCMD curl localhost:8080\n```\n\n3. `lima nerdctl build -t curltest .` - build the test container.\n4. `lima nerdctl run --rm -it --net=host curltest:latest` - runs the container and outputs the html from nginx.\n\nIn conclusion, I think Lima is a great project which provides headless Linux with simple file-sharing and containerd service.\n"},{"slug":"how-to-add-sql-constraint-without-applying-it-to-existing-rows","category":"blog","title":"How To Add SQL Constraint Without Applying It To The Existing Rows in PostgreSQL","description":"How to Add New SQL Constraint Without Applying It to the Existing Rows in PostgreSQL. How to apply SQL constraint only to future insertions or updates on rows. How to defer SQL constraint evaluation.","tags":["sql","postgresql","constraint","tip"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n    <img style=\"min-width:100px;max-width:200px\" src=\"/images/blog/postgresql_logo.svg\"\n            alt=\"Postgres Logo\"\n            style=\"margin:0;\"\n            />\n</div>\n\nOften times product requirements change and software engineers need to change the existing data model. For example, imagine an intentionally simplistic Postgres table for book orders which contains such details as order id, and order quantity (number of book copies for the order). One day the company decides to add expedited shipping option. Now the order table should have another column `shipping_type` which can be `standard` or `expedited`. Suppose the table has millions of rows and the company doesn't care about the shipping type of old orders. The new column should be not null and have only the two above-mentioned values for now (perhaps in the future there will be a third option). How can the column be added in the most efficient way? Before we consider the available options let's create the `orders` table:\n\n```sql\nCREATE TABLE IF NOT EXISTS orders (\n  id BIGSERIAL PRIMARY KEY,\n  quantity SMALLINT\n);\n\nINSERT INTO orders(quantity)\nVALUES (2);\n```\n\n#### Option 1: Add Not Null Constraint with Default Value\n\nRemember the old rows don't have the `shipping_type` column and because the new column needs to be added with a not null constraint some default value needs to be given for the column in the old rows. For example, their `shipping_type` can be `standard`:\n\n```sql\nALTER TABLE orders\nADD COLUMN IF NOT EXISTS shipping_type varchar(9) NOT NULL DEFAULT 'standard';\n```\n\nThis works but what happens behind the scenes? Depending on the character set defined in the database a character requires at least 1 byte in memory. In addition, depending on the string length there's an additional [overhead](https://www.postgresql.org/docs/current/datatype-character.html) of 1-4 bytes per row. Because `shipping_type` is `varchar(9)` (there're 9 letters in the word `expedited`) we add 1 more byte of overhead which becomes 10 bytes per row at least. Suppose the database has 1 million rows adding the default value will result in 10 MB additional storage which is not that bad. In case the database character set was UTF8 up to 4 bytes can be required to store 1 character in which case adding the default value would add `(9 * 4) + 1 = 37` (1 byte for the overhead) bytes per row, that is another 37 MB for a million rows. If there's more than a million rows and columns with default values are occasionally added this can certainly add up (as a sidenote using a Postgres enum could be beneficial in such case as it occupies only [4 bytes](https://www.postgresql.org/docs/current/datatype-enum.html#id-1.5.7.15.8) in memory).\n\nAn even more important concern is a multi-million row update of a production database. Firstly, this may significantly affect database performance and is echoed in Postgres [docs](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-NOTES):\n>Scanning a large table to verify a new foreign key or check constraint can take a long time, and other updates to the table are locked out until the ALTER TABLE ADD CONSTRAINT command is committed.\n\nIndeed, adding a constraint [will result](https://www.postgresql.org/docs/current/sql-altertable.html#SQL-ALTERTABLE-NOTES) in `ACCESS EXCLUSIVE` lock for the table which will block [both updates and reads](https://www.postgresql.org/docs/current/explicit-locking.html)(!) to the table until the constraint is validated.\n\nThere's a better way.\n\n#### Option 2: Adding Constraint Only For Future Rows\n\nAs already mentioned, the company isn't really interested in the old orders `shipping_type`. Is there a way to add the not null constraint in such a way that it's applied only to the future rows and not to the existing rows? Yes there is: [NOT VALID](https://www.postgresql.org/docs/current/sql-altertable.html). Normally, when a new constraint is added the table is scanned to verify that the existing rows satisfy the constraint. However, if `NOT VALID` is applied the full table scan is skipped but the constraint will still be enforced against subsequent inserts or updates. This is a win-win situation where we still get constraint checking on future updates while not locking out the table:\n\n```sql\nALTER TABLE orders\nADD COLUMN IF NOT EXISTS shipping_type varchar(9);\n\nALTER TABLE orders ADD CONSTRAINT shipping_type_not_null CHECK (shipping_type IS NOT NULL) NOT VALID;\n```\n\nIf additional space which would be created by setting a default value to existing rows is not a concern one could write a migration script which would incrementally add default value to the existing rows and then run `ALTER TABLE orders VALIDATE CONSTRAINT shipping_type_not_null;` in order to validate that there aren't any rows left without a default value. This operation acquires `SHARE UPDATE EXCLUSIVE` lock which protects a table against concurrent schema changes (but doesn't block row reads/writes). However, in case of foreign key constraint a ROW SHARE lock is also required on the table referenced by the constraint which can have performance consequences.\n\n**To summarize**: using `NOT VALID` can significantly boost performance when adding new constraint on a Postgres table and even potentially prevent application downtime because of table locking."},{"slug":"how-to-implement-infinite-scroll-in-react","category":"blog","title":"Simple Pagination In React Or How To Implement Infinite Scroll","description":"Simple Pagination In React Or How To Implement Infinite Scroll","tags":["react","infinite-scroll","tutorial"],"body":"\nOften times a website needs to implement some kind of pagination solution, for example if a list of products is displayed and all products can't be shown at once (for performance or other reasons). Firstly, standard button-based pagination can be used (as in Amazon.com for example). However, implementing such pagination can be a bit tricky for the sheer number of edge cases, especially if you want to implement `...` functionality in pagination. That is, say, there're 20 pages in total and you want to display the pagination as follows: `1 2 3 ... 20`.\n\nLuckily, there's a much simpler solution which also has the modern feel: infinite scroll. The way it works is you scroll to the bottom of a page and this triggers loading more items/products in the website. Perhaps the technique is most familiar from Facebook news feed.\n\nImplementing infinite scroll is extremely easy. It all comes down to attaching a handler to `window.onscroll` event:\n\n```js\nimport { useEffect } from \"react\"\nimport debounce from \"lodash/debounce\"\n\nexport const ProductsList = props => {\n  // component business logic\n\n  useEffect(() => {\n    const onscrollHandler = debounce(\n      () => {\n        if (\n          // height of the visible window\n          window.innerHeight +\n            // how far the user has scrolled\n            document.documentElement.scrollTop ===\n          // full height of the window (both visible and not visible)\n          document.documentElement.offsetHeight\n        ) {\n          // onscroll logic (querying for more products perharps)\n        }\n      },\n      100,\n      { leading: true }\n    )\n\n    window.addEventListener(\"scroll\", onscrollHandler)\n    return () => {\n      window.removeEventListener(\"scroll\", onscrollHandler)\n    }\n  }, [])\n\n  return <div>List of products here</div>\n}\n```\n\nIt is recommended to use `debounce` in order to prevent overflooding of scroll events (there's actually a great [article](https://css-tricks.com/debouncing-throttling-explained-examples/) on debouncing events). As you can see the solution involves only 1 `if` block which is pretty neat!\n"},{"slug":"how-to-make-nodejs-request-to-ksqldb","category":"blog","title":"How To Make Requests to ksqlDB in Node.js","description":"How To Make Requests to ksqlDB in Node.js","tags":["ksqlDB","node.js","http2","kafka"],"body":"\n1. [HTTP/1.1 Version](#http1)\n2. [HTTP/2](#http2)\n\n[ksqlDB](https://docs.ksqldb.io/en/latest/) is an amazing tool to make SQL queries on data stored in Kafka topics (and much more). It also has a handy [REST API](https://docs.ksqldb.io/en/latest/developer-guide/api/) to make queries. I didn't find a lot of resources on how to make REST API query requests to ksqlDB in Node.js therefore I thought it'd be useful to write this post.\n\nIt's worth noting that there're 2 main query endpoints in ksqlDB:\n\n- `/query`\n- `/query-stream`\n\nAccording to the official [docs](https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-rest-api/query-endpoint/) `/query` endpoint will be deprecated so it's recommended to use the `/query-stream` alternative. However, the alternative endpoint can only be used over HTTP/2 which is great technology but using it may require some learning curve and a different mental model. The code below was written for Node.js **version 14**.\n\n<a name=\"http1\">HTTP/1.1</a>\n\nIf you just want to quickly try out ksqlDB query API without delving into HTTP/2 below is an example which uses the familiar `fetch` API (`node-fetch@2` npm package is used because version 3 doesn't support CommonJS syntax. CommonJS is used in the tutorial for simplicity):\n\n```js\nconst fetch = require(\"node-fetch\")\nconst KSQLDB_QUERY_ENDPOINT = \"http://localhost:8088/query\"\n\nconst main = async () => {\n  try {\n    const query = {\n      ksql: `SELECT * FROM test_view WHERE your_column='something';`,\n    }\n    const response = await fetch(KSQLDB_QUERY_ENDPOINT, {\n      method: \"POST\",\n      headers: {\n        Accept: \"application/vnd.ksql.v1+json\",\n      },\n      body: JSON.stringify(query),\n    })\n\n    const json = await response.json()\n    console.log(\"result\", json)\n  } catch (error) {\n    consoler.error(error)\n  }\n}\n\nmain()\n```\n\n`EMIT CHANGES` type queries can also be run via this endpoint however they only fetch the state of the table as of the time of the request and do not update on subsequent changes.\n\n<a name=\"http2\">HTTP/2</a>\n\n`/query-stream` endpoint is used over HTTP/2. This allows to receive updates on changes to materialized views as well as run queries with `WHERE` clause. Below is a very basic implementation of ksqlDB client:\n\n```js\nconst http2 = require(\"http2\")\n\nclass KsqlDBClient {\n  constructor(ksqlDBBaseUrl) {\n    this.client = http2.connect(ksqlDBBaseUrl)\n    this.client.on(\"error\", error => console.error(error))\n  }\n\n  request(query) {\n    const session = this.client.request({\n      [http2.constants.HTTP2_HEADER_PATH]: \"/query-stream\",\n      [http2.constants.HTTP2_HEADER_METHOD]: \"POST\",\n      [http2.constants.HTTP2_HEADER_CONTENT_TYPE]:\n        \"application/vnd.ksql.v1+json\",\n    })\n\n    session.setEncoding(\"utf8\")\n    session.on(\"data\", queryResult => {\n      console.log(\"queryResult\", queryResult)\n    })\n\n    const payload = Buffer.from(JSON.stringify(query))\n    session.end(payload)\n  }\n}\n\nconst query = {\n  sql: `SELECT * FROM test_view EMIT CHANGES;`,\n}\nconst client = new KsqlDBClient(\"http://localhost:8088\")\nclient.request(query)\n```\n\nAs can be seen query result is returned on `data` event.\n"},{"slug":"how-to-properly-invalidate-aws-cloudfront-cache","category":"blog","title":"How to Properly Invalidate Files Cached By AWS Cloudfont","description":"How to Properly Invalidate Files Cached By AWS Cloudfont","tags":["aws","cache","cdn","cloudfront"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n    <img src=\"/images/blog/awslogo.svg\"\n            alt=\"Amazon Web Services Logo\"\n            style=\"margin:0;\"\n            />\n</div>\n\nUsing CDN can greatly increase website load times and as a result user experience. [Cloudfront](https://aws.amazon.com/cloudfront/?nc=sn&loc=1) is a powerful CDN solution provided by Amazon Web Services (AWS). You can control origin requests cache by defining which routes (called behaviors in AWS) to cache and whether to cache headers, query parameters etc. And of course you can specify for how long to cache the requests. You can read more on how to set up a distribution and customize behaviors [here](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/controlling-origin-requests.html).\n\nSometimes you will need to reset the cache on certain routes, for instance, after deployment. It is important to keep in mind that if you change the cache TTL setting in a specific behavior this [will not immediately](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/RemovingObjects.html) invalidate the cache.\n\nThe proper way to invalidate a specific route would be to use the `Invalidate` [option](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Invalidation.html) in the AWS console. This will immediately invalidate the files.\n\nIn addition I've found that if you have several behaviors, invalidating the default one (the root) will not invalidate a more specific route. For example, if you have 2 behaviors: `Default (*)` and `api/*` then in order to invalidate the second behavior you will need to enter `api/*` in the invalidate form.\n"},{"slug":"how-to-set-up-dapr-with-dead-letter-queue-in-java","category":"blog","title":"How To Set Up Dapr Pub-Sub With Dead Letter Queue In Java","description":"How To Set Up Dapr Pub/Sub With Dead Letter Queue In Java, dapr pub-sub tutorial, dapr pub-sub intro.","tags":["dapr","pub-sub","dead-letter-queue"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n    <img style=\"min-width:100px;max-width:200px\" src=\"/images/blog/dapr_logo.svg\"\n            alt=\"DAPR Logo\"\n            style=\"margin:0;\"\n            />\n</div>\n\n1. [What is Dapr/Why Dapr?](#why)\n2. [Installing Dapr](#installing-dapr)\n3. [Setting Up Redis Pub/Sub](#redis-pub-sub)\n4. [Setting Up Kafka Pub/Sub](#kafka-pub-sub)\n5. [Bonus](#bonus)\n6. [References](#references)\n\n### <a name=\"why\"></a>What is Dapr/Why Dapr?\n[Dapr](https://dapr.io) is an amazing open-source project which was initially started at Microsoft and is now part of Cloud Native Computing Foundation which abstracts away a lot of the implementation details of various infrastructure technologies when developing your application.\n\nFor example, say you have two microservices and you want to connect them via a message queue. Depending on the queue implementation chosen you will need to develop the necessary infrastructure. This process can be non-trivial if your company or team is small and mainly it distracts from the real purpose: developing the business logic of your application.\n\nDapr provides a runtime and SDK/connectors to such infrastructure technologies and more. For example, using Dapr SDK you can use Redis Streams/Kafka/AWS SQS/Rabbit MQ/Azure Service Bus message queues and many more. In addition to queues Dapr also streamlines secret management, telemetry between services, state management and [much more](https://dapr.io).\n\nIn the tutorial below I will show how to set up a simple pub/sub integration using Dapr first using Redis Streams then using Kafka, with both integrations using [dead letter queues](https://en.wikipedia.org/wiki/Dead_letter_queue) - destinations where messages are sent when they couldn't be processed successfully. The tutorial uses the official Dapr quickstarts [repo](https://github.com/dapr/quickstarts) as the basis.\n\n### <a name=\"installing-dapr\"></a>Installing Dapr\n\n**Prerequisites:** Docker runtime.\n\nDapr can be run in several ways including as a Kubernetes sidecar but for the sake of the tutorial it will be easiest to run it via Docker containers. First, we need to install [Dapr CLI](https://docs.dapr.io/getting-started/install-dapr-cli/). Once the CLI is installed run `dapr init` and refer to the [docs](https://docs.dapr.io/getting-started/install-dapr-selfhost/) for any issues. After running `docker ps` you should see 3 new containers: `dapr_placement` which is the Dapr runtime itself, `dapr_redis` and `dapr_zipkin` for tracing.\n\n### <a name=\"redis-pub-sub\"></a>Setting Up Redis Pub/Sub\nThe next step is to set up a Java Spring Boot project which will use Dapr for pub/sub messaging between its services. You can clone the [repo](https://github.com/yossisp/dapr-pubsub-with-dlq-demo) I specifially created for this tutorial: `git clone https://github.com/yossisp/dapr-pubsub-with-dlq-demo.git` which already contains all the code we'll need. Below is the repo structure:\n```\n.\nâ”œâ”€â”€ components-kafka\nâ”œâ”€â”€ components-redis\nâ”œâ”€â”€ docker-compose.yml\nâ””â”€â”€ java\n```\nwhere components folders contain Dapr settings, `docker-compose.yml` contains Kafka broker + UI tool services and `java` folder contains the business logic code. Business logic applications can interact with Dapr runtime declaratively using Dapr YAML settings files or programmatically. In this tutorial we'll use the declarative approach. Let's take a look at `components-redis` folder:\n```\n.\nâ”œâ”€â”€ dlq-subscription.yaml\nâ”œâ”€â”€ observability.yaml\nâ”œâ”€â”€ pubsub-subscription.yaml\nâ””â”€â”€ pubsub.yaml\n```\n`pubsub.yaml` is a Dapr component of type `pubsub.redis`:\n```yaml\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: orderpubsub\nspec:\n  type: pubsub.redis\n  version: v1\n  metadata:\n  - name: redisHost\n    value: localhost:6379\n  - name: redisPassword\n    value: \"\"\n```\n`pubsub-subscription.yaml` is of type subscription and defines subscription settings:\n```yaml\napiVersion: dapr.io/v1alpha1\nkind: Subscription\nmetadata:\n  name: order\nspec:\n  topic: orders\n  route: /orders\n  pubsubname: orderpubsub\n  deadLetterTopic: deadLetterTopic\n```\nIt defines `orders` stream in Redis (Redis Docker container was created by `dapr init`) whose messages will be delivered to our application via `/orders` server endpoint. That is Dapr will communicate with the message queue while the actual application will receive messages via a REST endpoint. It also defines a dead letter topic called `deadLetterTopic`. Whenever an error will occur when processing some message this message will be forwarded to this topic, more specifically to `/failedMessages` endpoint as defined in `dlq-subscription.yaml`.\n\nOur Java application is composed of two services: checkout and order-processing component. Checkout component generates and publishes messages while order-processing component subscribes to the topics and receives the messages. Below is the order-processing component code:\n```java\npackage com.service.controller;\n\nimport io.dapr.Topic;\nimport io.dapr.client.domain.CloudEvent;\n\nimport lombok.Getter;\nimport lombok.Setter;\nimport org.springframework.http.MediaType;\nimport org.springframework.http.ResponseEntity;\nimport org.springframework.web.bind.annotation.*;\n\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n@RestController\npublic class OrderProcessingServiceController {\n\n    private static final Logger logger = LoggerFactory.getLogger(OrderProcessingServiceController.class);\n    private static final int MESSAGE_NUM_TO_THROW_ON = 2;\n\n    @PostMapping(path = \"/orders\", consumes = MediaType.ALL_VALUE)\n    public ResponseEntity getCheckout(@RequestBody(required = false) CloudEvent<Order> cloudEvent) {\n        try {\n            logger.info(\"Orders subscriber message received: \" + cloudEvent.getData().getOrderId());\n            if (cloudEvent.getData().getOrderId() == MESSAGE_NUM_TO_THROW_ON) {\n                throw new RuntimeException(\"some error\");\n            }\n            return ResponseEntity.ok(\"SUCCESS\");\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n\n    @PostMapping(path = \"/failedMessages\", consumes = MediaType.ALL_VALUE)\n    public ResponseEntity failedMessages(@RequestBody(required = false) CloudEvent<Order> cloudEvent) {\n        try {\n            logger.info(\"failedMessages subscriber message received: \" + cloudEvent.getData().getOrderId());\n            return ResponseEntity.ok(\"SUCCESS\");\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }\n}\n\n@Getter\n@Setter\nclass Order {\n    private int orderId;\n}\n```\nIt's a very simple Spring Boot MVC with two controllers: `/orders` and `/failedMessages`. Checkout service will send 11 messages, on the 3rd message an error will occur inside `/orders` controller which will result in the message being sent to the dead letter queue to `/failedMessages` controller. We'll generate the Java jars and start the services:\n\n- Checkout service\n```bash\ncd java/sdk/checkout\nmvn clean install\ndapr run --app-id checkout --components-path ../../../components-redis -- java -jar target/CheckoutService-0.0.1-SNAPSHOT.jar\n```\n- Order-processing service\n```bash\ncd java/sdk/order-processor\nmvn clean install\ndapr run --app-port 8080 --app-id order-processor --components-path ../../../components-redis -- java -jar target/OrderProcessingService-0.0.1-SNAPSHOT.jar 2>&1 | tee log\n```\n\nThe output of order-processing service is saved into the file `java/sdk/order-processor/log` which we can search for log messages. We can see that each message first arrives to the `/orders` controller via `Orders subscriber message received` logs. And the `failedMessages subscriber message received` log tells us that one message wasn't successfully processed and therefore arrived to `/failedMessages` controller.\n\n--------\n\nSo far great! Using Dapr SDK and runtime we implemented a pub/sub infrastructure backed by Redis Streams in no time ðŸ”¥. On top of that we have dead letter queue functionality without writing a single line of Java code.\n\nNow suppose that our requirements changed and we want to use Kafka as our message queue of choice. Usually, this would result in a time-consuming new integration and possibly code refactor. But using Dapr it's just a matter of creating a new YAML configuration.\n\n### <a name=\"kafka-pub-sub\"></a>Setting Up Kafka Pub/Sub\nIn order to use Kafka for pub/sub we can use the configurations defined under `components-kafka` directory. The component type is now `pubsub.kafka` along with some other Kafka-specific settings as can be seen in `pubsub.yaml`:\n```yaml\napiVersion: dapr.io/v1alpha1\nkind: Component\nmetadata:\n  name: orderpubsub\nspec:\n  type: pubsub.kafka\n  version: v1\n  metadata:\n    - name: brokers # Required. Kafka broker connection setting\n      value: \"localhost:9092\"\n    - name: consumerGroup # Optional. Used for input bindings.\n      value: \"group1\"\n    - name: clientID # Optional. Used as client tracing ID by Kafka brokers.\n      value: \"my-dapr-app-id\"\n    - name: authType # Required.\n      value: \"none\"\n    - name: maxMessageBytes # Optional.\n      value: 1024\n    - name: consumeRetryInterval # Optional.\n      value: 200ms\n    - name: disableTls # Optional. Disable TLS. This is not safe for production!! You should read the `Mutual TLS` section for how to use TLS.\n      value: \"true\"\n    - name: initialOffset\n      value: \"oldest\"\n```\nThe rest of the files: `pubsub-subscription.yaml`, `dlq-subscription.yaml` and `observability.yaml` remain the same.\n\nFirst, we need to start `docker-compose.yml` to have a Kafka broker up: run `docker-compose up` from the root directory. This will spin up a Kafka broker as well as a Kafka UI tool which will allow up to peek at the messages. Now the services can be invoked again with the only change being components folder:\n\n- Checkout service\n```bash\ncd java/sdk/checkout\ndapr run --app-id checkout --components-path ../../../components-kafka -- java -jar target/CheckoutService-0.0.1-SNAPSHOT.jar\n```\n- Order-processing service\n```bash\ncd java/sdk/order-processor\ndapr run --app-port 8080 --app-id order-processor --components-path ../../../components-kafka -- java -jar target/OrderProcessingService-0.0.1-SNAPSHOT.jar 2>&1 | tee log\n```\n\nWe can verify through the logs that the same scenario occurred as when using Redis Streams: all messages passed through `/orders` controller while one message passed through `/failedMessages` controller. We can additionally verify this if we use Kafka UI tool at `localhost:9000`:\n\n![Kafka UI tool list of topics](/images/blog/kafka-ui-tool.png)\n and click `orders` or `poisonMessages` topics to see the list of messages.\n\n As you can see switching from Redis to Kafka was a breeze, not a single line of Java code changed ðŸš€. I hope this tutorial showcased one of the main strengths of Dapr: delegating infrastructure code to its runtime and SDK and the ability to switch infrastructure providers seamlessly without any Java code changes. [Link](https://github.com/yossisp/dapr-pubsub-with-dlq-demo) to the full code.\n\n ### <a name=\"bonus\"></a>Bonus\n Dapr provides observability via [Zipkin](https://zipkin.io/). If you ran `dapr init` at the beginning of the tutorial, go to `localhost:9411` which is Zipkin UI and hit `Run query` to see that each message generated a trace.\n\n Also Dapr provides a nice dashboard where all services can be viewed, run this in Terminal: `dapr dashboard -p 8081` to explore.\n\n ### <a name=\"references\"></a>References\n 1. [Initialize Dapr in your local environment](https://docs.dapr.io/getting-started/install-dapr-selfhost/)\n 2. [Quickstart: Publish and Subscribe](https://docs.dapr.io/getting-started/quickstarts/pubsub-quickstart/)\n 3. [List of Dapr-supported pub/sub brokers](https://docs.dapr.io/reference/components-reference/supported-pubsub/)\n 4. [Dead Letter Topics in Dapr](https://docs.dapr.io/developing-applications/building-blocks/pubsub/pubsub-deadletter/)\n 5. [Dapr Declarative and programmatic subscription methods](https://docs.dapr.io/developing-applications/building-blocks/pubsub/subscription-methods/)\n 6. [Dapr pub/sub integration with Apache Kafka](https://docs.dapr.io/reference/components-reference/supported-pubsub/setup-apache-kafka/)"},{"slug":"how-to-stream-mongodb-changes-to-kafka","category":"blog","title":"How To Stream MongoDB Changes To Elasticsearch (via Kafka)","description":"How To Stream MongoDB Changes To Kafka, How To Connect MongoDB and Kafka, How To Connect MongoDB and Elasticsearch","tags":["mongodb","kafka","elasticsearch","debezium","cdc"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;padding-bottom:32px;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/apache_kafka_logo.svg\"\n            alt=\"Kafka Logo\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 64px;padding-left:16px;padding-right:32px;\">+</span>\n    <div style=\"width:50%;padding-top:32px;\">\n    <img src=\"/images/blog/elasticsearch_logo.svg\"\n        alt=\"Elasticsearch Logo\"\n        />\n    </div>\n</div>\n\n1. [Why I chose Kafka to stream changes to Elasticsearch.](#why)\n2. [Take me straight to the tutorial.](#tutorial)\n\n<a name=\"why\"></a>I recently had to stream all changes (insertions/updates/deletions) from MongoDB to Elasticsearch in order to allow users to search the most up-to-date data. In addition I needed to transform MongoDB documents before inserting them into Elasticsearch, and many of those transformations weren't trivial. There're several options to do this which include [Logstash](https://www.elastic.co/logstash/) and [MongoDB River Plugin for ElasticSearch](https://github.com/richardwilly98/elasticsearch-river-MongoDB). Logstash is an official component of ELK stack which is nice but it has its own syntax for modifying documents which requires a certain learning curve. MongoDB River plugin is an [Elasticsearch plugin](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-plugins.html) which also allows document [transformations](https://github.com/richardwilly98/elasticsearch-river-MongoDB/wiki) although it seems that the transformations are more along the likes of including/excluding specific fields in a document. It also requires a relatively complicated setup to enable transformations. All of the above solutions require MongoDB to be started as a replica set because then MongoDB creates a special collection called `oplog` under `local` database. The oplog contains all document changes from any collection and is therefore used by the tools above to monitor for MongoDB document changes.\n\nThere's also a 3rd option: Kafka connector! The advantage of Kafka connector is that it will stream document changes to a Kafka topic which can then be consumed by a Kafka client in a language of your choice. Once I have access to the changed document I can easily apply all the transformations I need using the programming language of my choice and then insert/delete the document into Elasticsearch. A popular connector for this use case is [Debezium](https://debezium.io/) which supports streaming changes from a number of databases including MongoDB. There's an official Debezium [tutorial](https://debezium.io/documentation/reference/1.6/connectors/MongoDB.html) for connecting to MongoDB but I'd like to share a more straightforward guide to performing the setup. While the tutorial is great and I encourage everyone to read it to get a better understanding of Debezium I think I can offer a simplified version of the tutorial where I explain some of the assumptions made in the original tutorial.\n\n<a name=\"tutorial\"></a>First of all:\n\n> Debezium is built on top of Apache Kafka and provides Kafka Connect compatible connectors that monitor specific database management systems.\n\n[Kafka Connect](https://kafka.apache.org/documentation/#connect) is an open-source tool to ingest data from data systems (e.g. databases) and to stream changes to data systems. When data is ingested into Kafka a source connector is used, when data is streamed from Kafka sink connector is used. Kafka connectors and by extension Debezium are managed via REST API. We will use a source connector in order to stream database changes to a Kafka topic.\n\nDebezium also requires MongoDB to be run as a replica set. If you use a standalone Mondodb instance you can easily [convert](https://docs.MongoDB.com/v4.0/tutorial/convert-standalone-to-replica-set/) it to replica. I also had to convert mine and I'll use a minimal setup MongoDB docker image for the tutorial:\n\n- create a file `mongoinit.sh` and paste the lines in it:\n\n```bash\nsleep 5 && mongo --eval \"rs.initiate()\" &\nmongod --port 27017 --replSet rs0 --bind_ip_all\n```\n\nThe shell script initiates a MongoDB replica set which will consist of one primary node.\n\n- create a file named `Dockerfile.mongo` and paste the following lines in it:\n\n```docker\nFROM mongo:4.0\nWORKDIR /src\nCOPY ./mongoinit.sh ./mongoinit.sh\nRUN chmod +x ./mongoinit.sh\nCMD [\"./mongoinit.sh\"]\n```\n\n- the mongo docker image can be built: `docker build -f Dockerfile.mongo -t mongo .`\n\nNow that we have a custom MongoDB image we can create the `docker-compose.yml` file which will contain all the necessary parts we need for the project: a Kafka broker instance, MongoDB instance, Debezium instance and [Kafdrop](https://github.com/obsidiandynamics/kafdrop) instance which allows to browse Kafka topics/message via a web GUI:\n\n```yaml\nversion: \"3\"\nservices:\n  kafdrop:\n    image: obsidiandynamics/kafdrop\n    restart: \"no\"\n    ports:\n      - \"9000:9000\"\n    environment:\n      KAFKA_BROKERCONNECT: \"kafka:29092\"\n      JVM_OPTS: \"-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify\"\n    depends_on:\n      - \"kafka\"\n\n  kafka:\n    image: obsidiandynamics/kafka\n    restart: \"no\"\n    ports:\n      - \"2181:2181\"\n      - \"9092:9092\"\n      - \"29092:29092\"\n    environment:\n      KAFKA_LISTENERS: \"INTERNAL://:29092,EXTERNAL://:9092\"\n      KAFKA_ADVERTISED_LISTENERS: \"INTERNAL://kafka:29092,EXTERNAL://localhost:9092\"\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: \"INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n      KAFKA_INTER_BROKER_LISTENER_NAME: \"INTERNAL\"\n      KAFKA_ZOOKEEPER_SESSION_TIMEOUT: \"6000\"\n      KAFKA_RESTART_ATTEMPTS: \"10\"\n      KAFKA_RESTART_DELAY: \"5\"\n      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: \"0\"\n\n    mongo:\n      image: mongo\n      container_name: mongo\n\n    connect:\n      image: debezium/connect:1.6\n      ports:\n        - 8083:8083\n      environment:\n        - BOOTSTRAP_SERVERS=kafka:29092\n        - GROUP_ID=connect-cluster\n        - CONFIG_STORAGE_TOPIC=my_connect_configs\n        - OFFSET_STORAGE_TOPIC=my_connect_offsets\n        - STATUS_STORAGE_TOPIC=my_connect_statuses\n      depends_on:\n        - \"kafka\"\n        - \"mongo\"\n```\n\nOnce the file is created `docker-compose up` can be run from the same directory as the file. Notice that Debezium configuration contains several environment variables which are Kafka Connect variables. They can be found in the [docs](https://kafka.apache.org/documentation/#connect) by their namesakes: `config.storage.topic`, `offset.storage.topic`, `status.storage.topic` etc. These topics require specific configuration which is mentioned in the [docs](https://kafka.apache.org/documentation/#connect). Debezium will attempt to auto create the topics if your Kafka broker allows it. You can check if the topics were created with the required settings in Kafdrop at `localhost:9000`:\n\n![upload new configuration](/images/blog/kafdrop.png)\n\nelse the topics must be created manually with via a Kafka admin client of your choice.\n\nWhat is left is to connect Debezium to MongoDB. Firstly, Debezium requires a certain set of permissions to access oplog collection. A user can be created for this purpose in mongo shell. Run `docker exec -it mongo /bin/bash`. Once inside the container execute `mongo` command to start mongo shell. Run the following command:\n\n```js\nuse admin\ndb.createUser(\n  {\n    user: 'your_username',\n    pwd: 'your_password',\n    roles: [ { role: 'root', db: 'admin' } ]\n  }\n);\n```\n\nNow a Debezium connector needs to be created via Kafka Connect REST API (below is a CURL command which can be executed in terminal):\n\n```bash\ncurl --location --request POST 'http://localhost:8083/connectors' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n  \"name\": \"mongo\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.MongoDB.MongoDBConnector\",\n    \"MongoDB.hosts\": \"mongo:27017\",\n    \"MongoDB.name\": \"mongoconnector\",\n    \"MongoDB.user\": \"your_username\",\n    \"MongoDB.password\": \"your_password\",\n    \"MongoDB.authsource\": \"admin\"\n  }\n}'\n```\n\nThe object keys in the POST body JSON object are Debezium [properties](https://debezium.io/documentation/reference/1.6/connectors/MongoDB.html#MongoDB-connector-properties). `monngodb.hosts` value is `mongo:27017` because both Debezium and MongoDB are run inside docker containers and docker allows one docker service to refer to another service via `servicename:port` convention (provided that the containers are on the same docker network which they are in our case because docker-compose automatically creates one). Because the name of the MongoDB service is `mongo` and the database runs on port `27017` it can be accessed on the network by `mongo:27017` address.\n\nLet's create some data in the same mongo shell that we opened earlier:\n\n```js\nuse testdb\ndb.products.insert({ item: \"envelopes\", qty: 100, type: \"Clasp\" })\n```\n\nAfter visiting Kafdrop at `localhost:9000` the `mongoconnector.testdb.products` topic should appear:\n\n![kafdrop mongo connector](/images/blog/kafdropmongoconnector.png)\n\nAfter clicking the topic a message will be seen which will contain data referring to the document just created:\n\n![kafdrop message](/images/blog/kafdropmessage.png)\n\nThe created document can be accessed under the Kafka message value `payload.after` key for insertions and under `payload.patch` key for updates.\n\nAt this point a Kafka consumer can be started to consume messages from `mongoconnector.testdb.products` topic, perform the necessary transformations and insert the resulting documents into Elasticsearch (ðŸ”¥). Specific databases/collections can be defined to be monitored, these settings can be found in Debezium [docs](https://debezium.io/documentation/reference/1.6/connectors/MongoDB.html#MongoDB-connector-properties).\n"},{"slug":"how-to-stream-postgres-mysql-changes-to-elasticsearch-via-kafka","category":"blog","title":"How To Stream Postgres, MySQL Changes To Elasticsearch (via Kafka)","description":"How To Stream Postgres, MySQL Changes To Kafka, How To Connect Postgres, MySQL and Kafka, How To Connect Postgres and Elasticsearch","tags":["postgres","mysql","elasticsearch","debezium"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;padding-bottom:32px;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/apache_kafka_logo.svg\"\n            alt=\"Kafka Logo\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 64px;padding-left:16px;padding-right:32px;\">+</span>\n    <div style=\"width:50%;padding-top:32px;\">\n    <img src=\"/images/blog/elasticsearch_logo.svg\"\n        alt=\"Elasticsearch Logo\"\n        />\n    </div>\n</div>\n\n1. [Postgres Setup](#postgres-setup)\n2. [MySQL Setup](#mysql-setup)\n\nThis is the second post from [change data capture](https://en.wikipedia.org/wiki/Change_data_capture) series. You can read more about the motivation and introduction into Debezium in the [previous post](https://www.spektor.dev/how-to-stream-mongodb-changes-to-kafka/).\n\nIn this post I will show how to set up Debezium with Postgres and MySQL databases. As in the [previous post](https://www.spektor.dev/how-to-stream-mongodb-changes-to-kafka/) the prerequisites are Debezium and Kafka images. Postgres and MySQL official images will be used in the `docker-compose.yml` below:\n\n```yaml\nversion: \"3\"\nservices:\n  kafdrop:\n    image: obsidiandynamics/kafdrop\n    restart: \"no\"\n    ports:\n      - \"9000:9000\"\n    environment:\n      KAFKA_BROKERCONNECT: \"kafka:29092\"\n      JVM_OPTS: \"-Xms16M -Xmx48M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify\"\n    depends_on:\n      - \"kafka\"\n\n  kafka:\n    image: obsidiandynamics/kafka\n    restart: \"no\"\n    ports:\n      - \"2181:2181\"\n      - \"9092:9092\"\n      - \"29092:29092\"\n    environment:\n      KAFKA_LISTENERS: \"INTERNAL://:29092,EXTERNAL://:9092\"\n      KAFKA_ADVERTISED_LISTENERS: \"INTERNAL://kafka:29092,EXTERNAL://localhost:9092\"\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: \"INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n      KAFKA_INTER_BROKER_LISTENER_NAME: \"INTERNAL\"\n      KAFKA_ZOOKEEPER_SESSION_TIMEOUT: \"6000\"\n      KAFKA_RESTART_ATTEMPTS: \"10\"\n      KAFKA_RESTART_DELAY: \"5\"\n      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: \"0\"\n\n  connect:\n    image: debezium/connect:1.6\n    ports:\n      - 8083:8083\n    environment:\n      - BOOTSTRAP_SERVERS=kafka:29092\n      - GROUP_ID=connect-cluster\n      - CONFIG_STORAGE_TOPIC=my_connect_configs\n      - OFFSET_STORAGE_TOPIC=my_connect_offsets\n      - STATUS_STORAGE_TOPIC=my_connect_statuses\n    depends_on:\n      - \"kafka\"\n      - \"postgres\"\n      - \"mysql\"\n\n  postgres:\n    image: postgres:12.7\n    restart: \"no\"\n    container_name: postgres\n    ports:\n      - 5432:5432\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_DB: postgres\n    command: [\"sleep\", \"1000000\"]\n\n  # superuser `root` is created automatically with the password as the value of `MYSQL_ROOT_PASSWORD` parameter\n  mysql:\n    image: mysql:5.7\n    restart: \"no\"\n    ports:\n      - \"3306:3306\"\n    environment:\n      MYSQL_ROOT_PASSWORD: rootpass\n      MYSQL_DATABASE: mydb\n      MYSQL_USER: admin\n      MYSQL_PASSWORD: mypass\n    container_name: mysql\n    volumes:\n      - \"./my.cnf:/etc/mysql/my.cnf\"\n\n  adminer:\n    depends_on:\n      - postgres\n      - mysql\n    image: adminer\n    restart: \"no\"\n    ports:\n      - 8080:8080\n```\n\nNotice that `mysql` service uses a file `my.cnf`:\n\n```\n[mysqld]\n\nserver-id         = 223344\nlog_bin           = mysql-bin\nbinlog_format     = ROW\nbinlog_row_image  = FULL\nexpire_logs_days  = 10\n\n!includedir /etc/mysql/conf.d/\n!includedir /etc/mysql/mysql.conf.d/\n```\n\nThis is MySQL config file which enables [binlog](https://debezium.io/documentation/reference/1.6/connectors/mysql.html#enable-mysql-binlog). In addition `adminer` service is a handy database client with web interface which will allow us to connect to databases.\n\nAs opposed to MongoDB update events, Postgres and MySQL update events contain the values of all of the table columns, that is you get the full row. This can be very handy in some cases. As in MongoDB, the connectors start by reading every existing row from relevant tables and send a `READ` event for each row.\n\n<a name=\"postgres-setup\"></a>\n\n### Postgres Setup\n\nAs you may have noticed the docker command for Postgres container is `sleep`. This is because Postgres configuration needs to be changed and then Postgres server needs to be restarted. If the docker command was starting Postgres then killing Postgres process would cause docker container to exit which is not what we want. Here're the steps to enable [logical decoding](https://www.postgresql.org/docs/9.4/logicaldecoding-explanation.html) in Postgres (this is necessary in order for Debezium to intercept data changes):\n\n- Run `docker exec -it postgres /bin/bash`\n- Start Postgres process `/usr/local/bin/docker-entrypoint.sh -c 'shared_buffers=256MB' -c 'max_connections=200'`\n- Visit `localhost:8080` (adminer) and enter the login details for Postgres: (System=PostgreSQL, Server=postgres, Username=postgres, Password=postgres, Database=postgres)\n- Click on \"SQL Command\" and run this query: `SHOW wal_level;`. This will show `replica` but what we need is `logical`. Therefore run again: `ALTER SYSTEM SET wal_level = logical;`\n- Restart Postgres process by killing it and running again `/usr/local/bin/docker-entrypoint.sh -c 'shared_buffers=256MB' -c 'max_connections=200'`\n- Verify that `SHOW wal_level;` now returns `logical`\n- Sometimes changes in the databases you don't want to monitor happen much more frequently than changes in the databases you want to monitor. Since WAL is shared among all databases Debezium can't confirm the [LSN](https://www.postgresql.org/docs/9.4/datatype-pg-lsn.html) because the database that Debezium monitors didn't receive an event for a period of time. This will cause WAL to grow considerably and eventually the instance may run out of storage (this can easily happen on AWS RDS Postgres instances because of many system database writes). In order to prevent such scenario we will create a heartbeat table for the sole purpose of allowing Debezium to make changes to it every `heartbeat.interval.ms` milliseconds. This will ensure that even in the case there haven't been any changes in the database monitored Debezium will still periodically confirm the LSN and WAL will not cause out of storage issues. To create the table:\n\n```sql\nCREATE TABLE IF NOT EXISTS debezium_heartbeat (\n\tid serial PRIMARY KEY,\n\theartbeat_text VARCHAR (15)\n);\n```\n\nNext we'll set the `heartbeat.action.query` to the actual change for Debezium to make: `INSERT INTO debezium_heartbeat (heartbeat_text) VALUES ('test_heartbeat')`.\n\nThe last step is to create the Debezium connector:\n\n```bash\ncurl --location --request POST 'http://localhost:8083/connectors' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"name\": \"postgres-connector\",\n    \"config\": {\n        \"connector.class\": \"io.debezium.connector.postgresql.PostgresConnector\",\n        \"database.hostname\": \"postgres\",\n        \"database.port\": \"5432\",\n        \"database.user\": \"postgres\",\n        \"database.password\": \"postgres\",\n        \"database.dbname\": \"postgres\",\n        \"database.server.name\": \"postgres-server-name\",\n        \"plugin.name\": \"pgoutput\",\n        \"schema.include.list\": \"yourSchema\",\n        \"table.include.list\": \"yourCommaSeparatedTables\",\n        \"publication.autocreate.mode\": \"filtered\",\n        \"heartbeat.action.query\": \"INSERT INTO debezium_heartbeat (heartbeat_text) VALUES ('test_heartbeat')\",\n        \"heartbeat.interval.ms\": \"300000\"\n    }\n}'\n```\n\n- `publication.autocreate.mode` only applies if `pgoutput` plugin is used and it's quite important. By default, when using `pgoutput` Debezium will create a [publication](https://www.postgresql.org/docs/10/logical-replication-publication.html) for **all** tables (unless the publication was already created with all the required settings and provided to Debezium via `publication.name` setting). If you want Debezium to create the publication only for the tables and schemas specified in `table.include.list` and `schema.include.list` parameters then `publication.autocreate.mode` must be set to `filtered`.\n\nWe can now create a test table to verify that the connector works:\n\n```sql\nCREATE TABLE test (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR (50),\n    age SMALLINT\n);\n\nINSERT INTO \"test\" (\"name\", \"age\")\nVALUES ('Paul McCartney', '79');\n```\n\nNow if we visit `http://localhost:9000/` a topic `postgres-server-name.public.test` will contain a message with the inserted data (ðŸ”¥).\n\n<a name=\"mysql-setup\"></a>\n\n### MySQL Setup\n\nMySQL setup is the easiest, the only thing left to do is to create the Debezium connector:\n\n```bash\ncurl --location --request POST 'http://localhost:8083/connectors' \\\n--header 'Content-Type: application/json' \\\n--data-raw '{\n    \"name\": \"mysql-connector\",\n    \"config\": {\n        \"connector.class\": \"io.debezium.connector.mysql.MySqlConnector\",\n        \"database.hostname\": \"mysql\",\n        \"database.port\": \"3306\",\n        \"database.user\": \"root\",\n        \"database.password\": \"rootpass\",\n        \"database.server.id\": \"184054\",\n        \"database.server.name\": \"mysql-server-name\",\n        \"database.history.kafka.bootstrap.servers\": \"kafka:29092\",\n        \"database.history.kafka.topic\": \"dbhistory.schema-changes\",\n        \"include.schema.changes\": \"false\",\n        \"heartbeat.interval.ms\": \"300000\"\n    }\n}'\n```\n\nWe can now verify the connector by inserting some test data. First connect to the MySQL instance from adminer:\n\n- Go to `http://localhost:8080`\n- The login details are System=MySQL, Server=mysql, Username=admin, Password=mypass, Database=mydb\n\nOnce inside adminer sample data my be created:\n\n```sql\nCREATE TABLE test (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR (50),\n    age SMALLINT\n);\n\nINSERT INTO test (name, age)\nVALUES ('Paul McCartney', '79');\n```\n\nIf we visit `http://localhost:9000/` a new topic will appear `mysql-server-name.mydb.test` which will contain a message with the inserted data (ðŸ”¥).\n"},{"slug":"how-to-use-artificial-intelligence-apis-at-scale","category":"blog","title":"How To Use Artificial Intelligence (AI) APIs At Scale","description":"Learn how to use AI APIs at scale by using serverless functions to make asynchronous API requests","tags":["ai","serverless","qstash","performance"],"body":"\n![logo for application using serverless functions and artificial intelligence api](/images/blog/logo_for_application_using_serverless_functions_and_artificial_intelligence_api.png)\n<div style=\"font-size:14px;font-style:italic;color:gray;padding-bottom:64px;\">The above image was generated by <a href=\"https://openai.com/dall-e-2/\">Dalle2</a> AI image generation service for the prompt: \"logo for application using serverless functions and artificial intelligence api\".</div>\n\n1. [Why Use Serverless When Working With AI Services?](#why-serverless)\n2. [Serverless Solution](#solution)\n3. [Qstash Showcase](#qstash)\n4. [Link To Code](#link-to-code)\n\nGiven the current boom of AI technologies one thing to keep in mind is that API responses from large AI models can be [slow](https://community.openai.com/t/open-ai-reponse-is-slow/22527) especially given large prompts, sometimes even spanning minutes.\n\n### <a name=\"why-serverless\"></a>Why Use Serverless When Working With AI Services?\nLong response time from AI services can present a challenge if your application makes a lot of concurrent requests to an AI service like [ChatGPT](https://openai.com/blog/chatgpt/) and especially to image generation services like [Dalle](https://openai.com/dall-e-2/) or [Midjourney](https://midjourney.com/home/?callbackUrl=%2Fapp%2F). While you're less likely to run into [C10k problem](https://en.wikipedia.org/wiki/C10k_problem) with modern machines it can still result in issues like:\n\n- timeouts (your client/service mesh provider may time out)\n- high memory consumption: holding a high number of concurrent HTTP connections will result in higher memory consumption or even memory leaks\n\nA very elegant solution to the above problems is to use a serverless function, essentially delegate the \"waiting time\" to a serverless service and have the serveless function forward AI service response to a callback endpoint in your application. I first ran into this approach in this Github [project](https://github.com/domeccleston/dalle-2) which includes an example for a [Next.js](https://nextjs.org) application. \n\n### <a name=\"solution\"></a>Serverless Solution\nThe diagram below shows an example for a server application which receives a request with some prompt, uses serverless service to forward the request to OpenAI [Dalle](https://openai.com/dall-e-2/) image generation API and allows the user to check if their image is ready:\n\n![user application serverless diagram](/images/blog/user-application-serverless-diagram.png)\n\nThe flow is:\n\n- The user sends a request to `/generate-image` with the prompt which describes the image to generate. The user response includes submission id which the user will use later to check if their image is ready.\n- The application passes the prompt to a serverless provider.\n- Serverless provider makes a request to Dalle with the prompt and forwards the response to `/image-callback` callback in the application.\n- The application saves the URL to the generated image in some kind of storage service (in a real application a service like [Redis](https://redis.io/) could be used) along with the submission id as the key.\n- The user can check if the image is ready from time to time via `/get-image-is-ready` endpoint. In a real application, the application itself could be polling the cache to check if the image was generated and notify the user via some kind of an event like Websocket communication/[server-side events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events), HTTP/2 which allows bidirectional streaming or message queues. An alternative to polling would be to save the data in a database like Postgres and then use a change data capture project like [Debezium](https://debezium.io/) to send an event to the application once new data with image URL was inserted (you can check out my blog [post](https://www.spektor.dev/how-to-stream-postgres-mysql-changes-to-elasticsearch-via-kafka/) which details this setup).\n\n### <a name=\"qstash\"></a>Qstash Showcase\nWhat I'd like to showcase in this post is the serverless solution using [Qstash](https://docs.upstash.com/qstash) by Upstash. Upstash is a cloud service which provides [Redis](https://redis.io/) and [Kafka](https://kafka.apache.org/) as a service with free tiers. In addition, it provides a serverless solution similar to [AWS Lambda](https://aws.amazon.com/lambda/) called [Qstash](https://docs.upstash.com/qstash). What's really cool about it is the simplicity of usage. For example, in the tutorial we want to make a request to OpenAI API `https://api.openai.com/v1/images/generations`. This is how the request would look like using Qstash:\n```bash\ncurl -XPOST \\\n    -H 'Authorization: Bearer QSTASH_TOKEN' \\\n    -H 'Upstash-Forward-My-Custom-Header: my-value' \\\n    -H 'Upstash-forward-Authorization: Bearer OPENAI_API_KEY' \\\n    -H 'Content-type: application/json' \\\n    -H 'Upstash-Callback: myapp.com/callback' \\\n    -H 'Upstash-Retries: 3' \\\n    -d '{ \"prompt\": \"some prompt\" }' \\\n    'https://qstash.upstash.io/v1/publish/https://api.openai.com/v1/images/generations'\n```\n\nLet's break down the request, there's a lot going on here:\n- In order to have Qstash pass the request to OpenAI all we had to do was to concatenate Qstash URL (`https://qstash.upstash.io/v1/publish`) and the destination URL. That's it!\n- `Authorization: Bearer QSTASH_TOKEN` pass Qstash access token.\n- We can pass any custom headers to OpenAI via `Upstash-Forward-My-Custom-Header`. For example, in order to pass OpenAI API key we used `Upstash-forward-Authorization: Bearer OPENAI_API_KEY`.\n- `Upstash-Callback: myapp.com/callback` header defines the callback URL that Qstash should forward the OpenAI response to.\n- `Upstash-Retries: 3` how many times should Qstash retry the request.\n- `{ \"prompt\": \"some prompt\" }` body payload to be sent to destination service (OpenAI).\n\nIn my opinion the simplicity of usage is amazing ðŸ”¥. What's even greater is that you can even create a CRON job using Qstash by making a request as above and adding this header `Upstash-Cron: 0 1 * * *`. This could be very handly if you want to access some external API at an interval and save its response without having to orchestrate CRON job in your application code.\n\n### <a name=\"link-to-code\"></a>Link To Code\nYou can find the full Typescript code for the application described in the post in this Github [repo](https://github.com/yossisp/async-ai-image-generation). "},{"slug":"http-request-utility-golang","category":"blog","title":"How To Write an HTTP Request Utility in Go","description":"How To Write an HTTP Request Utility in Go","tags":["golang","http-request","tutorial"],"body":"\nI recently started a project in Go language which is really fun. In my project I frequently make requests to an external API so I wrote I handy request utility which uses recursion to make retries. One of the main reasons for writing such a utility it that if authentication is required to an API you use then you want to write authentication logic once. This way if it's Oauth for example and your access token has expired the utility will always take care of getting a new access token (using the refresh token). In addition the utility can set headers common to all requests like `Content-Type` for example or perform any other logic common to all requests like logging. I will explain some of the gotchas I ran into while writing the code after the code snippet:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"log\"\n\t\"net/http\"\n\t\"time\"\n)\n\nconst (\n\ttimeout = 30\n)\n\n// Provider holds state relevant to an external API\ntype Provider struct {\n\tmaxRetries    int\n\tactualRetries int\n\tclient        *http.Client\n}\n\nfunc (provider *Provider) request(req *http.Request) (response *http.Response, err error) {\n\tclonedReq := req.Clone(req.Context())\n\tif req.Body != nil {\n\t\tclonedReq.Body, err = req.GetBody()\n\t\tif err != nil {\n\t\t\tlog.Println(err)\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tdefer func() {\n\t\tif clonedReq.Body != nil {\n\t\t\tclonedReq.Body.Close()\n\t\t}\n\t}()\n\n\t// set headers common to all requests\n\treq.Header.Set(\"Content-Type\", \"application/json\")\n\tresponse, err = provider.client.Do(req)\n\tif err != nil {\n\t\tlog.Println(\"request: \", err)\n\t\treturn nil, err\n\t}\n\n\tif provider.maxRetries == provider.actualRetries {\n\t\terr = fmt.Errorf(\"request: too many retries in request for request: %v\", req)\n\t\tresponse.Body.Close()\n\t\treturn nil, err\n\t}\n\n\tif response.StatusCode == http.StatusUnauthorized && provider.maxRetries > provider.actualRetries {\n\t\tbodyBytes, err := ioutil.ReadAll(response.Body)\n\t\tif err != nil {\n\t\t\tlog.Println(err)\n\t\t} else {\n\t\t\tlog.Println(string(bodyBytes))\n\t\t}\n\t\tprovider.actualRetries++\n\t\t// ask for a new refresh token\n\t\tresponse.Body.Close()\n\t\tresponse, err = provider.request(clonedReq)\n  }\n\n  return response, err\n}\n\nfunc (provider *Provider) callExternalAPI() {\n\treq, err := http.NewRequest(http.MethodGet, \"https://cat-fact.herokuapp.com/facts/random?animal_type=cat&amount=2\", nil)\n\tif err != nil {\n\t\treturn\n\t}\n\tresponse, err := provider.request(req)\n\tif err != nil {\n\t\tlog.Println(\"callExternalAPI: \", err)\n\t\treturn\n\t}\n\tdefer response.Body.Close()\n\t// do something with the response\n\tcatFact, err := ioutil.ReadAll(response.Body)\n\tif err != nil {\n\t\tlog.Println(err)\n\t} else {\n\t\tlog.Println(string(catFact))\n\t}\n}\n\nfunc main() {\n\tprovider := Provider{\n\t\tmaxRetries:    5,\n\t\tactualRetries: 0,\n\t\tclient: &http.Client{\n\t\t\tTimeout: time.Duration(timeout * time.Second),\n\t\t},\n\t}\n\tprovider.callExternalAPI()\n}\n```\n\nThe general idea of `request` is that if 401 http status error is received (Unauthorized) another recursive call of `request` is made until the max amount of retries is reached or the call is successful in terms of the http status.\n\n### Gotchas\n\n1. Setting timeout is in `http.Client` is important, otherwise the connection may [hang indefinitely](https://medium.com/@nate510/don-t-use-go-s-default-http-client-4804cb19f779).\n2. The client is created once per provider because it's safe for concurrent use and this is actually encouraged in the official [docs](https://golang.org/pkg/net/http/#Client).\n3. Request body (if present) should be cloned. This is because the request body is closed automatically on the first request. Therefore if the request contains a body a copy should be made so that the request can be still used on future retries. The cloned request should be closed in the case it wasn't used.\n4. `defer response.Body.Close()` can't be used in the code because the caller of `request` needs it in order to get the necessary information, therefore it should close it. However, if retries are made the body of previous responses should definitely be closed manually.\n"},{"slug":"introduction-to-prometheus-and-grafana","category":"blog","title":"Introduction to Monitoring with Prometheus and Grafana","description":"Introduction to Monitoring with Prometheus and Grafana","tags":["monitoring","prometheus","grafana"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;padding-bottom:50px;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/grafana_logo.svg\"\n            alt=\"Grafana Logo\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 32px;padding-left:16px;padding-right:16px;\"> + </span>\n    <div style=\"width:30%;\">\n    <img src=\"/images/blog/prometheus_logo.svg\"\n        alt=\"Prometheus Logo\"\n        />\n    </div>\n</div>\n\n_The link to the github repo with the code:_ [link](https://github.com/yossisp/prometheus-grafana-tutorial).\n\n[Prometheus](https://prometheus.io) is an open-source project which is both a monitoring system and a time-series database. It's a popular choice for monitoring services, for example, it can be used to monitor metrics like CPU usage, heap consumption as well as business logic events (more on that later). Its time-series database also supports labels (e.g. metrics can be classified for example by mobile/desktop, client type etc.). Additional features include:\n\n- PromQL language which can be used to query for certain metrics/labels/perform aggregations etc.\n- Alerts\n\nPrometheus uses pull model to get metrics data. This means that a server runs on the same hardware as your application and exposes a REST endpoint which Prometheus instance makes requests to, based on the set interval (by default 15 seconds). Push model is also supported however it's recommended only for very specific scenarios when pulling model may miss the actual events (for example missing a rare business logic event).\n\n[Grafana](https://grafana.com/) is an open-source project which provides great visualizations for projects like Prometheus. There's a nice Grafana tutorial [here](https://grafana.com/tutorials/grafana-fundamentals/).\n\nBelow I will show how to set up Prometheus, Grafana and a simple server application in order to visualize Prometheus data. Full code of the app can be found [here](https://github.com/yossisp/prometheus-grafana-tutorial). First let's create a very simple NodeJS server app. It will implement `/metrics` route which will return metrics in Prometheus format using [prom-client NPM package](https://www.npmjs.com/package/prom-client) (by default Prometheus pulls from `/metrics` endpoint but this can be changed) and `/home` route which would contain business logic:\n\n```ts\nimport express, { Request, Response } from \"express\"\nimport promClient from \"prom-client\"\n\nconst main = () => {\n  const app = express()\n\n  // collect default metrics like CPU/heap/event loop stats\n  promClient.collectDefaultMetrics()\n  app.get(\"/metrics\", async function(req: Request, res: Response) {\n    const metrics = await promClient.register.metrics()\n    res.set(\"Content-Type\", promClient.register.contentType)\n    res.send(metrics)\n  })\n\n  app.get(\"/home\", function(req: Request, res: Response) {\n    // business logic\n    res.send(\"Hello World!\")\n  })\n  app.listen(3000)\n  console.log(\"Server started on port 3000\")\n}\n\nmain()\n```\n\nSupposing the app is running locally at `localhost:3000` if we access the app at endpoint `localhost:3000/metrics` the following response will be received (only 2 metrics are displayed for brevity, the actual output contains more):\n\n```\n# HELP process_cpu_user_seconds_total Total user CPU time spent in seconds.\n# TYPE process_cpu_user_seconds_total counter\nprocess_cpu_user_seconds_total 0.017604\n\n# HELP process_resident_memory_bytes Resident memory size in bytes.\n# TYPE process_resident_memory_bytes gauge\nprocess_resident_memory_bytes 32145408\n```\n\nAs can be seen one of the metrics is called `process_cpu_user_seconds_total` and its value is `0.017604`. `HELP` explains the meaning of the metric while `TYPE` mentions the type of the metric which is a counter. Prometheus supports several types of metrics 2 of which will be covered in the post:\n\n- **Counter** is used for always increasing values e.g. total user CPU time spent in seconds. As long as our app is active this value will only increase. A counter will also be suitable to track the number of requests a server received as it will only increase.\n\n- **Gauge** is used for values which increase and decrease. For example, heap consumption or number of active users.\n\nFor the purposes of the tutorial we'll create a custom counter: the number of requests to `/home` route:\n\n```ts\nimport express, { Request, Response } from \"express\"\nimport promClient from \"prom-client\"\n\nconst main = () => {\n  const app = express()\n  const homeRequestsNumCounter = new promClient.Counter({\n    name: \"home_requests_num\",\n    help: \"Number of requests to /home route\",\n  })\n\n  // collect default metrics like CPU/heap/event loop stats\n  promClient.collectDefaultMetrics()\n  app.get(\"/metrics\", async function(req: Request, res: Response) {\n    const metrics = await promClient.register.metrics()\n    res.set(\"Content-Type\", promClient.register.contentType)\n    res.send(metrics)\n  })\n\n  app.get(\"/home\", function(req: Request, res: Response) {\n    // business logic\n    homeRequestsNumCounter.inc()\n    res.send(\"Hello World!\")\n  })\n  app.listen(3000)\n  console.log(\"Server started on port 3000\")\n}\n\nmain()\n```\n\nThe line `homeRequestsNumCounter.inc()` increments the custom counter each time a new request for `/home` is received. Let's make a request to `localhost:3000/home` so that when we get Prometheus stats at `localhost:3000/metrics` we can receive the following:\n\n```\n# HELP home_requests_num Number of requests to /home route\n# TYPE home_requests_num counter\nhome_requests_num 1\n```\n\nGreat! Now what is left is containerize our server and add `prometheus.yml` config. Prometheus supports [targets auto-discovery](https://prometheus.io/docs/prometheus/latest/http_sd/) however for simplicity purposes the connection details of our app target can be configured in a configuration file `prometheus.yml`:\n\n```yaml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: \"my_app\"\n\n    static_configs:\n      # the address is `myapp:3000` because the application\n      # will be run within docker-compose as `myapp` service\n      - targets: [\"myapp:3000\"]\n```\n\nFull setup including the application code and `docker-compose.yml` can be found [here](https://github.com/yossisp/prometheus-grafana-tutorial). If `docker-compose up` is run three services are available:\n\n- the application server\n- Prometheus\n- Grafana\n\nLet's access Grafana at `localhost:3001`, the initial username and password are `admin`:\n\n- First, a datasource needs to be created in our case it will be Prometheus. Go to `localhost:3001/datasources`, click `Add data source`, select Prometheus whose URL should be `prometheus:9090` and click `Save & Test`.\n- Now a dashboard for our application can be created. Go to `localhost:3001/dashboard/new?orgId=1` and click `Add new panel`:\n\n  ![Grafana metric addition](/images/blog/grafana_metric.png)\n\nUnder metrics enter `up{job=\"my_app\"}`. Each Prometheus target is called a job and the query returns `0` if the application is not responding, else it returns `1`. The job can contain multiple targets, for example, there could be a producer and consumer microservice as part of the same job. `up` query is handy to set up an alert to notify if a service is down. This panel can be called `Uptime`.\n\n- The next panel we'll create will be for a gauge metric `nodejs_heap_space_size_used_bytes`. Click `Add new panel`, set `Panel title` to `Heap Usage` and set the metrics field to `nodejs_heap_space_size_used_bytes{job=\"my_app\"}`. This panel can be called `Heap Usage`. Go to `Field` menu and select bytes as field unit:\n\n  ![Select field unit](/images/blog/heap_bytes.png)\n\nNow we can see the heap usage graph:\n\n![Heap usage graph](/images/blog/heap_usage.png)\n\n- The last metric we'll add is the custom counter `home_requests_num`. Create new panel, name it `/home requests num` and set the metrics query to be `home_requests_num{job=\"my_app\"}`. After making a few requests to `/home` route this will produce the following graph:\n\n  ![Simple counter graph](/images/blog/simple_counter_graph.png)\n\nBecause counter metrics are always increasing such graph will not be very informative. That's why Prometheus provides `rate` and `irate` functions. `rate` calculates the per-second average rate of increase of the time series in the range vector. Let's set the metrics query to `rate(home_requests_num{job=\"my_app\"}[5m]) * 60`:\n\n![counter rate](/images/blog/counter_rate.png)\n\nThe graph now indeed provides some insights, like for example that at 12:34 there was a spike in requests. By default `rate` function calculates the rate of increase per-second that's why in the query above the result is multiplied by 60 so that we see the stats per minute. In addition `[5m]` means that the rate of increase over the last 5 minutes will be calculated.\n\nThere's also `irate` function which calculates the per-second instant rate of increase of the time series in the range vector based on the last two data points. It can be used for metrics which have sudden spikes of activity (like CPU activity) which `rate` will smooth out because `rate` calculates an average over an interval which is usually a couple of minutes. In practice however because `irate` calculates rate of increase based only on the last two data points it can also miss the spike event. The query would be:\n\n```\nirate(home_requests_num{job=\"my_app\"}[5m]) * 60\n```\n\nNone of those functions is perfect. [This](https://utcc.utoronto.ca/~cks/space/blog/sysadmin/PrometheusSubqueriesForSpikes) post recommends using intervals to catch sudden spikes:\n\n```\nmax_over_time((irate(home_requests_num{job=\"my_app\"}[45s]))[$__interval:10s]) * 60\n```\n\nThe problem with interval is that it currently [can't be used with alerts](https://github.com/grafana/grafana/issues/15168).\n\nIn Grafana multiple queries can be added in the same panel, so let's compare the results of rate/irate and interval query:\n\n![Comparison or rate/irate and interval queries](/images/blog/rate_irate_interval.png)\n\nAs can be seen in the screenshot both `irate` and interval show a sudden spike of requests more clearly than `rate`.\n\nRegarding counters and gauges it can clearly be seen that counters are more complicated that gauges because rate functions are usually used to make counters meaningful. This means that if a metric can be represented as a gauge it certainly should be implemented as such.\n\nI hope the above introduction to Prometheus and Grafana was helpful. I found [this](https://www.metricfire.com/blog/understanding-the-prometheus-rate-function/) link very helpful to better explain `rate` function, [this](https://valyala.medium.com/why-irate-from-prometheus-doesnt-capture-spikes-45f9896d7832) link explains `irate` in more detail while [this](https://utcc.utoronto.ca/~cks/space/blog/sysadmin/PrometheusRateVsIrate) article compares `rate` and `irate`.\n"},{"slug":"is-mongodb-lookup-really-a-left-outer-join","category":"blog","title":"Is MongoDB $lookup Really a Left Outer Join?","description":"MongoDB $lookup Join Behavior (With $unwind)","tags":["mongodb","lookup"],"body":"\nI had a case recently where I ran a [`$lookup`](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/#lookup-join-pipeline) stage in MongoDB aggregation. I had 2 collections: one which contains products (Product) and one which contains availability regions (Region). Each product may be available in certain regions, therefore Product documents have `regions` field which contains an array of regions the product is available in, for example:\n\n```js\n{\n    _id: 1,\n    name: \"Great Product\",\n    regions: [\n        11,\n        12\n    ]\n}\n```\n\nwhere each element in the `regions` array is an ObjectID of a Region document. In my aggregation I needed to return regions with their product. This can be done with a `$lookup` like so:\n\n```js\ndb.Region.aggregate([\n  {\n    $lookup: {\n      from: \"Product\",\n      let: {\n        id: \"$_id\",\n      },\n      pipeline: [\n        {\n          $match: {\n            $expr: {\n              $in: [\"$$id\", \"$regions\"],\n            },\n          },\n        },\n        {\n          $project: {\n            _id: 1,\n            name: 1,\n          },\n        },\n      ],\n      as: \"product\",\n    },\n  },\n  {\n    $unwind: \"$product\",\n  },\n])\n```\n\nThis worked but the results contained only the regions which were assigned a product. I visited MongoDB [documentation](https://docs.mongodb.com/manual/reference/operator/aggregation/lookup/index.html#join-conditions-and-uncorrelated-sub-queries) on `$lookup` stage which clearly states that `$lookup`:\n\n> Performs a **left outer join** to an unsharded collection in the same database to filter in documents from the â€œjoinedâ€ collection for processing.\n\nTranslating SQL terminology into \"MongoDB speak\" we get that even regions which don't have a product assigned should be added in the result. So I decided to investigate the `$unwind` stage. Interestingly enough, it turns out that `$unwind` has `preserveNullAndEmptyArrays` [parameter](https://docs.mongodb.com/manual/reference/operator/aggregation/unwind/#unwind-preservenullandemptyarrays): if `preserveNullAndEmptyArrays` is false (which is the default value) and the looked up document is null, missing, or an empty array then `$unwind` does not output a document! This is the very reason which caused me to believe that `$lookup` was performing a left inner join. The following query sets the `preserveNullAndEmptyArrays` to true which returns even regions without a product:\n\n```js\ndb.Region.aggregate([\n  {\n    $lookup: {\n      from: \"Product\",\n      let: {\n        id: \"$_id\",\n      },\n      pipeline: [\n        {\n          $match: {\n            $expr: {\n              $in: [\"$$id\", \"$regions\"],\n            },\n          },\n        },\n        {\n          $project: {\n            _id: 1,\n            name: 1,\n          },\n        },\n      ],\n      as: \"product\",\n    },\n  },\n  {\n    $unwind: {\n      path: \"$product\",\n      preserveNullAndEmptyArrays: true,\n    },\n  },\n])\n```\n\nYou can play with the query in the [playground](https://mongoplayground.net/p/eoO_nxRpxkP) as well.\n"},{"slug":"is-saving-passwords-in-memory-fail-safe","category":"blog","title":"Is Saving Application Secrets Or Passwords In Memory Fail-Safe?","description":"Is Saving Application Secrets Or Passwords In Memory Fail-Safe?","tags":["security","java"],"body":"<div style=\"display:flex;justify-content:center;\">\n  <div style=\"width:50%;padding-top:32px;padding-bottom:52px;\">\n    <img src=\"/images/blog/password.jpg\"\n      alt=\"Stealing Password\"\n    />\n  </div>\n</div>\n\n<span style=\"color:orange;text-decoration: underline;font-size:24px;\">TL;DR:</span>\n\n1. If an attacker gains access to a deployment instance they can get access to secrets saved in memory by performing a heap dump of the process.\n2. Saving secrets to `char` arrays in Java can possibly mitigate the risk but not prevent it completely.\n\n------\n\nThere're multiple ways to pass sensitive data (access tokens/passwords etc.) to an instance of an application deployment, for example:\n\n- environment variables/files.\n- using network (e.g. HTTPS requests) to get sensitive data.\n\nUsing network in theory is the safest method because the result of an HTTPS request which would return some secret is assigned to some variable in application memory so an attacker can't easily retrieve it from the memory as opposed to using environment variables because if someone gains access to the deployment machine, then printing environment variables is as easy as running `env` command in Linux.\n\nHowever, in the case the attacker gained access to the machine where the application runs (e.g. via SSH) they can easily perform a heap dump of the application memory. **Heap dumps in Java for example will contain the names of class methods, instance fields name and their values, static fields name and their values.** In addition the values of stack (local) variables are available, although their names according to my checks are not available.\n\nProduction deployments of Java usually do not include JDK for security reasons but are rather run on JRE. Assuming JDK will not be present in the deployment machine, the attacker will not be able to use command-line utilities shipped with JDK to produce heap dumps like `jmap`. However, if the attacker has access to the deployment machine `jmap` can be used to connect remotely or it can be installed in the deployment machine by the attacker. The point is that if an attacker has access to the machine they can generate a heap dump. \n\nLet's look at an example Java application made up of two classes: `Test.java` and `Worker.java`:\n\n```java\npublic class Worker {\n    public static String SOME_STATIC_VARIABLE = \"some static value\";\n    private String some_field = \"some field value\";\n    public void doSomething() throws InterruptedException {\n        String secret = \"some secret\"; // assume the value is the result of an HTTP request\n        Thread.sleep(1000 * 500); // simulate work\n        System.out.println(secret); // prevent GC to dispose of `secret` until this line\n    }\n}\n\npublic class Test {\n    public static void main(String[] args) throws InterruptedException {\n        Worker worker = new Worker();\n        worker.doSomething();\n    }\n}\n```\n\nOnce the heap dump is generated it can be analyzed with a variety of tools, for example with [Memory Analyzer (MAT)](https://www.eclipse.org/mat/). Once the heap dump is opened in MAT navigate to the highlighted icon in the screenshot below and select `Java Basics->Thread Overview and Stacks`, then click `Finish`:\n\n![MAT intial menu navigation](/images/blog/initial.png)\n\nIn the resulting list of threads running in the program, right-click the `main` thread and navigate to `doSomething` method as in the screenshot below: \n\n![static variables](/images/blog/statics.png)\n\nIn the sidebar to the left under `Statics` tab we can clearly see the name of the static variable in `Worker` class and its value. Next the name and value of the instance field of `Worker` class can be seen:\n\n![instance variables](/images/blog/instance_field.png)\n\nLastly, we can see the value of the local variable `secret` however its name is not available:\n\n![local variables](/images/blog/local_variable.png)\n\nIf secrets are not saved to class fields but rather to primitive local variables like string it may be quite hard for an attacker to figure out what service is a given secret for. However, because usually the response of an HTTP call is in JSON form, Java application will have to parse the JSON using some class to model JSON fields which means that both field names and values will be available to the attacker until the class instance is garbage collected.\n\nOne of the best practices in Java is to assign secrets to variables of type `char[]` instead of `String` as described in [Java â„¢ Cryptography Architecture (JCA) Reference Guide](https://docs.oracle.com/javase/6/docs/technotes/guides/security/crypto/CryptoSpec.html#PBEEx):\n\n>It would seem logical to collect and store the password in an object of type java.lang.String. However, here's the caveat: Objects of type String are immutable, i.e., there are no methods defined that allow you to change (overwrite) or zero out the contents of a String after usage. This feature makes String objects unsuitable for storing security sensitive information such as user passwords. You should always collect and store security sensitive information in a char array instead.\n\nSuppose the secret is assigned to a `String` variable `secret` and then the secret is consumed by some class and you want to \"zero\" the variable to delete secret value:\n\n```java\nString secret = \"some value\";\n// consume the variable\nsecret = null; // delete `some value`\n```\n\nBecause strings are immutable in Java, until the previous value of `secret` is garbage collected it will \"live\" in memory and may be accessed via heap dump. If on other hand if it was saved as `char` array, it could be effectively overwritten as such:\n\n```java\nchar[] secret = new char[10];\n// copy secret data into `secret` array\nsecret = null; // zero out the char array \n```\n\n Of course if a heap dump was performed before the `char` array variable was zeroed this still wouldn't prevent security leak.\n"},{"slug":"is-sqs-really-exactly-once","category":"blog","title":"Does AWS SQS Really Provide Exactly Once Guarantees?","description":"Does Amazon (AWS) SQS Really Provide Exactly Once Guarantee?","tags":["aws","sqs","message-queue","distributed"],"body":"\nExactly-once delivery in distributed message queues is admittedly one of the hardest problems in software engineering:\n\n![Two hard distributed problems](/images/blog/two-problems.png)\n\nAchieving exactly-once semantics in Apache Kafka Streams is as easy as setting the property `processing.guarantee=exactly_once`. In Apache Kafka this is a [bit more complex](https://kafka.apache.org/documentation/#semantics), one needs to use the transactional producer but also design consumer logic in a specific way to support exactly-once delivery.\n\nAmazon SQS is a fully managed message queue service. It's available in two flavors:\n- Standard Queue: it provides at-least once delivery, best-effort ordering and unlimited throughput.\n- FIFO Queue: it has throughtput limitations (up to 30k messages per second) but guarantees correct message ordering. The eye-catching feature of FIFO queue is its claim to [exactly-once processing](https://aws.amazon.com/sqs/features/):\n> Exactly-Once Processing: A message is delivered once and remains available until a consumer processes and deletes it. Duplicates aren't introduced into the queue.\n\nThis claim immediately caught my attention since it's a quite bold claim. I delved into AWS SQS docs for more details. In its [developer guide](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/FIFO-queues-exactly-once-processing.html) dedicated to exactly-once processing it says the following:\n> Unlike standard queues, FIFO queues don't introduce duplicate messages. FIFO queues help you avoid sending duplicates to a queue. If you retry the SendMessage action within the **5-minute** deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue.\n\nThe first thing to note here is the word __processing__. While Apache Kafka and Apache Kafka Streams offer __exactly-once delivery__ AWS SQS offers __exactly-once processing__. The second caveat is that according to AWS SQS own docs:\n> If you retry the SendMessage action within the **5-minute** deduplication interval, Amazon SQS doesn't introduce any duplicates into the queue.\n\nSo if SQS FIFO queue producer is down for more than 5 minutes because there's a network issue and the same message is sent again then it actually **will become duplicate** (this point is also echoed in this excellent AWS [article](https://aws.amazon.com/blogs/developer/how-the-amazon-sqs-fifo-api-works/) on SQS architecture where you can read more on AWS take regarding exactly-once semantics). This certainly means that SQS FIFO queue can't be fully relied on to prevent duplicate messages. If you want to use SQS FIFO queues and your service needs to guarantee that the same message is not processed twice you must implement idemponent processing mechanism. For example, update database with messages that were processed and check database whether new message was already processed before acting on it.\n\nIn my opinion SQS docs regarding exactly-once are a bit misleading, what do you think? Feel free to comment on [Twitter](https://twitter.com/SpektorYossi/status/1567970808999596033?s=20&t=cEtsZxyGOfAetuW4jo-CIg)."},{"slug":"javascript-regex-objects-gotchas","category":"blog","title":"Javascript Regular Expressions Objects Gotchas","description":"Javascript Regular Expressions Objects Gotchas","tags":["javascript","regex"],"body":"\nI recently came across an interesting behavior when working with regular expressions (regex) in Javascript. I came across the following piece of code:\n\n```js\nconst str = \"food is great!\"\nconst regex = /foo.*/g\n\nconsole.log(regex.test(str)) // true\nconsole.log(regex.exec(str)) // null\n```\n\nI was surprised that `regex.exec(str)` returns `null` because `regex.test(str)` returns `true`. After reading this [MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/exec) reference I learned that Javascript regex objects with global or sticky flags are stateful. In particular they store `lastIndex` property:\n\n```js\nconst str = \"food is great!\"\nconst regex = /foo.*/g\n\nconsole.log(str.length) // 14\nconsole.log(regex.test(str)) // true\nconsole.log(regex.lastIndex) // 14\nconsole.log(regex.exec(str)) // null\nconsole.log(regex.lastIndex) // 0\nconsole.log(regex.exec(str)) // [\"food is great!\"]\n```\n\n`lastIndex` specifies the index within the string at which to start the next match. The property is incremented (according to the rules described [here](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/RegExp/lastIndex)) when `exec()` or `test()` methods are used. Therefore, when `regex.test(str)` is called `lastIndex` is set to `14` because there're no other matches for `/foo.*/g` within the string. When `regex.exec(str)` is executed it searches for match between `lastIndex` and the rest of the string which is an empty string by now and no match is found. After the last `exec()` call `lastIndex` is set to `0` therefore when `regex.exec(str)` is called again a match is found because the search now starts from the beginning of the string. This behavior is important to take into account because one may write the following code:\n\n```js\nconst str = \"food is great!\"\nconst regex = /foo.*/g\nif (regex.test(str)) {\n  const match = regex.exec(str)\n  // do something...\n}\n```\n\nbut the code will not work as expected due to the reasons described above. In such a case it will suffice to use a single call to `exec()`:\n\n```js\nconst str = \"food is great!\"\nconst regex = /foo.*/g\nconst match = regex.exec(str)\nif (match) {\n  // do something...\n}\n```\n"},{"slug":"javascript-to-golang-first-impressions","category":"blog","title":"Javascript to Golang: First Impressions","description":"First Impressions of Golang From a Javascript Developer","tags":["golang","javascript"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;padding-bottom:50px;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/javascript.png\"\n            alt=\"Javascript\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 32px;padding-left:16px;padding-right:16px;\"> vs </span>\n    <div style=\"width:30%;\">\n    <img src=\"/images/blog/golang.jpg\"\n        alt=\"Golang\"\n        />\n    </div>\n</div>\n\nI started learning Golang recently and after completing a project I wanted to share some of the first impressions of transitioning from Javascript/Node.js to Go.\n\n1. **Types**. Types are built-in in Go. This is way cooler than Typescript because there's no extra setup effort, all Go libraries are typed so absolutely all of the ecosystem has types. This also means that there's no need for tools like Webpack and build step is so much simpler!\n\n2. **Go modules** are the equivalent to NPM packages. However, Go's package manager is very basic, for example, you can't execute scripts from it. In my project I had to use [make](https://www.gnu.org/software/make/manual/make.html) to launch scripts.\n\n3. **There're no truthy or false values in Go** (similarly to Java). The following code will not compile:\n\n```go\ntest := 5\nif test {\n    // ... do something\n}\n```\n\n4. **Structs in Go are values**, like in C. This means that after passing a struct as function parameter, changing the struct inside that function and returning from the function the struct will not retain the changes:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\ntype test struct {\n\tnumber int\n}\n\nfunc change(num test) test {\n\tnum.number = 7\n\treturn num\n}\n\nfunc main() {\n\tnum := test{ 5 }\n\tchange(num)\n\tfmt.Println(num.number) // prints 5\n}\n```\n\nThis is because function parameters are copied by value and structs are values in Go. In case you want to mutate a struct from inside another function it should be passed as a pointer. Passing struct as a pointer is also preferred if the struct is big so that all of its fields are not copied.\n\n5. **Exports start with a capital letter**. I really liked this Go feature because it's so simple and straightforward as opposed to Javascript where first of all there're two export styles (CommonJS and ES6) and you need to both export and import a variable/function. In Go just name a variable or a function you want to export with a capital letter and it will automatically be available to other packages.\n\n6. **Functions can return tuples**. This is nice but is mostly used to return a result and an error (which is [kind of like](http://callbackhell.com) Javascript callbacks having a result and error parameters).\n\n7. **Defer** statement. Simply put: \"A defer statement defers the execution of a function until the surrounding function returns.\" - [Go tour](https://tour.golang.org/flowcontrol/12). This is nice in terms of developer self-discipline: immediately after you allocate a resource you add a `defer` statement to clean up the resource when it's not needed anymore, but takes some time to get used to. Specifically to the fact that `defer` statements are executed in [LIFO](<https://en.wikipedia.org/wiki/Stack_(abstract_data_type)>) order.\n\n8. **const** has different meaning in Go. In Javascript `const` is used to tell the compiler that a variable can't be re-assigned to. However, in Go **const** can only be used with scalar types and has a bit different [role](https://blog.golang.org/constants).\n\n9. **Go also supports closures**.\n\n10. **There's no `this` in Go**. Go doesn't have classes but methods can be assigned to types via method receivers. At first it's a bit strange but you quickly get used to it.\n\n11. **Arrays are deep-copied when assigned to another array**. For example:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n)\n\nfunc main() {\n\tarr1 := [3]int{1,2,3}\n\tarr2 := arr1\n    arr1[0] = 5\n    fmt.Println(arr1) // prints [5 2 3]\n\tfmt.Println(arr2) // prints [1 2 3]\n}\n```\n\nMost of the time this is not desired so instead slices can be used, which are similar to arrays and can be thought of as \"views\" of an array. One of the nicest features of arrays and slices is that you can easily create subsets. For example, `arr2[1:2]` is a subset which contains only the second and third element. This is so much more concise than Javascript!\n\n12. **String interpolation**. Go doesn't have [template literals](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals) like in Javascript but it has [printf](https://en.wikipedia.org/wiki/Printf_format_string)-style utilities.\n\n13. **Error handling**. Go doesn't have `try/catch`. Instead it's common that functions return error as the second return parameter. Errors can be \"thrown\" using `panic` function and they can be caught by `recover` function. However, the usage of `recover` is kind of verbose and less intuitive than `try/catch` facility.\n\n14. **Unit-testing**. Go has built-in unit-testing functionality which is really cool. All you need to do is to create a file in the same package you want to test and name it `something_test.go`. Inside that file you just write unit tests which are functions which receive special parameters and Go takes care of the rest. Test suites can then be run from command line via `go test` command.\n\n15. **JSON**. JSON is a first-class citizen in Javascript. Parsing, stringifying and changing JSON is effortless. This simply isn't the case in Go because it's a typed language. In order to create JSON objects one can use `map` as an ad-hoc solution but mostly you'll want to create a custom struct and declare all the necessary fields. In the latter case all fields must start with capital letter because they must be exported so that `json` package can access them. If you want the resulting JSON fields to start with lower-case letter then you can use Go's JSON directives as follows:\n\n```go\ntype Car struct {\n    Wheel int `json:\"wheel\"`\n    WindShield int `json:\"windShield\"`\n}\n```\n\n---\n\n## Overall Impression\n\nI didn't mention all of the differences of course because there're a lot of them (concurrency model, pointers etc.). However, the overall impression is that Go is a very friendly language. It's simple on purpose both in terms of its syntax and in terms of the features that it doesn't have (e.g. classes, inheritance). Pointers handling is greatly simplified in comparison to C (there's no pointer arithmetic and there's automatic pointer dereference). The standard library is quite comprehensive. I felt that debugging and build work out of the box as opposed to always having some kind setup in Javascript/Node.js which involves Babel.js or Webpack + Babel.js. Types are built-in and there's automatic type inference.\n\nI immediately missed Javascript JSON handling, destructuring and template literals though. Also having to write `== nil` or `!= nil` in order to check for nullness is kind of a nuissance (in Go you have write `if someVariable != nil { ... }` instead of just `if (someVariable) { ... }` in Javascript). For me most of the learning curve was spent on goroutines and channels which are Go concurrency facilities. Yet in my opinion the advantages of Go far outweigh its drawbacks. I would happily switch to Go for server-side projects at work if I could.\n"},{"slug":"mongodb-vs-sql-sorting","category":"blog","title":"MongoDB vs SQL: Sorting","description":"MongoDB vs SQL: Sorting","tags":["mongodb","sql","sorting"],"body":"\n<div style=\"display:flex;align-items:center;padding-left:10%;padding-right:10%;padding-bottom:50px;\">\n    <div style=\"width:30%;\">\n        <img src=\"/images/blog/mongodb.png\"\n            alt=\"MongoDB\"\n            style=\"margin:0;\"\n            />\n    </div>\n        <span style=\"font-size: 32px;padding-left:16px;padding-right:16px;\"> vs </span>\n    <div style=\"width:30%;\">\n    <img src=\"/images/blog/sql.png\"\n        alt=\"SQL\"\n        />\n    </div>\n</div>\n\nRecently, I had to add several API endpoints which provide pagination, filter and sort parameters to the client while using MongoDB as database. Implementing pagination and filtering wasn't complicated however I hit a major setback when implementing sorting logic. The reason was that sorting is not enabled on virtual fields in MongoDB. This is as opposed to SQL where calculated fields can be sorted. For applications which have a lot of virtual fields the above-mentioned MongoDB missing feature can be a major disadvantage (for example if database data has a lot of fields which need to be translated based on user language).\n\nEventually, I ended up with creating a script which would populate the database with the virtual fields which is reasonable because the database is relatively small and is not expected to become very big. However, for applications with large databases this may not be a reasonable solution, of course. In that case perhaps migrating to an SQL based database may be needed.\n"},{"slug":"multiple-and-conditions-on-same-column-sql","category":"blog","title":"How To Make SQL Queries With Multiple AND Conditions On Same Column in SQL","description":"How To Make SQL Queries With Multiple AND Conditions On Same Column in SQL.","tags":["sql"],"body":"\nRecently, a colleague approached me to help with building an SQL query. It was an interesting problem in my opinion and the solution to it is quite generic and can be applied to many similar problems.\n\nMy colleague had a table of producers and consumers, where consumer to producer was a many-to-many relationship. She wanted to get all consumers which interacted with **at least all** the producers from a given list.\n\nThis is a sample table called `mytable`:\n\n| id  | consumer | producer |\n| --- | -------- | -------- |\n| 1   | c1       | p1       |\n| 2   | c1       | p2       |\n| 3   | c2       | p1       |\n| 4   | c2       | p2       |\n| 5   | c3       | p1       |\n| 6   | c3       | p2       |\n| 7   | c3       | p3       |\n| 8   | c4       | p1       |\n\nIn this table given a list of producers `p1` and `p2`, the output should be consumers `c1`, `c2` and `c3`. `c4` will not be in the output because it only interacted with `p1` but not with `p2`. Also `c3` will be in the output because it's enough that it interacts with `p1` and `p2` even though `c3` also interacts with `p3`.\n\n1. [SQL solution](#sqlsolution)\n2. [PostgreSQL solution](#postgresqlsolution)\n\n## SQL solution <a name=\"sqlsolution\"></a>\n\nThe first instinct is to use SQL `IN` operator, something like:\n\n```sql\nSELECT * FROM mytable\nWHERE producer IN ('p1', 'p2')\n```\n\nHowever this will not produce correct results because SQL `IN` operator is just a shorthand for multiple `OR` conditions. However, we really need multiple `AND` conditions. Multiple `AND` conditions can't be used on the same column, therefore after `IN` is used, we can first group by consumer and then make sure that the number of rows equals the number of producers in the input:\n\n```sql\nSELECT consumer FROM mytable\nWHERE producer IN ('p1', 'p2')\nGROUP BY consumer\nHAVING COUNT(*) = 2\n```\n\nThe above query correctly returns the list of consumers (`c1`, `c2` and `c3`). It's important to note that if the combination of consumers and producers wasn't unique, then `DISTINCT` would need to be added into rows count:\n\n```sql\nSELECT consumer FROM mytable\nWHERE producer IN ('p1', 'p2')\nGROUP BY consumer\nHAVING COUNT(DISTINCT producer) = 2\n```\n\nFinally, in order to get the rows of `mytable` which satisfy the original condition the following query can be executed:\n\n```sql\nSELECT * FROM mytable\nWHERE consumer IN (\n    SELECT consumer FROM mytable\n    WHERE producer IN ('p1', 'p2')\n    GROUP BY consumer\n    HAVING COUNT(*) = 2\n)\n```\n\nYou can run the sample query in this playground [link](https://www.db-fiddle.com/f/hHCyXADG7dPGgsdf6kVf9d/8).\n\n## PostgreSQL solution <a name=\"postgresqlsolution\"></a>\n\nIf you're using PostgreSQL there's another solution using `array_agg` aggregate [function](https://www.postgresql.org/docs/9.5/functions-aggregate.html). First a list of producers can be aggregated per each consumer using `array_agg`. Secondly, `@>` contains [operator](https://www.postgresql.org/docs/current/functions-array.html) can be used to check whether input array of producers is contained in the result of `array_agg`:\n\n```sql\nSELECT * FROM (\n  SELECT consumer, array_agg(producer) producers FROM mytable\n  GROUP BY consumer\n) AS t WHERE t.producers @> '{p1, p2}'\n```\n\nYou can run the sample query in this playground [link](https://www.db-fiddle.com/f/hHCyXADG7dPGgsdf6kVf9d/8).\n"},{"slug":"multiple-commands-per-dockerfile","category":"blog","title":"How To Execute Multiple Commands Per Dockerfile","description":"How To Execute Multiple Commands Per Dockerfile, How To Efficiently Dockerize .NET App","tags":["docker",".net"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n    <img src=\"/images/blog/docker.svg\"\n            alt=\"Docker Logo\"\n            style=\"margin:0;\"\n            />\n</div>\n\nI dabbled into .NET recently and after finishing the project I ended up with 2 microservices which were .NET projects under the same [solution](https://docs.microsoft.com/en-us/visualstudio/ide/solutions-and-projects-in-visual-studio?view=vs-2019). It turns out that when dockerizing the microservices their build instructions are identical. The only difference is the command which needs to be invoked in order to start the microservice.\n\nBelow you can see a sample Dockerfile for a .NET application with multiple microservices:\n\n```docker\nFROM mcr.microsoft.com/dotnet/sdk:5.0 AS build\nWORKDIR /app\n\n# install necessary packages\nCOPY *.sln .\nCOPY MicroserviceA/*.csproj ./MicroserviceA/\nCOPY MicroserviceB/*.csproj ./MicroserviceB/\nCOPY Common/*.csproj ./Common/\nRUN dotnet restore ./MySolution.sln\n\n# build executables of all projects/microservices\nCOPY . ./\nRUN dotnet publish -c Release -o out\n\n# Copy executables\nFROM mcr.microsoft.com/dotnet/aspnet:5.0\nWORKDIR /app\nCOPY --from=build /app/out .\nCMD [\"dotnet\", \"MicroserviceA.dll\"]\n```\n\nAs you can see the `CMD` directive invokes `MicroserviceA`, but what about `MicroserviceB`? Because all previous stages are the same for `MicroserviceB` it's redundant to have another Dockerfile dedicated just to `MicroserviceB`.\n\nLuckily, the `CMD` specified in the Dockerfile is only a default. This means that it can be [overridden](https://docs.docker.com/engine/reference/run/#cmd-default-command-or-options). Same logic goes for `ENTRYPOINT` [directive](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime).\n\nThis means that executing `docker run myimage` on the image built by the Dockerfile above will start `MicroserviceA` by default. However, executing `docker run myimage dotnet MicroserviceB.dll` will override the default `CMD` and invoke `MicroserviceB` instead. As a result the Dockerfile can be re-used for any microservice project in the .NET app.\n\nAn even cleaner solution is to use `docker-compose` `command` option in order to specify each microservice:\n\n```\nversion: \"3.9\"\nservices:\n  microservicea:\n    image: myimage/microservicea\n    build: .\n    command: [\"dotnet\", \"MicroserviceA.dll\"]\n  microserviceb:\n    image: myimage/microserviceb\n    build: .\n    command: [\"dotnet\", \"MicroserviceB.dll\"]\n```\n\nI hope the post shed some light on overriding Dockerfile default `CMD` and on organizing your .NET apps!\n"},{"slug":"performing-multiple-aggregation-pipelines-in-single-mongodb-stage","category":"blog","title":"How To Perform Multiple Aggregation Pipelines Within a Single Stage In MongoDB","description":"How To Perform Multiple Aggregation Pipelines Within a Single Stage In MongoDB","tags":["mongodb","aggregation"],"body":"\nRecently, I had to add pagination feature to our website (our data resides in MongoDB). I wanted to define a single MongoDB aggregation pipeline which would fetch the required data (products) based on the criteria the user selects for the `n`-th page. But I also wanted the aggregation to return the total amount of products matched in the same aggregation. The last part turned out a bit tricky.\n\nFirst, MongoDB aggregation pipeline stages work sort of like Unix \"pipes\": you do something in one stage and the next stage receives the output of the previous one. So let's say these are the stages needed to output the products:\n\n```js\nProductCollection.aggregate([\n  { $match: { score: { $gt: 0 } } },\n  {\n    $lookup: {\n      from: \"ProductSupplier\",\n      localField: \"productSupplierId\",\n      foreignField: \"_id\",\n      as: \"productSupplier\",\n    },\n  },\n  {\n    $sort: {\n      price: 1,\n    },\n  },\n  {\n    $skip: offset,\n  },\n  {\n    $limit: productsPerPage,\n  },\n])\n```\n\n`{ $count: 'totalCount' }` stage can be used to return the overall number of products. However, it can't be used anywhere within the pipeline because its result is simply (if the number of products is `4`):\n\n```json\n{ \"totalCount\": 4 }\n```\n\nwhich means all products data will be lost.\n\nFortunately, MongoDB provides an option to perform multiple pipelines within a single aggregation. It's called [\\$facet](https://docs.mongodb.com/manual/reference/operator/aggregation/facet/). In `$facet` each child stage is run independently. This is exactly what I needed: 2 pipelines which are completely independent of each other:\n\n```js\nProductCollection.aggregate([\n  { $match: { score: { $gt: 0 } } },\n  {\n    $facet: {\n      totalMatchedCount: [{ $count: \"count\" }],\n      products: [\n        {\n          $lookup: {\n            from: \"ProductSupplier\",\n            localField: \"productSupplierId\",\n            foreignField: \"_id\",\n            as: \"productSupplier\",\n          },\n        },\n        {\n          $sort: {\n            price: 1,\n          },\n        },\n        {\n          $skip: offset,\n        },\n        {\n          $limit: productsPerPage,\n        },\n      ],\n    },\n  },\n  {\n    $unwind: \"$totalMatchedCount\",\n  },\n])\n```\n\nAfter such query the following result will be returned:\n\n```js\n{\n  products: [\n    // ... products\n  ],\n  totalMatchedCount: 100 // if 100 products were matched\n}\n```\n"},{"slug":"prisma-with-mongodb-setup","category":"blog","title":"How To Set Up prisma.io With Existing Mongodb Database","description":"How To Set Up prisma.io With Existing Mongodb Database","tags":["graphql","prisma.io","mongodb"],"body":"\n[Prisma](https://www.prisma.io/) does a lot of cool stuff but I think it's primarily known for allowing to perform CRUD operations on a database via GraphQL. It started out with support for SQL databases but it now supports Mongodb as well.\n\n**Prisma is great because it automatically:**\n\n1. creates queries/mutations/subscriptions for each GraphQL type.\n2. allows for powerful options to select the required documents.\n3. creates input types which correspond to the declared entity types (useful in mutations).\n\nAlthough several tutorials exist on how to create new mongodb instance from scratch and hook it up to prisma, I couldn't find comprehensive tutorials on how to connect prisma to an existing database so I hope this guide will help someone.\n\nFirst, create a folder in your project called `prisma`. Add docker-compose file `prisma-server.yml` with the following content:\n\n```yaml\nversion: \"3\"\nservices:\n  prisma:\n    image: prismaGraphQL/prisma:1.34\n    restart: always\n    ports:\n      - \"4466:4466\"\n    environment:\n      PRISMA_CONFIG: |\n        managementApiSecret: __YOUR_MANAGEMENT_API_SECRET__\n        port: 4466\n        databases:\n          default:\n            connector: mongo\n            uri: mongodb://pathToYourDb/admin\n            database: yourDbName\n```\n\nA couple of notes:\n\n1. `managementApiSecret` is needed to enable authentication of requests to prisma server.\n2. For local developement on Mac, you need to define `uri` as `mongodb://host.docker.internal:27017/admin` assuming your mongodb instance is running on the host machine itself. This is because prisma is running from a docker instance and docker's networking namespace is not the same as of your host machine. On Linux you can use `host` [option](https://docs.docker.com/network/network-tutorial-host/) when starting docker container.\n3. Fun fact: you may be wondering what the vertical slash (`|`) is for. In YAML files it is used for multi-line input. So `PRISMA_CONFIG` is just one big string.\n\nYou can start the prisma server by `docker-compose -f prisma-server.yml up -d` from inside `prisma` folder.\n\nNow install prisma cli tool `npm i -g prisma`. Add another file in the prisma folder called `prisma.yml` and paste the content into it:\n\n```yaml\nendpoint: http://localhost:4466\ndatamodel: datamodel.prisma\ndatabaseType: document\ngenerate:\n  - generator: javascript-client\n    output: ./generated/prisma-client/\n```\n\nThe file contains deployment and cofiguration settings for prisma service. It's important to specify `databaseType` as document because we're dealing with mongodb.\n\nThe last major step is to generate `datamodel.prisma` file. `datamodel.prisma` is prisma's representation of your GraphQL schema. It's based on SDL but adds some directives of its own. If you're starting with a clean database you would just write the GraphQL schema for each collection from scratch. But if you have a lot of collections it would be a more time-consuming process. Luckily, prisma cli comes with `introspect` option which will help generate the datamodel from existing documents. The generation is done by random sampling of existing documents (looks like [50](https://github.com/prisma/prisma/issues/3529)). My generated datamodel was very accurate and definitely saved a lot of time.\n\nWhen you run `prisma introspect` the cli starts an interactive session where you first select the type of database you have (mongodb in our case). Then you need to enter your mongodb connection string which was `mongodb://localhost:27017/yourDbName` and lastly you need to select the schema you want to introspect which is `yourDbName`. Finally, prisma cli will create a file `datamodel-id.prisma`. Rename it to `datamodel.prisma` to match the name in `prisma.yml`.\n\nThe next step is to run `prisma generate`. This will take the datamodel you defined and generate the prisma client which can be used in your application in order to write custom resolvers and perform advanced queries/mutations. In my case client generation failed because of the error with incorrect `scalarList` directive setting. For example, one of type fields had:\n\n```\nmethod: [String] @scalarList(strategy: RELATION)\n```\n\nThis won't work in NoSQL database like mongodb therefore\n\n```\nmethod: [String] @scalarList(strategy: EMBEDDED)\n```\n\nneeds to be specified in `datamodel.prisma`. Lastly, run `prisma deploy` to deploy all the settings. This is it, you can now go to `http://localhost:4466` to open prisma GraphQL playground and shoot some queries. If you want to find out more about using mongodb with prisma check out this [page](https://www.prisma.io/docs/datamodel-and-migrations/datamodel-MONGO-knun/#sdl-directives) and [this](https://www.prisma.io/docs/releases-and-maintenance/features-in-preview/mongodb-b6o5/).\n"},{"slug":"project-architecture-testing-in-java-applications","category":"blog","title":"Project Architecture Testing In Java Applications","description":"Project Architecture Testing In Java Applications using ArchUnit and JMolecules","tags":["architecture","ddd","java","jmolecules"],"body":"\n![ArchUnit Logo](/images/blog/archunit_logo.png)\n\n1. [What Is Architecture Testing?](#what-is-architecture-testing)\n2. [How ArchUnit Works?](#how-archunit-works)\n3. [Common Use Cases](#common-use-cases)\n    1. [Classes With a Certain Name Reside In Certain Package](#classes-with-a-certain-name-reside-in-certain-package)\n    2. [Classes Are Not Allowed To Use Other Classes](#classes-are-not-allowed-to-use-other-classes)\n    3. [Cycles Between Packages Are Not Allowed](#cycles-between-packaged-are-not-allowed)\n4. [Architecture Metrics](#architecture-metrics)\n5. [Testing Layer/Onion/Hexagonal Architecture](#testing-layer-onion-hexagonal-architecture)\n6. [Incremental Rules Adoption](#incremental-rules-adoption)\n7. [Rules Generation Based On PlantUML Diagram](#rules-generation-based-on-plantuml-diagram)\n8. [Summary](#summary)\n\n### <a name=\"what-is-architecture-testing\"></a>What Is Architecture Testing?\nAll of us are familiar with unit/integration/end-to-end testing. There's one more area of testing that we can add: architecture testing which will verify that a project conforms to certain architecture rules, for instance:\n- how do we enforce that model layer doesn't use low-level infrastructure utilities?\n- how circular dependendencies are prevented?\n\nand the list goes on.\n\nNot a lot of projects have such artchitecture rules in the first place. Often times developers find themselves racing fast to complete proof of concept projects or deadlines such that having to enforcing a certain architecture style is a luxury they cannot afford. That being said as the project grows it's crucial to have defined project structure else it becomes harder to find components, to refactor and as a result project velocity suffers.\n\nEnter [ArchUnit](https://www.archunit.org/) for JVM applications which [aims](https://www.archunit.org/motivation) to bring and enforce structure in projects. While there're other projects in this area like [AspectJ](https://www.eclipse.org/aspectj/), [Checkstyle](https://checkstyle.sourceforge.io/) or [FindBugs](https://findbugs.sourceforge.net/) according to [ArchUnit](https://www.archunit.org/motivation):\n> each of those tools also has some limitations you might run into, if your rules become more complex. This might be as simple as not being able to specify a pointcut to only apply to interfaces in AspectJ or no back references within pointcuts. Other tools are not even able to define complex rules, like AspectJ allows, at all. Furthermore, you might need to learn a new language to specify the rules, or need different infrastructure to evaluate them.\n\n### <a name=\"how-archunit-works\"></a>How ArchUnit Works?\nArchUnit works by analyzing Java [bytecode](https://www.archunit.org/userguide/html/000_Index.html#_introduction) and building a [graph](https://www.archunit.org/userguide/html/000_Index.html#_domain) which represents a project structure. Essentially, we can add tests which call ArchUnit APIs to verify that the project structure conforms to the rules which were defined. For example, given the following Maven project:\n```\n.\n|-- pom.xml\n|-- src\n|   |-- main\n|   |   `-- java\n|   |       `-- example\n|   |           `-- order\n|   |               `-- Order.java\n```\nthe below JUnit 5 test checks that all classes whose name contains the word `Order` reside in `order` <a name=\"name-example\"></a>package:\n```java\n    @Test\n    public void orderRelatedEntitiesResideInOrderPackage() {\n        // Arrange\n        JavaClasses importedClasses = new ClassFileImporter().importPackages(\"example\");\n        ArchRule rule = classes().that().haveNameMatching(\".*Order.*\")\n                .should().resideInAPackage(\"..order..\")\n                .as(\"Order related entities should reside in package order\");\n\n        // Assert\n        rule.check(importedClasses);\n    }\n```\n\n### <a name=\"common-use-cases\"></a>Common Use Cases\nBelow are some common checks that can be performed using ArchUnit. It's worth mentioning that it has direct integration with JUnit however it can be used standalone as well.\n\n#### <a name=\"classes-with-a-certain-name-reside-in-certain-package\"></a>Classes With a Certain Name Reside In Certain Package\nMaking sure that classes with a certain name reside under certain packages as in the example [above](#name-example).\n\n#### <a name=\"classes-are-not-allowed-to-use-other-classes\"></a>Classes Are Not Allowed To Use Other Classes\n```java\n    @Test\n    public void orderShouldNotUseDatabaseConfiguration() {\n        // Arrange\n        JavaClasses importedClasses = new ClassFileImporter().importPackages(\"example\");\n        ArchRule rule = noClasses().that().resideInAPackage(\"..order..\")\n                .should().accessClassesThat().haveNameMatching(\".*DatabaseConfiguration.*\")\n                .as(\"Order entities should not use DatabaseConfiguration directly\");\n\n        // Assert\n        rule.check(importedClasses);\n    }\n```\n\n#### <a name=\"cycles-between-packaged-are-not-allowed\"></a>Cycles Between Packages Are Not Allowed\n```java\n    @Test\n    public void noCyclesBetweenPackages() {\n        // Arrange\n        JavaClasses importedClasses = new ClassFileImporter().importPackages(\"example\");\n        SliceRule sliceRule = SlicesRuleDefinition.slices().matching(\"..example.(*)..\")\n                .should().beFreeOfCycles()\n                .as(\"There should be no cyclic dependencies between packages\");\n\n        // Assert\n        sliceRule.check(importedClasses);\n    }\n```\nAs you can see ArchUnit allows to define powerful checks with relative ease. More use cases can be found [here](https://www.archunit.org/userguide/html/000_Index.html#_what_to_check).\n\n### <a name=\"architecture-metrics\"></a>Architecture Metrics\nInterestingly, certain architecture aspects can be measured with several approaches existing to this end. ArchUnit provides implementations of several such [measurements](https://www.archunit.org/userguide/html/000_Index.html#_software_architecture_metrics):\n- cumulative dependency metrics by John Lakos\n- component dependency metrics by Robert C. Martin\n- visibility metrics by Herbert Dowalil\n\nFor example, with [cumulative dependency metrics](https://www.archunit.org/userguide/html/000_Index.html#_cumulative_dependency_metrics_by_john_lakos) by John Lakos the basic idea of such measurement is to\n> calculate the `DependsOn` value for each component, which is the sum of all components that can be transitively reached from some component including the component itself.\n\nSuch measurement can provide interesting insights into a project. Below is an example test which calculates cumulative dependency metrics:\n```java\n    @Test\n    public void johnLakosArchitectureMetrics() {\n        JavaClasses classes = new ClassFileImporter().importPackages(\"example\");\n        Set<JavaPackage> packages = classes.getPackage(\"example\").getSubpackages();\n        MetricsComponents<JavaClass> components = MetricsComponents.fromPackages(packages);\n        LakosMetrics metrics = ArchitectureMetrics.lakosMetrics(components);\n\n        System.out.println(\"CCD: \" + metrics.getCumulativeComponentDependency());\n        System.out.println(\"ACD: \" + metrics.getAverageComponentDependency());\n        System.out.println(\"RACD: \" + metrics.getRelativeAverageComponentDependency());\n        System.out.println(\"NCCD: \" + metrics.getNormalizedCumulativeComponentDependency());\n    }\n```\n### <a name=\"testing-layer-onion-hexagonal-architecture\"></a>Testing Layer/Onion/Hexagonal Architecture\n[jMolecules](https://github.com/xmolecules/jmolecules#ideas-behind-jmolecules) is another project which aim is to:\n> explicitly express architectural concepts for easier code reading and writing.\n\nIt provides a set of Java annotations to express various architecture concepts e.g. `@DomainLayer`/`ApplicationLayer` etc. annotations. We can perform powerful architecture checks when combining jMolecules and ArchUnit for example, we can leverage [jMolecules ArchUnit integration](https://github.com/xmolecules/jmolecules-integrations/tree/main/jmolecules-archunit) in order to test that a project conforms to layer/onion or hexagonal architecture:\n```java\n    @Test\n    public void projectConformsToLayerArchitecture() {\n        // Arrange\n        JavaClasses classes = new ClassFileImporter().importPackages(\"example\");\n\n        // Act\n        EvaluationResult evaluation = JMoleculesArchitectureRules.ensureLayeringStrict().evaluate(classes);\n\n        // Assert\n        Assertions.assertFalse(evaluation.hasViolation());\n    }\n```\n\n### <a name=\"incremental-rules-adoption\"></a>Incremental Rules Adoption\nIf ArchUnit is introduced in an existing big and established project the amount of rules violations will probably be very high to fix them initially. Therefore, ArchUnit provides a handy [feature](https://www.archunit.org/userguide/html/000_Index.html#_freezing_arch_rules) to record all existing violations and start enforcing rules only for the code added after the point in time ([by default](https://www.archunit.org/userguide/html/000_Index.html#_configuration) the list of existing violations is written to a file which can be added to source control).\n\n### <a name=\"rules-generation-based-on-plantuml-diagram\"></a>Rules Generation Based On PlantUML Diagram\nAnother cool ArchUnit feature is that it can automatically [generate](https://www.archunit.org/userguide/html/000_Index.html#_plantuml_component_diagrams_as_rules) rules based on a PlantUML diagram. For example, given the following diagram\n\n![plantUml example](/images/blog/plantuml.png)\n\nif `Facade` class uses `Client2` it will be reported as violation.\n\n### <a name=\"summary\"></a>Summary\nArchUnit is a great tool to enforce software projects architecture by simply defining tests. It allows to define both intuitive and relatively simple rules as well as more complex ones. Its learning curve is not steep and it doesn't require special compilers like AspectJ for example. When used in combination with jMolecules it allows to check for even more archtitecture styles. To see more examples of tests possible with ArchUnit check out this [repo](https://github.com/TNG/ArchUnit-Examples/tree/main/example-junit5/src/test/java/com/tngtech/archunit/exampletest/junit5).\n\n\n"},{"slug":"ranking-data-from-kafka-with-apache-flink","category":"blog","title":"How To Rank Data From Kafka Using Apache Flink","description":"How To Rank Data From Kafka Using Apache Flink And Save The Result To SQL Database","tags":["apache-flink","ranking","kafka","postgres"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n  <div style=\"width:300px;\">\n    <img src=\"/images/blog/flink-header-logo.svg\"\n            alt=\"Apache Flink\"\n            style=\"margin:0;\"\n            />\n  </div>  \n</div>\n\nSometimes data needs to be ranked according to top-n entities. For example, a company wants to find out its top 5 products per each country by the amount of products are sold. If all the product data resides in a single SQL table and the dataset is small then a single groupwise query can be run against the table. However the dataset can be quite large so that the cost of running such query can be prohibitive in a production environment. In addition the source of the data may be other than a database e.g.: a message queue, files etc. In such cases a processing engine like [Apache Flink](https://flink.apache.org) can be used. **In this post I will show how to set up such processing pipeline with the help of only 3 (ðŸ”¥) Flink SQL commands.**\n\nThe pipeline will consume from a Kafka topic and output the result to a Postgres table. The input data will contain country and product information, where each Kafka message is sent after the product was bought. The results will be summarized in a table which will have 3 columns: country, product, purchases, specifically top 3 products by amount of purchases.\n\nFlink has [several API layers](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/concepts/overview/) the most high level of which uses familiar SQL syntax while the SQL commands are run via its embedded SQL client, therefore there's no need for even a line of Java/Scala code.\n\nFirst we need to set up a Flink application cluster. By default official Flink docker image comes with a limited set of connectors. For the demo we'll need a Kafka and JDBC connector as well as Postgres driver therefore we'll extend the official docker image with the jar files of the connectors. Kafka connector can be downloaded [here](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/table/kafka/) while JDBC connector and Postgres driver can be downloaded from [here](https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/connectors/table/jdbc/). Place the downloaded files in the same folder as the `Dockerfile`. In the `Dockerfile` the jar files are copied to `/opt/flink/lib` folder because Flink classpath is set to this folder:\n\n```docker\nFROM flink:1.14.0\nCOPY flink-connector-jdbc_2.12-1.14.0.jar /opt/flink/lib/flink-connector-jdbc_2.12-1.14.0.jar\nCOPY flink-sql-connector-kafka_2.11-1.14.0.jar /opt/flink/lib/flink-sql-connector-kafka_2.11-1.14.0.jar\nCOPY postgresql-42.3.1.jar /opt/flink/lib/postgresql-42.3.1.jar\n```\n\nPlace the `docker-compose.yml` in the same folder as the `Dockerfile`:\n\n```yaml\nversion: \"2.1\"\nservices:\n  jobmanager:\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n    ports:\n      - \"8081:8081\"\n    command: jobmanager\n    container_name: jobmanager\n    environment:\n      - JOB_MANAGER_RPC_ADDRESS=jobmanager\n\n  taskmanager:\n    build:\n      context: .\n      dockerfile: ./Dockerfile\n    depends_on:\n      - jobmanager\n    command: taskmanager\n    container_name: taskmanager\n    environment:\n      - JOB_MANAGER_RPC_ADDRESS=jobmanager\n\n  kafka:\n    image: obsidiandynamics/kafka\n    restart: \"no\"\n    ports:\n      - \"2181:2181\"\n      - \"9092:9092\"\n    environment:\n      KAFKA_LISTENERS: \"INTERNAL://:29092,EXTERNAL://:9092\"\n      KAFKA_ADVERTISED_LISTENERS: \"INTERNAL://kafka:29092,EXTERNAL://localhost:9092\"\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: \"INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT\"\n      KAFKA_INTER_BROKER_LISTENER_NAME: \"INTERNAL\"\n      KAFKA_ZOOKEEPER_SESSION_TIMEOUT: \"6000\"\n      KAFKA_RESTART_ATTEMPTS: \"10\"\n      KAFKA_RESTART_DELAY: \"5\"\n      ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL: \"0\"\n\n  postgres:\n    image: postgres:12.7\n    container_name: postgres\n    ports:\n      - 5432:5432\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n      POSTGRES_DB: postgres\n\n  adminer:\n    image: adminer\n    ports:\n      - 8080:8080\n```\n\nThe `docker-compose.yml` contains all the required services as well as [adminer](https://hub.docker.com/_/adminer) service which provides a web GUI for Postgres client.\n\n1. We'll create the aggregations table in Postgres using adminer. Go to `localhost:8080` and login (the password is `postgres` as well):\n\n![adminer](/images/blog/adminer.png)\n\n2. Create the table:\n\n```sql\nCREATE TABLE IF NOT EXISTS aggregations\n(\n  country VARCHAR ( 50 ) NOT NULL,\n  product VARCHAR ( 50 ) NOT NULL,\n  purchases BIGINT NOT NULL,\n  PRIMARY KEY (country, product)\n);\n```\n\n3. The Kafka topic called `purchases` should be created and populated with messages whose values are of JSON format as follows (messages can be created using [kcat CLI utility](https://github.com/edenhill/kcat)):\n\n```json\n{\n  \"country\": \"some country\",\n  \"product\": \"some product\"\n}\n```\n\nFinally, we can enter Flink jobmanager docker container in order to interact with its SQL API. Run `docker exec -it jobmanager ./bin/sql-client.sh`. Once inside the Flink SQL shell we need to create a `purchases` table which uses source connector to Kafka:\n\n```sql\nCREATE TABLE purchases (\n  country STRING,\n  product STRING\n) WITH (\n   'connector' = 'kafka',\n   'topic' = 'purchases',\n   'properties.bootstrap.servers' = 'kafka:29092',\n   'value.format' = 'json',\n   'properties.group.id' = '1',\n   'scan.startup.mode' = 'earliest-offset'\n);\n```\n\nNext we create an `aggregations` table which will output the top 3 products by purchase volume to the corresponding Postgres table:\n\n```sql\nCREATE TABLE aggregations (\n  `country` STRING,\n  `product` STRING,\n  `purchases` BIGINT NOT NULL,\n  PRIMARY KEY (`country`, `product`) NOT ENFORCED\n) WITH (\n  'connector' = 'jdbc',\n  'url' = 'jdbc:postgresql://postgres:5432/postgres?&user=postgres&password=postgres',\n  'table-name' = 'aggregations'\n);\n```\n\nLastly, we start the processing by running the following query:\n\n```sql\ninsert into aggregations\nSELECT `country`, `product`, `purchases`\nFROM (\n  SELECT *,\n    ROW_NUMBER() OVER (PARTITION BY country ORDER BY `purchases` DESC) AS row_num\n  FROM (SELECT country, product, count(*) AS `purchases` FROM purchases GROUP BY country, product))\nWHERE row_num <= 3;\n```\n\nThe Flink [Top-N](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/queries/topn/) query which was used above is results updating which means if we select all rows in `aggregations` table in Postgres we will receive the most up-to-date result of the processing (meaning that if previously product `x` had top amount of purchases but now there're 3 other products which have more purchases than `x` it will disappear from `aggregations` table). It's worth noting that the query above uses [No Ranking Output Optimization](https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/queries/topn/#no-ranking-output-optimization) that is the table indeed contains top 3 products by country by purchases volume however the results are not sorted by purchases. Since the `aggregations` table generally should not be large it can easily be sorted by the consuming application:\n\n```sql\nSELECT * FROM aggregations\nORDER BY country, purchases DESC\n```\n\nIn one of my next posts I will describe how to run a similar aggregation using Flink [Window Top-N](https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/sql/queries/window-topn/) query which is even cooler!\n"},{"slug":"share-localhost-connections-with-other-people","category":"blog","title":"How To Share Your Localhost Connection To The Outer World","description":"How To Share Your Localhost Connection To The Outer World By Openning a Tunnel","tags":["networking"],"body":"\nIn the times of coronavirus epidemic working remote has become the new norm for many of us. As a software engineer it's import for me to maintain excellent communication with product managers. Sometimes I want to share my work while it's still in intermediate stage to receive feedback or make sure we're on the same page. Specifically, I'm talking about sharing the developement server which runs on `localhost`. This is really handy because it gives the product manager the ability to check out the exact functionality they want to and however they want to.\n\nThe solution I'm using is called [ngrok](https://ngrok.com). Simply put, it allows you to expose your `localhost` connection and it's incredibly easy to use! First you download their [client](https://ngrok.com/download). Then you extract the binary `ngrok` and you can run it like so:\n\n```\n./ngrok http 8000\n```\n\nprovided you server runs at `localhost:8000`. This is the command line output you will get:\n\n![ngrok output](/images/blog/ngrok.png)\n\nAs you can see from the example you get a secure and non-secure URL which you can then send to anyone and they will be able to access your local development server running on port `8000`.\n\n`ngrok` works by opening an http tunnel essentially but you can read a more in-depth explanation [here](https://github.com/inconshreveable/ngrok/blob/master/docs/DEVELOPMENT.md). You can also open use [auth tokens](https://ngrok.com/docs#getting-started-authtoken) or protect the connection with [password](https://ngrok.com/docs#http-auth).\n"},{"slug":"spring-modulith","category":"blog","title":"Introducing Spring Modulith","description":"Spring Modulith is a Spring Boot project which focuses on architectural best-practices.","tags":["java","spring","ddd","event-driven"],"body":"\n<div style=\"display:flex;justify-content:center;padding-right:10%;padding-bottom:50px;padding-top:30px;\">\n    <img style=\"min-width:60%;max-width:70%;\" src=\"/images/blog/Spring_Framework_Logo_2018.svg\"\n            alt=\"Spring Framework Logo\"\n            style=\"margin:0;\"\n            />\n</div>\n\n1. [What is Spring Modulith?](#what-is-spring-modulith)\n2. [Modulith Approach to Application Events](#modulith-approach-to-application-events)\n3. [Integration Testing](#integration-testing)\n    1. [ApplicationModuleTest Annotation](#applicationModuleTest-annotation)\n    2. [Testing Application Events](#applicationModuleTest-testing-application-events)\n4. [Passage of Time Events](#passage-of-time-events)\n5. [Self-Documentated Modules](#self-documented-modules)\n6. [Module Initialization Logic On Startup](#module-initialization-logic-on-startup)\n7. [Production-ready Features](#production-ready-features)\n      1. [Actuator](#actuator)\n      2. [Observability](#observability)\n8. [Spring Modulith vs Java 9 Modules](#spring-modulith-vs-java-9-modules)\n8. [Summary](#summary)\n\n### <a name=\"what-is-spring-modulith\"></a>What is Spring Modulith?\n[Spring Modulith](https://docs.spring.io/spring-modulith/docs/current-SNAPSHOT/reference/html/) is a Spring Boot project which focuses on architectural [best-practices](https://docs.spring.io/spring-modulith/docs/current-SNAPSHOT/reference/html/#fundamentals):\n\n>Spring Modulith supports developers implementing logical modules in Spring Boot applications.\n\nThe project aims to provide structure based on [domain-driven design](https://en.wikipedia.org/wiki/Domain-driven_design) principles. It encourages to create Java packages by domain: instead of grouping all controllers under `controllers` folder or all models under `model` folder we're encouraged to have a Java package for orders or users which will contain all the services/controllers/models related to those domains. It's worth noting that Spring Modulith is still in **experimental stage** and depends on Spring Boot 3 which as result means that it requires **JDK 17**. In order to play with the project features follow the [quickstart](https://github.com/spring-projects/spring-modulith#quickstart).\n\nIt is beside the scope of the post to argue in favor of the domain-driven structure however a good parallel would be organizing household items in your home: you probably wouldn't have an area of your house dedicated solely to electrical appliances but rather you would have a mixer in the kitchen, a TV in the living room and so on. Below are the main architectural principles of Spring Modulith:\n\n- The project regards an application module as a unit of functionality in a Spring Boot application: it consists of externally exposed interfaces and internal logic.\n- By default, each direct sub-package of the root package is considered an application module package. \n- Any sub-package of an application module package is considered to be internal.\n- Code of application modules is allowed to refer to types of other application modules.\n- Other application modules must not refer to the code within internal packages.\n\nLet's demonstrate the last point by looking at the project <a name=\"basic-structure\"></a>structure below:\n```\n.\n|-- README.md\n|-- pom.xml\n`-- src\n    |-- main\n    |   |-- java\n    |   |   `-- example\n    |   |       |-- Application.java\n    |   |       |-- inventory\n    |   |       |   |-- InventoryManagement.java\n    |   |       `-- order\n    |   |           |-- Order.java\n    |   |           |-- internal\n    |   |           |   `-- OrderInternal.java\n\n```\n`Order` uses `OrderInternal` logic, thus `OrderInternal` cannot be package-private and must be made public which unfortunately makes it accessible to other Java packages. This is where Modulith comes in: if some code in `inventory` package refers to `OrderInternal` it will automatically recognize this as architecture violation. In practice this is achieved by adding an integration test:\n```java\nclass ModularityTests {\n\n\tApplicationModules modules = ApplicationModules.of(Application.class);\n\n\t@Test\n\tvoid verifiesModularStructure() {\n\t\tmodules.verify();\n\t}\n}\n```\nRunning the above test would fail with the following error message:\n```\nModularityTests.verifiesModularStructure:33 ? \nViolations - Module 'inventory' depends on non-exposed \ntype example.order.internal.OrderInternal\nwithin module 'order'!\n```\nBehind the scenes Modulith uses [ArchUnit](https://www.archunit.org/) project to enforce various architectural rules (you can read more on how ArchUnit works in my other [post](https://www.spektor.dev/project-architecture-testing-in-java-applications/))\n\nBy default, any non-internal application module is allowed to use other non-internal application modules. We can restrict even further the module dependencies of another module by using `ApplicationModule` annotation in `package-info.java` file:\n```java\n@org.springframework.modulith.ApplicationModule(\n  allowedDependencies = \"order\"\n)\npackage example.inventory;\n```\nThe above annotation means that the only module dependency of `inventory` module is `order`.\n\nWhile it's great to stick to these conventions, in real life the architecture of a project may be more complex and you may need to expose logic from an internal package to another application module. This can be achieved by creating a `package-info.java` file inside that internal package and adding `@org.springframework.modulith.NamedInterface(\"some-internal-package\")`. If we wanted to expose `order.internal` package in the following [project](#basic-structure) we would create the following `package-info.java` file in `order.internal` package:\n```java\n@org.springframework.modulith.NamedInterface(\"order-internal\")\npackage example.order.internal;\n```\n<a name=\"explicitly-import-module\"></a>This would immediately allow to use `OrderInternal` class in `inventory` package. However, in case `inventory` module already had declared explicit dependencies then the internal package would have to be added to those dependencies as follows:\n```java\n@org.springframework.modulith.ApplicationModule(\n        allowedDependencies = {\"order\", \"order::order-internal\"}\n)\npackage example.inventory;\n```\n\nTo summarize, the default rules which are checked for when running `verify()` on `ApplicationModules` are:\n\n- No cycles on the application module level.\n\n- Reference to other non-internal application modules are allowed.\n\n- All references to types that reside in application module internal packages are rejected.\n\n- If explicit dependencies are configured, dependencies to other non-declared application modules are rejected.\n\nIt's worth noting that if the default architecture model which comes with Modulith doesn't suit your needs it can be [customized](https://docs.spring.io/spring-modulith/docs/current-SNAPSHOT/reference/html/#fundamentals.customizing-modules).\n\n### <a name=\"modulith-approach-to-application-events\"></a>Modulith Approach to Application Events\nLet's take a look at the following example:\n```java\n@Service\n@RequiredArgsConstructor\npublic class OrderManagement {\n\n  private final InventoryManagement inventory;\n\n  @Transactional\n  public void complete(Order order) {\n    inventory.updateStockFor(order);\n  }\n}\n```\n>The `complete(â€¦)` method creates functional gravity in the sense that it attracts related functionality and thus interaction with Spring beans defined in other application modules. \n\nThis means that in order to test `OrderManagement` its dependencies must be available either as real instances or mocked. Spring events can be used to decouple these dependencies:\n```java\n@Service\n@RequiredArgsConstructor\npublic class OrderManagement {\n\n  private final ApplicationEventPublisher events;\n\n  @Transactional\n  public void complete(Order order) {\n    events.publishEvent(new OrderCompleted(order.getId()));\n  }\n}\n```\nAbove `ApplicationEventPublisher` Spring class is used to publish `OrderCompleted` event which can then be consumed by `InventoryManagement`. By default, Spring events are published [synchronously](https://www.baeldung.com/spring-events#anonymous-events), that is after the event is published it will be consumed in the same thread by a listener. On the one hand this offers a simpler mental model to reason about events, on the other hand the event consumer participates in the original transaction which widens  transaction boundary and increases the chances for the transaction to fail.\n\nThe above issue can be mitigated by implementing an asynchronous consumer of application events:\n```java\n@Component\nclass InventoryManagement {\n\n  @Async\n  @TransactionalEventListener\n  @Transactional(propagation = Propagation.REQUIRES_NEW)\n  void on(OrderCompleted event) { /* â€¦ */ }\n}\n```\nSpring transactions don't propagate to other threads therefore `@Async` allows to execute the event listener as not part of the original transaction. In addition, using `@TransactionalEventListener` allows to perform logic [right after](https://docs.spring.io/spring-framework/docs/current/javadoc-api/org/springframework/transaction/support/TransactionSynchronization.html#afterCompletion(int)) the original transaction has successfully been committed. That is right after the `complete()` method transaction is successfully committed the event listener `on()` method is invoked. Lastly, the event listener itself might need to be transactional hence the `@Transactional` annotation ([explanation of propagation type](https://github.com/spring-projects/spring-modulith/issues/80)). Since there's quite a lot of annotation boilerplate here, Spring Modulith provides `@ApplicationModuleListener` as a subsitute for the above 3 annotations.\n\nStill, there're two more issues with the setup:\n- If the listener fails the event is lost.\n- The application may crash just before the listener is invoked, so even `try/catch` mechanism inside a listener will not help.\n\nTo tackle the above issues Spring Modulith provides [event publication registry](https://docs.spring.io/spring-modulith/docs/current/reference/html/#events.publication-registry) which hooks into Spring event publication mechanism and persists an event for every subscribed listener. In case a listener successfully finishes, the event is persisted with non-null `completion_date`, in case it fails `completion_date` will be null. This allows to implement custom retry mechanisms while by default events whose listeners failed are resubmitted at application startup.\n\nThere're currently 3 Modulith event registry starters:\n- `spring-modulith-starter-jpa`\n- `spring-modulith-starter-jdbc`\n- `spring-modulith-starter-mongodb`\n\nConsider the following event:\n```java\n@Transactional\n\tpublic void complete() {\n\t\tevents.publishEvent(new OrderCompleted(UUID.randomUUID()));\n\t}\n```\nwith two subscribers:\n```java\n@Service\n@RequiredArgsConstructor\nclass InventoryManagement {\n\n\tprivate static final Logger LOG = LoggerFactory.getLogger(InventoryManagement.class);\n\n\tprivate final ApplicationEventPublisher events;\n\n\t@ApplicationModuleListener\n\tvoid on(OrderCompleted event) {\n\t\tLOG.info(\"Received order completion for {}.\", event.orderId());\n    // ...\n\t}\n}\n```\nand\n```java\n@Service\n@RequiredArgsConstructor\nclass UserManagement {\n\n\tprivate static final Logger LOG = LoggerFactory.getLogger(UserManagement.class);\n\n\tprivate final ApplicationEventPublisher events;\n\n\t@ApplicationModuleListener\n\tvoid on(OrderCompleted event)  {\n\t\tLOG.info(\"Received order completion for {}.\", event.orderId());\n\t\tthrow new RuntimeException(\"some UserManagement error\");\n\t}\n}\n```\nIn this scenario `UserManagement` subscriber fails while `InventoryManagement` succeeds which will result in the following records in `event_publication` table (if `spring-modulith-starter-jdbc` is used):\n```\npostgres=# select * from event_publication;\n-[ RECORD 1 ]----+-----------------------------------------------------------------------\nid               | 027f117e-2c8f-411a-a2c2-8786a6f35eea\nlistener_id      | example.user.UserManagement.on(example.order.OrderCompleted)\nevent_type       | example.order.OrderCompleted\nserialized_event | {\"orderId\":\"4850f124-a14c-4f8d-b13f-ff3db3d2fd9f\"}\npublication_date | 2023-06-03 11:18:12.173418+00\ncompletion_date  |\n-[ RECORD 2 ]----+-----------------------------------------------------------------------\nid               | bc5b6526-0e32-4401-80eb-a31e83845797\nlistener_id      | example.inventory.InventoryManagement.on(example.order.OrderCompleted)\nevent_type       | example.order.OrderCompleted\nserialized_event | {\"orderId\":\"4850f124-a14c-4f8d-b13f-ff3db3d2fd9f\"}\npublication_date | 2023-06-03 11:18:12.134889+00\ncompletion_date  | 2023-06-03 11:18:13.253296+00\n```\n\n### <a name=\"integration-testing\"></a>Integration Testing\n#### <a name=\"applicationModuleTest-annotation\"></a>ApplicationModuleTest Annotation\n\nSpring Modulith allows to set up integration tests via `@ApplicationModuleTest` annotation similar to `@SpringBootTest`. However `@ApplicationModuleTest` exposes more functionality, first and foremost, bootstrap modes:\n- `STANDALONE` (default)â€‰â€”â€‰Runs the current module only.\n\n- `DIRECT_DEPENDENCIES`â€‰â€”â€‰Runs the current module as well as imports all modules the current one directly depends on.\n\n- `ALL_DEPENDENCIES`â€‰â€”â€‰Runs the current module and imports the entire tree of modules depended on.\n\nThis is quite handy because we get these bootstrap modes out of the box as opposed to having to manually create [slices](https://spring.io/blog/2016/08/30/custom-test-slice-with-spring-boot-1-4) in Spring Boot tests.\n\n#### <a name=\"applicationModuleTest-testing-application-events\"></a>Testing Application Events\nIf the application is event driven it can require considerable effort and testing infrastructure in order to write integration tests. This is especially true if the events are consumed asynchronously. In order to help developers Modulith provides `Scenario` utility which can be consumed as an argument in a JUnit 5 test:\n```java\n@ApplicationModuleTest\nclass SomeIntegrationTest {\n\n  @Test\n  public void someCheck(Scenario scenario) {\n    // test definition here\n  }\n}\n```\nA test can be started either using `scenario.stimulate()` or `scenario.publish()` API. As an example let's consider `OrderManagement` class:\n```java\n@Service\n@RequiredArgsConstructor\npublic class OrderManagement {\n\n\tprivate final @NonNull ApplicationEventPublisher events;\n\n\t@Transactional\n\tpublic void complete() {\n\t\tevents.publishEvent(new OrderCompleted(UUID.randomUUID()));\n\t}\n}\n```\nand the following integration test:\n```java\n@ApplicationModuleTest\n@Import(AsyncTransactionalEventListener.class)\n@RequiredArgsConstructor\nclass EventPublicationRegistryTests {\n\n\tprivate final OrderManagement orders;\n\tprivate final AsyncTransactionalEventListener listener;\n\n\t@Test\n\tvoid inventoryToFulfillOrderIsX(Scenario scenario) throws Exception {\n\n\t\tvar order = new Order();\n\n\t\t// Initiate flow by calling stimulate()\n\t\tscenario.stimulate(() -> orders.complete(order))\n\t\t\t\t// wait until inventoryToFulfillOrder becomes not-null/non-Optional\n\t\t\t\t.andWaitForStateChange(() -> listener.getInventoryToFulfillOrder())\n\t\t\t\t.andVerify(inventoryToFulfillOrder -> {\n\t\t\t\t\tassertThat(inventoryToFulfillOrder).contains(\"X\");\n\t\t\t\t});\n\t}\n\n\tstatic class AsyncTransactionalEventListener {\n\n\t\tprivate static final Logger LOG = LoggerFactory.getLogger(AsyncTransactionalEventListener.class);\n\n\t\t@Getter\n\t\tprivate String inventoryToFulfillOrder;\n\n\t\t@ApplicationModuleListener\n\t\tvoid foo(OrderCompleted event) {\n\t\t\tLOG.info(\"Received order completion for {}.\", event.orderId());\n\n\t\t  // ...\n\t\t\tthis.inventoryToFulfillOrder = \"inventory X\";\n\n\t\t\tLOG.info(\"Finished order completion for {}.\", event.orderId());\n\t\t}\n\t}\n}\n```\nIn the example above `stimulate()` was used to start order management flow waiting for the state changes of `AsyncTransactionalEventListener`. Once the state has changed (`inventoryToFulfillOrder` was initiated to a non-null/non-`Optional` value) `verify()` is called.\n\nFor an example of a `scenario.publish()` test consider `InventoryUpdated`, `InventoryManagement` and `OrderManagement` classes. Below is `InventoryUpdated` implementation:\n```java\npackage example.inventory;\n\nimport org.jmolecules.event.types.DomainEvent;\n\npublic record InventoryUpdated(String inventoryId) implements DomainEvent {}\n```\n`OrderManagement` implementation:\n```java\n@Service\n@RequiredArgsConstructor\npublic class OrderManagement {\n\n  private final ApplicationEventPublisher events;\n\n  @Transactional\n  public void complete(Order order) {\n    events.publishEvent(new OrderCompleted(order.getId()));\n  }\n}\n```\n\nand `InventoryManagement`:\n```java\n@Service\n@RequiredArgsConstructor\nclass InventoryManagement {\n\n\tprivate final ApplicationEventPublisher events;\n\n\t@ApplicationModuleListener\n\tvoid on(OrderCompleted event) {\n\n\t\t// ...\n\t\tevents.publishEvent(new InventoryUpdated(event.orderId()));\n\t}\n}\n```\nFinally below is a test which checks that `InventoryUpdated` event is published after `OrderCompleted` is fired:\n```java\n@ApplicationModuleTest(extraIncludes = \"inventory\")\n@RequiredArgsConstructor\nclass ApplicationTests {\n\n    @Test\n    void inventoryUpdatedEventIsPublished(Scenario scenario) {\n        Order order = new Order();\n\n        scenario.publish(new OrderCompleted(order.getId()))\n                .andWaitForEventOfType(InventoryUpdated.class)\n                .toArrive();\n    }\n}\n```\n[Earlier](#applicationModuleTest-annotation) it was mentioned that when testing a module which invokes logic from other modules (`OrderManagement` expects `InventoryUpdated` event to be triggered by `InventoryManagement` in the test) we need to set the bootstrap mode in `@ApplicationModuleTest` accordingly. However, note that `OrderManagement` service **doesn't** directly depend on `InventoryManagement`, on the contrary it uses application events. In this case setting mode in `@ApplicationModuleTest(mode = ApplicationModuleTest.BootstrapMode.DIRECT_DEPENDENCIES)` won't help to trigger `InventoryUpdated` event. There're two options to address this:\n1. `extraIncludes` parameter can be used as in `@ApplicationModuleTest(extraIncludes = \"inventory\")`. This will import services from `inventory` package. Multiple extra modules can be declared: `@ApplicationModuleTest(extraIncludes = {\"inventory\", \"user\"})`.\n2. Adding `@SpringBootTest` and `@EnableScenarios` annotations to a test class.\n\n### <a name=\"passage-of-time-events\"></a>Passage of Time Events\nPassage of time events are another interesting feature provided by Spring Modulith. It was inspired by Matthias Verraes [blog post](https://verraes.net/2019/05/patterns-for-decoupling-distsys-passage-of-time-event/).\n\nImagine having multiple but unrelated pieces of logic which need to be executed each day for example. Instead of creating a CRON job which will run every day for every such piece of logic, the passage of time approach recommends to publish an event: `DayHasPassed` then have all relevant services subscribe to that event. Spring Modulith [provides](https://docs.spring.io/spring-modulith/docs/current/reference/html/#moments) an implementation of passage of time logic by exposing `HourHasPassed`, `DayHasPassed`, `WeekHasPassed`, `MonthHasPassed`, `QuarterHasPassed` and `YearHasPassed` events, just include the following dependency:\n```xml\n<dependency>\n  <groupId>org.springframework.modulith</groupId>\n  <artifactId>spring-modulith-moments</artifactId>\n</dependency>\n```\nIn addition, if `spring.modulith.moments.enable-time-machine` property is set to `true` then `TimeMachine` bean with `shiftBy` method will be exposed. `shiftBy` allows to forward time which can be a handy utility in integration tests to trigger passage of time-based logic. For instance, suppose we have the following listener:\n```java\n\t@ApplicationModuleListener\n\tvoid on(HourHasPassed event) {\n\t\t// ...\n\t\tevents.publishEvent(new OrderCompleted(\"some_order_id\"));\n\t}\n```\nWe wouldn't want to wait for a whole hour in the integration which would test the above logic, therefore `TimeMachine` can be shifted one hour forward:\n```java\n\t@Test\n\tvoid orderCompletedEventIsSent(Scenario scenario) {\n\t\tscenario.stimulate(() -> timeMachine.shiftBy(Duration.ofHours(1)))\n\t\t\t\t.andWaitForEventOfType(OrderCompleted.class)\n\t\t\t\t.toArrive();\n\t}\n```\n### <a name=\"self-documented-modules\"></a>Self-Documentated Modules\nWhat I've always wanted in a project is self-documentation. For example, there's an amazing project [XState](https://xstate.js.org/docs/) available to [Node.js](https://nodejs.org/en) developers which allows to write state machine logic and automatically generates interactive (!) [visualizations](https://xstate.js.org/docs/#visualizer) of the logic.\n\nSpring Modulith automatically generates PlantUML diagram of your application as well as module canvases (module metadata). The below snippet generates both [PlantUML](https://github.com/plantuml/plantuml) diagrams and canvases:\n```java\npublic class DocumentationTests {\n\n    ApplicationModules modules = ApplicationModules.of(Application.class);\n\n    @Test\n    void writeDocumentationSnippets() {\n        new Documenter(modules)\n                .writeModulesAsPlantUml()\n                .writeIndividualModulesAsPlantUml()\n                .writeModuleCanvases();\n        // Or simply: new Documenter(modules).writeDocumentation();\n    }\n}\n```\nBy default, the documentation will be generated to `target/spring-modulith-docs` in a [Maven](https://maven.apache.org/) project or `build/spring-modulith-docs` in a [Gradle](https://gradle.org/) project. This is how the folder output looks like in my test project which has 3 modules:\n```\ntarget/spring-modulith-docs\n|-- components.puml\n|-- module-inventory.adoc\n|-- module-inventory.puml\n|-- module-order.adoc\n|-- module-order.puml\n|-- module-warehouse.adoc\n`-- module-warehouse.puml\n```\n`components.puml` contains all application modules while each module has additional two files: `.puml` which is represents its own diagram of interactions specific just to the module and `.adoc` file which contains metadata about the module. Below is a sample `components.puml` file:\n![components.puml](/images/blog/components.puml.png)\n\nA sample `order` module canvas might look as follows:\n\n![order.adoc](/images/blog/order.adoc.png)\n\n### <a name=\"module-initialization-logic-on-startup\"></a>Module Initialization Logic On Startup\nSometimes there's certain logic that needs to be executed once per module on application startup. Importantly, this logic may need to be executed in certain order: if module `B` depends on module `A` the initialization logic should first be executed in module `A` then in module `B`. In a regular Spring application the order of beans initialization is controlled by Spring beans dependencies graph. However, in an event-based application the order of dependencies is also determined by `listens to` relationship. Therefore, Spring Modulith [provides](https://docs.spring.io/spring-modulith/docs/current/reference/html/#runtime.application-module-initializer) `ApplicationModuleInitializer` interface. First, the following dependency must be added:\n```xml\n<dependency>\n  <groupId>org.springframework.modulith</groupId>\n  <artifactId>spring-modulith-runtime</artifactId>\n  <scope>runtime</scope>\n</dependency>\n```\nnext, the initializer class can implement the interface as follows:\n```java\n@Component\npublic class InventoryInitializer implements ApplicationModuleInitializer {\n\n    @Override\n    public void initialize() {\n        // ...\n    }\n}\n```\nSpring Modulith [guarantees](https://docs.spring.io/spring-modulith/docs/current/reference/html/#runtime.application-module-initializer) that such initializers will be called in the same order as in application module dependency structure.\n\nWhile to some this feature will remind Golang `init()` [function](https://go.dev/doc/effective_go#init) which is called before any other logic in its package, it's important to point out that Spring Modulith does not guarantee to run the initializer before the constructor of other Spring beans in a given module. It's only guaranteed that a module's initializer will be run after the initializers of its modules dependencies.\n\n### <a name=\"production-ready-features\"></a>Production-ready Features\n#### <a name=\"actuator\"></a>Actuator\nSpring Modulith extends [Spring Boot actuator](https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html#actuator.enabling) with an additional [endpoint](https://docs.spring.io/spring-modulith/docs/current/reference/html/#production-ready.actuator) `/application-modules`. The endpoint provides modules metadata information including the list of modules and their dependencies. Below is an example output of the endpoint `http://localhost:8080/actuator/application-modules`:\n```json\n{\n  \"order\": {\n    \"displayName\": \"Order\",\n    \"basePackage\": \"example.order\",\n    \"dependencies\": []\n  },\n  \"inventory\": {\n    \"displayName\": \"Inventory\",\n    \"basePackage\": \"example.inventory\",\n    \"dependencies\": [\n      {\n        \"types\": [\n          \"EVENT_LISTENER\"\n        ],\n        \"target\": \"order\"\n      }\n    ]\n  }\n}\n```\nIn order to enable the actuator add these dependencies:\n```xml\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.experimental</groupId>\n\t\t\t<artifactId>spring-modulith-actuator</artifactId>\n\t\t\t<version>{projectVersion}</version>\n\t\t\t<scope>runtime</scope>\n\t\t</dependency>\n\n\t\t<!-- Spring Boot actuator starter required to enable actuators in general -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t<artifactId>spring-boot-starter-actuator</artifactId>\n      <version>{projectVersion}</version>\n\t\t\t<scope>runtime</scope>\n\t\t</dependency>\n```\n#### <a name=\"observability\"></a>Observability\nAdding the following dependency:\n```xml\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.experimental</groupId>\n\t\t\t<artifactId>spring-modulith-observability</artifactId>\n\t\t\t<version>{projectVersion}</version>\n\t\t</dependency>\n```\nwill enable observability capabilities. Note that additional dependencies will need to be included based on Spring Boot [recommendation](https://docs.spring.io/spring-boot/docs/current/reference/html/actuator.html#actuator.micrometer-tracing). When playing with the feature I chose to include:\n```xml\n\t\t<dependency>\n\t\t\t<groupId>io.micrometer</groupId>\n\t\t\t<artifactId>micrometer-tracing-bridge-brave</artifactId>\n\t\t\t<version>1.1.2</version>\n\t\t</dependency>\n\n    <!--\t\tfor latency reporting-->\n\t\t<dependency>\n\t\t\t<groupId>io.zipkin.reporter2</groupId>\n\t\t\t<artifactId>zipkin-reporter-brave</artifactId>\n\t\t\t<version>2.16.4</version>\n\t\t</dependency>\n```\nSpring Modulith will trace each Spring bean method invocation and mark each trace with the name of the application module. A sample trace JSON will look like this:\n```json\n  {\n    \"traceId\": \"6496cef3b37032a5d32c85ca582d855b\",\n    \"id\": \"d32c85ca582d855b\",\n    \"name\": \"warehouse\",\n    \"timestamp\": 1687604979559445,\n    \"duration\": 236,\n    \"localEndpoint\": {\n      \"serviceName\": \"application\",\n      \"ipv4\": \"10.100.102.4\"\n    },\n    \"tags\": {\n      \"module.method\": \"e.w.WarehouseManagement.on(â€¦)\",\n      \"org.springframework.modulith.module\": \"warehouse\"\n    }\n  }\n```\nIn [Zipkin](https://zipkin.io/) UI the traces will look as follows:\n\n![trace](/images/blog/trace.png)\n\n### <a name=\"spring-modulith-vs-java-9-modules\"></a>Spring Modulith vs Java 9 Modules\nJDK 9 release introduced [Java 9 Platform Module System (JPMS)](https://www.oracle.com/corporate/features/understanding-java-9-modules.html) which is an attempt to add higher-level modularity in Java projects. Basically, the feature allows to define:\n- a module as a group of packages\n- a module's allowed dependencies\n- a module's exports\n- the services a module consumes/exports\n- which other modules are allowed reflection\n\nIt's important to note that all Java 9 modules are private by default so every module which wants to depend on/consume some other modules functionality needs to do so explicitly.\n\nIn contrast, Spring Modulith base-level application modules (the direct children of the `main` package) are public by default. Only internal modules must be explicitly exported and optionally explicitly [imported](#explicitly-import-module). In addition, Spring Modulith project architecture rules can be [customized](https://docs.spring.io/spring-modulith/docs/current-SNAPSHOT/reference/html/#fundamentals.customizing-modules) to the needs of a specific project. The main difference of course is that Spring Modulith goes much further than to allow project modularity, rather it offers its vision on how to build Java applications effectively and offers the necessary tools:\n- advocating domain-driven design\n- advocating for event-driven application model and providing the tools to follow this model (both in business logic - including events persistence and testing code)\n- providing self-documenting modules\n- module initialization logic on startup\n\n### <a name=\"summary\"></a>Summary\nI think Spring Modulith is a great attempt at advocating domain-driven design as well as event-driven application model. The project is still experimental however in my opinion it has the potential to become a very useful Spring Boot extension if not its natural evolution. The project offers lots of tools to make developers lives easier like event testing infrastructure and self-documentation among others. Looking forward to see what the future holds for the project."},{"slug":"tech-articles-i-liked","category":"blog","title":"Tech Articles I Really Liked","description":"The Tech Articles I Really Liked","tags":["misc"],"body":"\nEvery once in a while I come across I really great tech article or video which offers great insights, sheds light on obscure matters or simply made a big impression on me. I'd like to share such articles/videos below with a brief summary (I will be updating the list occasionally).\n\n### Inventing On Principle by Bret Victor\n\nHis [lecture](http://worrydream.com/#!/InventingOnPrinciple) made a profound impression on me. Bret, who is a former human interface researcher at Apple, talks about just how important it is to have interactive tools when creating and to be able to explore when developing new features.\n\n### REST is the new SOAP by Pakal de Bonchamp\n\nThis [article](https://www.freecodecamp.org/news/rest-is-the-new-soap-97ff6c09896d/) showcases all the drawbacks and missed goals of REST architecture. Pakal argues REST implementation varies from organization to organization, there's no set standard and sometimes its semantics are misleading and unintuitive. It involves quite a lot of boilerplate and as a result requires more time in order to develop API then [RPC](https://en.wikipedia.org/wiki/Remote_procedure_call) technologies, for example. Indeed, I hear about [gRPC](https://grpc.io/about/) more and more lately (I hope to check it out in one of my projects soon ðŸ™‚). In addition, there's GraphQL which a direct competitor to REST. GraphQL is a very interesting technology which I've been working with for more than a year now. It has some great features (playground, type system, ability to select only the necessary data). I do feel it has some drawbacks as well though:\n\n- requests are POST by default which means no CDN cache out of the box and that you will need to spend some extra effort to [convert](https://www.apollographql.com/docs/apollo-server/performance/apq/) your queries to GET requests.\n- even errors return with 200 code which can be a nuissance.\n- for large datasets all that type validation and checking can be a heavy penalty on [response time](https://github.com/graphql/graphql-js/issues/723).\n"},{"slug":"tips-on-learning-guitar-from-software-engineer","category":"blog","title":"A Software Engineer's Journey To Learning Guitar","description":"A Software Engineer's Journey To Learning Guitar, how to learn guitar being a programmer, how to learn guitar as a grown-up, how to learn a guitar with a full time job.","tags":["guitar","lifestyle","well-being"],"body":"\n![guitar](/images/blog/guitar-pc.jpeg)\n\n1. [Tip #1 - How To Choose A Guitar](#how-to-choose-guitar)\n2. [Tip #2 - How To Learn The Basics of Playing Guitar](#how-to-learn)\n3. [Tip #3 - Comparison Between Learning to Code And How To Play Guitar](#comparison)\n4. [Tip #4 - Electric Guitar Amplifier and Effects](#effects)\n5. [Tip #5 - Big O Notation in Guitar Playing](#big-o)\n6. [Tip #6 - The Investment Is Worth It](#investment)\n7. [Recommendations](#recommendations)\n\nI usually share programming tips and tricks in this blog because I like building software. However, for the last two years I've had a new passion which is playing guitar. In this post I'd like share my journey to learning guitar while having a full-time job as a software engineer and share several tips which hopefully will prevent you from making the same mistakes I did. **For the record:** I started playing when I was 32 years old, I've been playing for 2 years (1 year with my [teacher](https://www.eladregev.com)), practicing for about 30 minutes every day (started playing since Covid-19).\n\nMy first guitar was a classical Yamaha C70. I didn't know the distinction between acoustic and classical guitars back then so I went for the cheapest guitar from a well-known brand.\n\nIn retrospect my first guitar was kind of horrible mainly because of its high [action](https://www.sweetwater.com/sweetcare/articles/guitar-setup-part-2-setting-action/) which is the distance between the guitar neck and the strings. Obviously the higher the action the harder one has to press on the strings which makes playing harder.\n\n### <a name=\"how-to-choose-guitar\"></a>Tip #1 - How To Choose A Guitar\nWhen choosing a guitar I greatly recommend going to a guitar store and playing the guitar first to check how comfortable it is and whether you like its sound. Ordering a guitar simply based on online reviews is not a good idea.\n\n#### Type Of Guitar\nFor a beginner the easiest type of guitar to learn is a classical guitar. It has a wide neck which will make it easier to play chords. Also it has nylon strings which are the easiest on the fingers (metal strings of an acoustic will inflict much more pain).\n\n#### Guitar Action\nCheap guitars for beginners will usually have high action (the distance between guitar neck and the strings), this makes playing harder. Not to worry: go to your local guitar shop and ask them if they can lower the action or can recommend someone (it's called guitar \"setup\") who can. It will cost about $70-100 (at least here in Israel) and makes a huge difference (I cannot stress this enough)!\n\nApart from action another important factor is guitar size. The bigger a guitar the harder it is to play it (at least for a beginner). Therefore, I'd recommend to choose a smaller body. In terms of electric guitars I used to try really expensive models in guitar stores and couldn't hear much difference with entry-level ones.\n\nAfter about a year of playing my classical guitar I bought an acoustic (Yamaha FG 820) and an electric (Kramer Guitars Focus VT-211S - about $125 at [Music Store](https://www.musicstore.com/en_OE/EUR/Kramer-Guitars-Focus-VT-211S-Pewter-Grey/art-GIT0051967-000)). Even after paying import taxes it was still much cheaper to buy the guitar and other equipment from Music Store.\n\n### <a name=\"how-to-learn\"></a>Tip #2 - How To Learn The Basics of Playing Guitar\nThere're a lot of Youtube videos where songs are taught however before you start learning songs it's good to learn the proper technique: how to hold your hands when playing for example. I subscribed to a beginner's course from [Guitar Tricks](https://www.guitartricks.com/home) when I just started learning. I liked that the course instructor talked about proper technique and taught the very basics in a structured way so each lesson built on the previous. Later when I started going to a guitar teacher he said my technique was pretty good so I guess Guitar Tricks do a good job. That being said taking lessons from a guitar teacher is much better than any online course. If you can afford it do it. After about a year of online courses I didn't feel I was making much progress so I started taking lessons from a guitar teacher. I started to progress much more quickly. Guitar teachers often teach tricks which allow you to play guitar more easily.\n\n### <a name=\"comparison\"></a>Tip #3 - Comparison Between Learning to Code And To Play Guitar\nI started learning to code at university. Concepts like variables, functions, loops were all new to me. However, after a semester I could write basic simple programs. Every error would take me considerable time to figure out but even the hardest setbacks would take at most a few days to figure out the problem. Therefore, the progress was made each day/week. Learning guitar was very different. Learning to play the most basic chords and transition between them would take months. After a year I could play only a few very easy songs. Whereas the progress in coding is very noticeable learning guitar chords and transitions is a very repetetive process which takes months and certain critical mass was achieved only after two years for me. Only now I start feeling that I can pick up new songs much easier. It now takes me a week or two to learn a new song to a reasonable level while it used to take me several months before. So if you choose to learn guitar be there for the long run.\n\n### <a name=\"effects\"></a>Tip #4 - Electric Guitar Amplifier and Effects\nIf you decide to buy an electric guitar you will need an amplifier. The first amplifier I bought was Marshall MG15FX a very basic model. It sounds very loud even at low volume and the sound itself is not great. I think that's true with many entry-level amplifiers. In addition, you will want to apply effects like distortion when you play hard rock songs. Often a guitar pedal can be bought which produces a certain effect like distortion/chorus etc. Pedals usually start at $50-100 which means that for every effect you'll need to fork out at least $50. In my opinion the best solution to the above problems (entry-level amplifier with good sound + effects) is a multi-effects guitar processor because it functions as both an amplifier and can apply effects. I use BOSS GT-1 which I love. Its amplifier sound is great (you can choose many amplifier sounds in its settings) and it can produce about 100 effects. It's also relatively inexpensive, it costs the sum of an entry-level amplifier and 1 effect so you get a really good value for money. You just need to hook it up to your speakers. Such processors often allow to use multiple effects simultaneously and have software which allows to control the processor from a PC.\n\n### <a name=\"big-o\"></a>Tip #5 - Big O Notation in Guitar Playing\nSoftware engineering puts great emphasis on algortithms and efficiency. I think guitar playing has certain parallels. For example, there're always multiple ways to play the same chord so when you transition between chords it's important to choose the fingerings which allow for faster/easier transitions. From an engineering perspective it's kind of cool to think of the most efficient combination of fingerings.\n\n### <a name=\"investment\"></a>Tip #6 - The Investment Is Worth It\nLearning to play guitar at an older age is not easy and took me 2 years until I was relatively comfortable playing the songs I wanted (still not very complicated songs). That being said it's a great gift, the feeling you get when playing a song you love is amazing. The thing to keep in mind is to practice regularly and eventually you will reach a critical mass to be able to play the songs you want. When you look back at this endeavor after 5-10 years you will be so grateful you did it. Who knows with developments like ChatGPT perhaps in 10 years software engineers will not be needed anymore, at least we'll be able to make a living playing in a wedding band.\n\n### <a name=\"recommendations\"></a>Recommendations\nIn this section I'd like to recommend the people/equipment who/which helped me in my journey:\n\n- [Guitar Tricks](https://www.guitartricks.com/home) - the online course I took where I learned the very basics.\n- [Elad Regev](https://www.eladregev.com) - my amazing guitar teacher.\n- [Herzl Raz](https://razguitars.business.site) - luthier who did the \"setup\" on my guitar.\n- [Music Store](https://www.musicstore.com/en_OE/EUR) - online guitar store where I ordered my electric guitar.\n- [My amplifier + effects processor](https://www.boss.info/global/products/gt-1/)"},{"slug":"upgrading-to-apollo-client-3","category":"blog","title":"Upgrading From Apollo Boost to Apollo Client 3","description":"Upgrading From Apollo Boost to Apollo Client 3","tags":["apollo-client","graphql"],"body":"\n![apollo logo](/images/blog/apollo.png)\n\nI've been using [Apollo Client/Server](https://www.apollographql.com/docs/) for a while and although these are great projects I did experience inconsistencies and issues with the client cache/local state. So much so that I was thinking of migrating to another local state solution. However Apollo Client 3 came out recently which significantly revamped local state functionality and claims to have fixed the earlier issues. So I decided to upgrade to it from using the older Apollo Boost.\n\nThe new client provides all of the functionality that Apollo Boost used to provide so the Boost project is now retired. The Apollo team posted a really nice [guide](https://www.apollographql.com/docs/react/migrating/apollo-client-3-migration/) on how to upgrade to the new client which is really helpful. Below are some of the steps I had to take in order to complete the migration in addition to the ones described in the guide:\n\n1. We were using [next-will-apollo](https://www.npmjs.com/package/next-with-apollo) with Apollo Boost. It's a nice library which calls [getDataFromTree](https://www.apollographql.com/docs/react/api/react/ssr/#getdatafromtree) Apollo function which determines which queries need to be run for render and fetches them. However, using this library with Apollo Client 3 resulted in this [error](https://github.com/apollographql/apollo-client/issues/5808): `Module not found: Can't resolve 'apollo-client' in 'node_modules/@apollo/react-hooks/lib'`. Upgrading `next-will-apollo` to the latest version solved the issue. However, in the latest version of `next-will-apollo` you need to explicitly pass `getDataFromTree` like so:\n\n```js\nimport withApollo from 'next-with-apollo';\nimport { getDataFromTree } from '@apollo/react-ssr';\n\nexport default withApollo({ ctx, headers, initialState }) {\n  return new ApolloClient({\n    // ... options\n  })\n}, { getDataFromTree });\n```\n\n2. For me personally, by far the most work involved in the upgrade was caused by the fact that the cache results are immutable. For example, performing a `sort` on such results will cause a run-time error because native Javascript `sort` method sorts in-place thus mutating the original array.\n\n3. Apollo Client 3 introduced a really nice feature called [reactive variables](https://www.apollographql.com/docs/react/local-state/reactive-variables/). From now on, you don't need to write custom client resolvers in order to read/write local state values, just use reactive variables to create them as simply as:\n\n```js\nimport { makeVar } from \"@apollo/client\"\nconst isLoggedIn = makeVar(false)\n```\n\nYou can then use the `isLoggedIn` function anywhere in code in order to either get the value: `const isLoggedIn = isLoggedIn()` or to set the value `isLoggedIn(true)`\n\nIt's important to reset reactive variables to their initial values if you reset the client store because their values are not reset automatically:\n\n```js\nimport withApollo from 'next-with-apollo';\nimport { getDataFromTree } from '@apollo/react-ssr';\n\nexport default withApollo({ ctx, headers, initialState }) {\n  const client = new ApolloClient({\n    // ... options\n  })\n  client.onResetStore(() => {\n    // reset reactive variables here...\n  });\n  return client\n}, { getDataFromTree });\n```\n\nLastly, make sure to pay attention to the [breaking changes](https://www.apollographql.com/docs/react/migrating/apollo-client-3-migration/#breaking-cache-changes) involved in the upgrade.\n"},{"slug":"use-dataloader-with-graphql-and-mongo","category":"blog","title":"How To Solve GraphQL N+1 Problem By Using Custom Dataloaders","description":"How To Solve GraphQL N+1 Problem Using Dataloader and Using MongoDB","tags":["mongodb","graphql","optimization","dataloader"],"body":"\nMany have heard about the famous N+1 problem in GraphQL. It often occurs when queries include relationships between entities, especially child-parent. For example, let's say we want to make a GraphQL query for a list of purchases and the buyers in some supermarket.\n\nBelow are the entities definition in GraphQL SDL:\n\n```graphql\ntype Buyer {\n  id: ID!\n  name: String!\n}\n\ntype Purchase {\n  id: ID!\n  buyerID: ID!\n  paymentAmount: Float!\n}\n\ntype Query {\n  buyer(id: ID!): Buyer\n  purchases: [Purchase]!\n}\n```\n\nLet's say we want to make the following query:\n\n```graphql\n{\n  query {\n    purchases {\n      paymentAmount\n      buyer {\n        name\n      }\n    }\n  }\n}\n```\n\nIn a typical implementation of GraphQL relationships (regardless of the framework be it [Prisma.io](https://www.prisma.io/) or [graphql-compose](https://github.com/graphql-compose/graphql-compose)) the `purchases` resolver will not contain the logic which fetches the buyer. Rather a relationship will be defined in the GraphQL library of your choice such that `buyer` resolver will automatically be called in order to provide `buyer` data to the `purchases` resolver.\n\nIn order to find the buyer of a given purchase we first need to obtain the buyer id from `Purchase` entity because in GraphQL an entity only has access to its parent. Suppose that the data is stored in a MongoDB database. First a query is made for a list of all purchases (let's say there're `N` purchases). Then for each individual purchase a `buyer` resolver is called. Therefore, N+1 queries are made to the database (one for the list of purchases and N for each buyer).\n\nThis is of course extremely inefficient because once the list of purchases along with buyer id's is known just one MongoDB query needs to be performed in order to obtain the array of buyers.\n\nIf only there was a way to accumulate the buyer queries and delegate their execution to some logic which would perform the lookups and seamlessly integrate the result into `purchases` resolver. Indeed such a way exists and it's called a [dataloader](https://github.com/graphql/dataloader). This project was originally developed at Facebook and is not limited solely to GraphQL but rather to any use case where one needs to group and delay queries execution until some point (it can be applied to REST architecture as well). However, it is especially useful when solving the N+1 problem. In short, instead of executing `buyer` resolver per each purchase, the buyer id is loaded int\no the dataloader. Once the purchases list is known, dataloader exposes the list of buyer ids to the callback provided to it. The callback can then make a MongoDB query to fetch the list of all relevant buyers and return them. Dataloader then takes care of merging the buyers with purchases! We've now gone from N+1 queries to the database to just 2. ðŸŽ‰\n\nBelow is my implementation of the dataloader for a project which uses [ApolloServer](https://www.apollographql.com/docs/apollo-server/) and [graphql-compose-mongoose](https://github.com/graphql-compose/graphql-compose-mongoose).\n\nFirstly, it's handy to have the dataloader helpers available at ApolloServer `context` so that they can always be used with whatever entity without the need of importing the `createDataLoaders` in each file (`createDataLoaders` returns an object of dataloaders, its implementation is below):\n\n```js\nconst server = new ApolloServer({\n  schema: mySchema,\n  context: async ({ req, res }) => {\n    return {\n      dataLoader: createDataLoaders(),\n    }\n  },\n})\n```\n\nThis is the `Purchase` model:\n\n```js\nimport { composeWithMongoose } from \"graphql-compose-mongoose\"\nimport { Purchase } from \"./mongooseModels\"\n\nconst PurchaseTC = composeWithMongoose(Purchase)\n\nconst purchasesResolver = \"purchases\"\nPurchaseTC.addResolver({\n  name: purchasesResolver,\n  type: PurchaseTC,\n  resolve: async ({ source, args, context, info }) => Purchase.find({}).lean(),\n})\n\n// define the relationship between Purchase and Buyer\nPurchaseTC.addFields({\n  buyer: {\n    type: \"Buyer!\",\n    resolve: async (source, args, context, info) => {\n      return (\n        // if buyer already exists then no need to look it up\n        // (it could exist because another Purchase resolver might perform\n        // an aggregation of purchases and buyers)\n        source.buyer ||\n        // source here includes all purchase data, including buyer id\n        context.dataLoader.purchaseToBuyer.load({ source, args, context, info })\n      )\n    },\n  },\n})\n\nschemaComposer.Query.addFields({\n  [purchasesResolver]: PurchaseTC.getResolver(purchasesResolver),\n})\n```\n\nFinally, this is the implementation of the `purchaseToBuyer` dataloader:\n\n```js\nimport DataLoader from \"dataloader\"\nimport { Buyer } from \"./mongooseModels\"\n\n/*\n There are a few constraints dataloader batch function must uphold:\n 1. The Array of values must be the same length as the Array of keys.\n 2. Each value in the array of values must correspond to the same key in the array of keys.\n */\n\nconst getPurchaseToBuyerDataLoader = () =>\n  new DataLoader(\n    // purchaseResolverParams is an array of graphql-compose-mongoose resolver params\n    async purchaseResolverParams => {\n      const relevantBuyers = await Buyer.find({\n        id: {\n          $in: purchaseResolverParams.map(\n            resolverParams => resolverParams.source.buyerID\n          ),\n        },\n      }).lean()\n      // the map is used to quickly get the buyer based on the buyer id below\n      const buyersMap = new Map()\n      relevantBuyers.forEach(buyer => buyersMap.set(buyer.id, buyer))\n      return new Promise(resolve => {\n        resolve(\n          purchaseResolverParams.map(resolverParams => {\n            const purchase = resolverParams.source\n            const buyer = buyersMap.get(purchase.buyerID)\n            return (\n              buyer ||\n              new Error(`buyer for purchase id '${purchase.id}' not found`)\n            )\n          })\n        )\n      })\n    },\n    {\n      // dataloader caches objects with same key\n      cacheKeyFn: key => key.source.buyerID,\n    }\n  )\n\nexport const createDataLoaders = () => ({\n  purchaseToBuyer: getPurchaseToBuyerDataLoader(),\n})\n```\n\nIt can easily be checked that the above solution performs only 2 MongoDB queries using database logs. First the appropriate MongoDB log level needs to be set in order to observe all query executions: run this `db.setLogLevel(2)` on your db in MongoDB shell. I personally like to to use [NoSQLBooster for MongoDB](https://nosqlbooster.com) because it provides a nice GUI to track MongoDB queries.\n"},{"slug":"vault-agent-docker-compose-setup","category":"blog","title":"Vault Agent (Persistent) Docker Compose Setup","description":"Vault Agent (Persistent) Docker Compose Setup","tags":["vault","docker"],"body":"\n![vault](/images/blog/vault.jpg)\n\n**TL;DR:** You can find the code in this Github [repo](https://github.com/yossisp/vault-agent-docker-compose).\n\nRecently I needed to integrate [Hashicorp Vault](https://www.hashicorp.com/products/vault) with a Java application. For local development I wanted to use [Vault Agent](https://www.vaultproject.io/docs/agent) which can connect to the Vault server. The advantage of using Vault Agent is that it bears the brunt of authentication complexity with Vault server (including SSL certificates). Effectively, this means that a client application can send HTTP requests to Vault Agent without any need to authenticate. This setup is frequently used in the real world for example by using [Agent Sidecar Injector](https://www.vaultproject.io/docs/platform/k8s/injector) inside a Kubernetes cluster. It makes it easy for client applications inside a K8s pod to get/put information to a Vault server without each one having to perform the tedious authentication process.\n\nSurprisingly, I couldn't find much information on using Vault with Vault Agent via docker-compose, which in my opinion is by far the easiest method to set up a Vault playground. I did find [this](https://gitlab.com/kawsark/vault-agent-docker/-/tree/master) example which served as the inspiration for this post however it involves a more complex setup as well as using Postgres and Nginx. I'd like to present the most minimal setup, the bare basics needed to spin up a Vault Agent and access it locally via `localhost`.\n\n**WARNING:** the setup is intentionally simplified, please don't use it in production.\n\nFirst of all we'll use the official Vault docker images for the `docker-compose.yml`:\n```yaml\nversion: '3.7'\n\nservices:\n  vault-agent:\n    image: hashicorp/vault:1.9.6\n    restart: always\n    ports:\n      - \"8200:8200\"\n    volumes:\n      - ./helpers:/helpers\n    environment:\n      VAULT_ADDR: \"http://vault:8200\"\n    container_name: vault-agent\n    entrypoint: \"vault agent -log-level debug -config=/helpers/vault-agent.hcl\"\n    depends_on:\n      vault:\n        condition: service_healthy\n  vault:\n    image: hashicorp/vault:1.9.6\n    restart: always\n    volumes:\n      - ./helpers:/helpers\n      - vault_data:/vault/file\n    ports:\n      - \"8201:8200/tcp\"\n    cap_add:\n      - IPC_LOCK\n    container_name: vault\n    entrypoint: \"vault server -config=/helpers/vault-config.hcl\"\n    healthcheck:\n      test: wget --no-verbose --tries=1 --spider http://localhost:8200 || exit 1\n      interval: 10s\n      retries: 12\n      start_period: 10s\n      timeout: 10s\n\nvolumes:\n  vault_data: {}\n```\n\nHere we're using the same image to start Vault server in dev mode as well as start the Vault Agent. In addition a volume is created for `helpers` directory which will contain:\n1. The policy for Vault server `admin-policy.hcl`:\n```hcl\npath \"sys/health\"\n{\n  capabilities = [\"read\", \"sudo\"]\n}\npath \"sys/policies/acl\"\n{\n  capabilities = [\"list\"]\n}\npath \"sys/policies/acl/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"auth/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"sys/auth/*\"\n{\n  capabilities = [\"create\", \"update\", \"delete\", \"sudo\"]\n}\npath \"sys/auth\"\n{\n  capabilities = [\"read\"]\n}\npath \"kv/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"secret/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"identity/entity-alias\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"identity/entity-alias/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"identity/entity\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"identity/entity/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"sys/mounts/*\"\n{\n  capabilities = [\"create\", \"read\", \"update\", \"delete\", \"list\", \"sudo\"]\n}\npath \"sys/mounts\"\n{\n  capabilities = [\"read\"]\n}\n```\n2. The policy for Vault Agent `vault-agent.hcl`:\n```hcl\npid_file = \"./pidfile\"\nvault {\n  address = \"http://vault:8200\"\n  retry {\n  num_retries = 5\n  }\n}\nauto_auth {\n  method {\n  type = \"approle\"\n    config = {\n      role_id_file_path = \"/helpers/role_id\"\n      secret_id_file_path = \"/helpers/secret_id\"\n      remove_secret_id_file_after_reading = false\n    }\n  }\n  sink \"file\" {\n    config = {\n      path = \"/helpers/sink_file\"\n    }\n  }\n}\ncache {\n  use_auto_auth_token = true\n}\nlistener \"tcp\" {\n  address = \"0.0.0.0:8200\"\n  tls_disable = true\n}\n```\n3. The `init.sh` script which will create AppRole auth method:\n```bash\napk add jq curl\nexport VAULT_ADDR=http://localhost:8200\nroot_token=$(cat /helpers/keys.json | jq -r '.root_token')\nunseal_vault() {\n  export VAULT_TOKEN=$root_token\n  vault operator unseal -address=${VAULT_ADDR} $(cat /helpers/keys.json | jq -r '.keys[0]')\n  vault login token=$VAULT_TOKEN\n}\nif [[ -n \"$root_token\" ]]\n  then\n      echo \"Vault already initialized\"\n      unseal_vault\n  else\n      echo \"Vault not initialized\"\n      curl --request POST --data '{\"secret_shares\": 1, \"secret_threshold\": 1}' http://127.0.0.1:8200/v1/sys/init > /helpers/keys.json\n      root_token=$(cat /helpers/keys.json | jq -r '.root_token')\n\n      unseal_vault\n\n      vault secrets enable -version=2 kv\n      vault auth enable approle\n      vault policy write admin-policy /helpers/admin-policy.hcl\n      vault write auth/approle/role/dev-role token_policies=\"admin-policy\"\n      vault read -format=json auth/approle/role/dev-role/role-id \\\n        | jq -r '.data.role_id' > /helpers/role_id\n      vault write -format=json -f auth/approle/role/dev-role/secret-id \\\n        | jq -r '.data.secret_id' > /helpers/secret_id\nfi\nprintf \"\\n\\nVAULT_TOKEN=%s\\n\\n\" $VAULT_TOKEN\n```\n\n4. Below is the config for the Vault server to be saved in `vault-config.hcl` file:\n\n```hcl\nstorage \"file\" {\n  # this path is used so that volume can be enabled https://hub.docker.com/_/vault\n  path = \"/vault/file\"\n}\n\nlistener \"tcp\" {\n  address     = \"0.0.0.0:8200\"\n  tls_disable = \"true\"\n}\n\napi_addr = \"http://127.0.0.1:8200\"\ncluster_addr = \"https://127.0.0.1:8201\"\nui = true\n```\n\nNext we'll create `startVault.sh` script to start Vault:\n```shell\nWAIT_FOR_TIMEOUT=120 # 2 minutes\ndocker-compose up --detach\necho Waiting for Vault Agent container to be up\ncurl https://raw.githubusercontent.com/eficode/wait-for/v2.2.3/wait-for | sh -s -- localhost:8200 -t $WAIT_FOR_TIMEOUT -- echo success\ndocker exec vault /bin/sh -c \"source /helpers/init.sh\"\ndocker restart vault-agent\n```\n\nAfter you created the above files in the `helpers` directory, the project structure should be as follows:\n```\n.\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ helpers\nâ”‚   â”œâ”€â”€ admin-policy.hcl\nâ”‚   â”œâ”€â”€ init.sh\nâ”‚   â”œâ”€â”€ vault-agent.hcl\nâ”‚   â””â”€â”€ vault-config.hcl\nâ””â”€â”€ startVault.sh\n```\n\nFinally, run `source startVault.sh` to start Vault server and Vault Agent.\n\nNow any client application can access Vault Agent over `http://localhost:8200` on the host machine, for example the following command creates a secret name `hello`:\n```shell\ncurl --request POST -H \"Content-Type: application/json\"  \\\n--data '{\"data\":{\"foo\":\"bar\"}}' http://localhost:8200/v1/kv/data/hello\n```\nwhile this command retrieves the secret name `hello`:\n```shell\ncurl http://localhost:8200/v1/kv/data/hello\n```\n\nIn addition Vault web UI is available at `http://localhost:8201/ui`. In order to log into the UI use the value of `root_token` field in `./helpers/key.json` file (using token login method in the UI).\n\nVault server uses file storage backend which makes this a persistent setup (a docker volume is mounted), so that tokens data will persist after machine restart or running `docker-compose down`.\n"},{"slug":"why-my-domain-is-forced-to-open-as-https","category":"blog","title":"Why Certain Domains Are Forced To Open As HTTPS in Some Browsers","description":"Why Certain Domains Are Forced To Open As HTTPS in Some Browsers","tags":["browser","google-chrome","firefox","https"],"body":"\nRecently, I set up a development environment server and I wanted to access it via a test domain I added in /etc/hosts file which I mapped to `localhost`. The domain was `test.dev`. While using `curl` to test server response worked, entering the domain in Google Chrome browser would produce **This site canâ€™t be reached** message:\n![This site canâ€™t be reached](/images/blog/site_cant_be_reached.png)\n\nIt turns out that Chrome was redirecting to https and because my server wasn't listening on port 443 I received the error. It was surprising because accessing the server on `localhost` doesn't result in redirect to https. In addition when I added `test.com` to the /etc/hosts file no redirection occurred as well. Accessing `test.dev` in Firefox resulted in the same behavior however accessing it in Brave browser did **not** result in redirect to https. After some digging into this I found that:\n\n1. `.dev` TLD was [bought by Google](https://icannwiki.org/.dev).\n2. [Chromium](<https://en.wikipedia.org/wiki/Chromium_(web_browser)>) codebase which Google Chrome is based on has a [file](https://chromium.googlesource.com/chromium/src.git/+/63.0.3239.118/net/http/transport_security_state_static.json#259) `transport_security_state_static.json` which specifies several TLDs where `mode` is `force-https`. The list includes `.dev`.\n\nTherefore, any `.dev` domain will be redirected to https in Chrome. According to [Wikipedia](https://en.wikipedia.org/wiki/HTTP_Strict_Transport_Security#Limitations) the list of browsers which have a list of TLDs which are forced to load via https includes Firefox and Edge as well.\n"}]